
R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ### Packages
> library(MASS)
Warning message:
package ‘MASS’ was built under R version 3.6.2 
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.2.1     ✔ dplyr   1.1.1
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ dplyr::select() masks MASS::select()
Warning messages:
1: package ‘tidyverse’ was built under R version 3.6.2 
2: package ‘ggplot2’ was built under R version 3.6.2 
3: package ‘tidyr’ was built under R version 3.6.2 
4: package ‘readr’ was built under R version 3.6.2 
5: package ‘purrr’ was built under R version 3.6.2 
6: package ‘forcats’ was built under R version 3.6.2 
> library(survey)
Loading required package: grid
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

Loading required package: survival

Attaching package: ‘survey’

The following object is masked from ‘package:graphics’:

    dotchart

Warning message:
package ‘Matrix’ was built under R version 3.6.2 
> #devtools::install_github("csterbenz1/KBAL", ref = "cat_kernel")
> library(kbal)
> library(parallel)
> library(knitr)
Warning message:
package ‘knitr’ was built under R version 3.6.2 
> library(glmnet)
Loaded glmnet 4.1-1
Warning message:
package ‘glmnet’ was built under R version 3.6.2 
> 
> #updated and cleaned: 9/28/23
> 
> ###### SET PARAMS  ###############
> set.seed(9345876)
> 
> if(detectCores() > 10 & detectCores() < 50 ) {
+   path_data = "/home/csterbenz/Data/"
+   cores_saved = 10
+ } else if(detectCores() > 50) {
+     path_data =  path_data = "/nas/home/ciaras/kpop/data/"
+     cores_saved = 20
+ } else if(detectCores() != 4) {
+   path_data= "/Users/Ciara_1/Dropbox/kpop/Updated/application/data/"
+   cores_saved = 1
+ } else {
+     path_data= "/Users/Ciara/Dropbox/kpop/Updated/application/data/"
+     cores_saved = 2
+ }
> options(dplyr.print_max = 1e9)
> 
> ####### Parameters for kpop results:
> #ebal tolerance and max iterations for kpop
> tolerance = 1e-4
> maxit = 500
> #adjust these both for runtime if needed
> #increment in svd(K) dims in biasbound minimization search
> increment = 5
> #specify min/max svd(K) dims in biasbound min search
> min_num_dims = NULL
> max_num_dims = NULL
> #to run just nonkpop methods
> eval_kpop = TRUE
> #to run kpop+mf
> kpop_constraints = TRUE
> SAVE = TRUE #save .Rdata results?
> #can adjust accordingly to machine for adequate number of sims
> #nsims = (detectCores()-cores_saved)*12
> nsims = 1000
> 
> ##### Central Params to adjust for Model Specification
> #sd(y)*noise; 1-> r^2 = .5; sqrt(2) -> r^2 = .33; 1/2*sqrt(2) -> r^2 = .66;
> noise = 1 
> #adjusts sample size by dividing p(S) by scalar pS_denom (i.e. pS = plogis(XBeta)/pS_denom)
> pS_denom = 60
> 
> ###################### Formulas ################
> formula_rake_demos_noeduc <- ~recode_age_bucket + recode_female + recode_race +
+   recode_region + recode_pid_3way
> #updated to include 6 way edu
> formula_rake_demos_weduc <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_educ + recode_pid_3way
> 
> formula_rake_all_vars <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_pid_3way + recode_educ +
+   recode_income_5way + recode_relig_6way + recode_born + recode_attndch_4way
> 
> create_targets <- function (target_design, target_formula) {
+   target_mf <- model.frame(target_formula, model.frame(target_design))
+   target_mm <- model.matrix(target_formula, target_mf)
+   wts <- weights(target_design)
+ 
+   return(colSums(target_mm * wts) / sum(wts))
+ }
> 
> ### Post-stratification function
> ## For now assumes that strata variable is already created and in
> ## the data set and called "strata"‚
> postStrat <- function(survey, pop_counts, pop_w_col, strata_pass, warn = T) {
+   survey_counts <- survey %>%
+     group_by(!!as.symbol(strata_pass)) %>%
+     summarize(n = n()) %>%
+     ungroup() %>%
+     mutate(w_survey = n / sum(n))
+ 
+   pop_counts <- pop_counts %>%
+     rename(w_pop = matches(pop_w_col))
+   
+   if(warn == T & nrow(survey_counts) !=  nrow(pop_counts)) {
+       missing_strat = pop_counts[! (( pop_counts[, strata_pass]%>% pull()) %in% (survey_counts[, strata_pass]%>% pull() )), strata_pass]
+       warning(paste("Strata in Pop not found in Sample. Dropping", 
+                     sum(pop_counts[(pop_counts[, strata_pass] %>% pull()) %in% 
+                                        (missing_strat %>% pull()),"n" ]), 
+                     "empty cells\n"), immediate.  =T )
+   } 
+   post_strat <- pop_counts %>%
+     left_join(survey_counts, by = strata_pass) %>%
+     filter(!is.na(w_survey)) %>%
+     ## Normalizes back to 1 after dropping
+     ## empty cells
+     mutate(w_pop = w_pop * 1/sum(w_pop),
+            w = w_pop / w_survey) %>%
+     dplyr::select(!!as.symbol(strata_pass), w)
+ 
+   survey <- survey %>%
+     left_join(post_strat)
+ 
+   return(survey)
+ }
> 
> check_sample <- function(sample, selection_model) {
+     check = model.matrix(selection_model, data = sample)
+     check = colSums(check)
+     fail = check[which(check ==0)]
+     fail_bin = length(check[which(check ==0)]) > 0 
+     check_prop = check/nrow(sample)
+     
+     return(list(samp_prop = check_prop,
+                 fail  = fail, 
+                 fail_bin = fail_bin,
+                 counts = check))
+ }
> 
> check_sample_outcome <- function(sample, pop = cces, selection_model, interaction_cols, interaction_cols_2 = NULL) {
+     
+     vars = all.vars(selection_model)
+     var = NULL
+     counts = NULL
+     prop = NULL
+     u_outcome = NULL
+ 
+     #uninteracted variables
+     run_counts <- function(data) {
+         for(i in 1:length(vars)) {
+             t = data %>% group_by_at(vars[i]) %>%
+                 summarise(n = n(), 
+                           avg_outcome = mean(outcome)) %>%
+                 mutate(prop = round(n/nrow(sample),4))
+             var = c(var, as.character(t[,1] %>% pull()))
+             counts = c(counts, t$n)
+             prop = c(prop,  t$prop)
+             u_outcome = c(u_outcome, t$avg_outcome)
+         }
+         #interactions
+         t = suppressMessages(data %>% group_by_at(interaction_cols) %>% 
+                                  summarise(n = n(),
+                                            avg_outcome = mean(outcome)) %>%
+                                  mutate(prop = round(n/nrow(sample), 4)))
+         interaction = apply(t, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+         counts = data.frame(var  = c(var, interaction),
+                             n = c(counts, t$n), 
+                             prop = c(prop, t$prop),
+                             avg_outcome = c(u_outcome, t$avg_outcome))
+         
+         if(!is.null(interaction_cols_2)) {
+             t2 = suppressMessages(data %>% group_by_at(interaction_cols_2) %>% 
+                                       summarise(n = n(),
+                                                 avg_outcome = mean(outcome)) %>%
+                                       mutate(prop = round(n/nrow(sample), 4)))
+             interaction = apply(t2, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+             append = cbind(data.frame(var = interaction), t2[, - c(1,2)])
+             counts = rbind(counts, append)
+         }
+     }
+     samp_counts = run_counts(sample)
+     pop_counts = run_counts(pop)
+     
+     if(nrow(pop_counts) > nrow(samp_counts)) {
+         samp_counts = rbind(samp_counts, data.frame(var = pop_counts$var[!(pop_counts$var %in% samp_counts$var)],
+                                                   n = rep(0,sum(!(pop_counts$var %in% samp_counts$var))),
+                                                   prop = rep(0,sum(!(pop_counts$var %in% samp_counts$var))),
+                                                   avg_outcome = rep(-99,sum(!(pop_counts$var %in% samp_counts$var)))))
+     }
+     
+     
+     fail = sum(samp_counts$n == 0)
+     bad = sum(samp_counts$prop <= 0.05)
+     bad_strata = data.frame(strata = as.character(samp_counts$var[samp_counts$prop <= 0.05]), prop = samp_counts$prop[samp_counts$prop <= 0.05])
+     v_bad = sum(samp_counts$prop <= 0.01)
+     v_bad_strata = data.frame(strata = as.character(samp_counts$var[samp_counts$prop <= 0.01]), prop = samp_counts$prop[samp_counts$prop <= 0.01])
+     counts$var[samp_counts$prop <= 0.01]
+     
+     samp_counts = samp_counts %>% mutate(leq_5pp = as.numeric(prop <= 0.05),
+                                leq_1pp = as.numeric(prop <= 0.01))
+     
+    
+     return(list(samp_counts = samp_counts,
+                 fail = fail, 
+                 bad = bad, 
+                 v_bad = v_bad, 
+                 bad_strata = bad_strata,
+                 v_bad_strata = v_bad_strata))
+ }
> 
> check_outcome <- function(outcome) {
+     beyond_support = (min(outcome) <0 | max(outcome) > 1)
+     return(beyond_support)
+ }
> 
> bound_outcome <- function(outcome, coefs, increment = 1, increment_intercept = .01, noise, cces_expanded, silent = T) {
+     denom = 10
+     fail = check_outcome(outcome)
+     while(fail) {
+         denom = denom + increment
+         if(!silent) { cat(denom, ": ") }
+         coefs_use = coefs/denom
+         outcome = cces_expanded %*% coefs_use
+         
+         outcome = outcome + rnorm(nrow(cces_expanded), mean = 0, sd = sd(outcome)*noise)
+         summary(outcome)
+         if(max(outcome) <=1 & min(outcome) <0 ) {
+             coefs[1] = coefs[1] + increment_intercept
+             if(!silent) { cat("\nmoving intercept up", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         if(max(outcome) >1 & min(outcome) >=0 ) {
+             coefs[1] = coefs[1] - increment_intercept
+             if(!silent) { cat("\nmoving intercept down", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         fail = check_outcome(outcome)
+         if(!silent) { cat(round(min(outcome),2), round(max(outcome),2),  "\n") }
+     }
+     if(!silent) { cat(paste("Min denom:", denom)) }
+     return(list(outcome = outcome, coefs = coefs_use, denom = denom))
+     
+ }
> 
> 
> ############# Load Data #####################
> #these data have been cleaned already see app_modeled for how it was done
> ### Load Target Data
> cces <- readRDS(paste0(path_data, "cces_lasso_061021.rds"))
> cces$recode_age_bucket = as.character(cces$recode_age_bucket)
> cces$recode_age_3way= as.character(cces$recode_age_3way)
> 
> cces <- cces %>% mutate(recode_agesq = recode_age^2/ mean(recode_age^2),
+                         recode_agecubed = recode_age^3/ mean(recode_age^3))
> 
>  ##################### LASSO: Selection #############################
> 
> selection_model = as.formula(~recode_pid_3way + recode_female + recode_age_bucket 
+                              + recode_educ_3way 
+                              + recode_race
+                              + recode_born 
+                              + recode_born:recode_age_bucket
+                              + recode_pid_3way:recode_age_bucket
+ )
> inter = c("recode_pid_3way", "recode_age_bucket")
> inter_2 = c("recode_born", "recode_age_bucket")
> cces_expanded = model.matrix(selection_model, data = cces)
> coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
> rownames(coefs) = colnames(cces_expanded)
> coefs
                                             [,1]
(Intercept)                                    NA
recode_pid_3wayInd                             NA
recode_pid_3wayRep                             NA
recode_femaleMale                              NA
recode_age_bucket36 to 50                      NA
recode_age_bucket51 to 64                      NA
recode_age_bucket65+                           NA
recode_educ_3wayCollege                        NA
recode_educ_3wayPost-grad                      NA
recode_raceHispanic                            NA
recode_raceOther                               NA
recode_raceWhite                               NA
recode_bornYes                                 NA
recode_age_bucket36 to 50:recode_bornYes       NA
recode_age_bucket51 to 64:recode_bornYes       NA
recode_age_bucket65+:recode_bornYes            NA
recode_pid_3wayInd:recode_age_bucket36 to 50   NA
recode_pid_3wayRep:recode_age_bucket36 to 50   NA
recode_pid_3wayInd:recode_age_bucket51 to 64   NA
recode_pid_3wayRep:recode_age_bucket51 to 64   NA
recode_pid_3wayInd:recode_age_bucket65+        NA
recode_pid_3wayRep:recode_age_bucket65+        NA
> #(p(S)) for negative bias select non dem voters
> coefs[,1] = c(-2, #intercept -5 w race
+               2, #selection of indep pos
+               2, #selection of R pos
+               .5, #male
+               .15, #36-50,
+               .2, #51-64,
+               .2, #65+,
+               .7, #college
+               -1 , #post-grad
+               .5,#hispanic
+               .3,#other
+               .7,#white
+               2, #bornagain
+               1,#bornagain x 36-50
+               1.5, #bornagain x 51-64
+               2, #bornagain x 65+
+               .3,#ind x 36-50
+               .5, #rep x 36-50,
+               1, #ind x 51-64,
+               1, #rep x 51-64,
+               -.2, #ind x 65+
+               2 #rep x 65+
+ )
> 
> xbeta = cces_expanded %*% coefs
> p_include = plogis(xbeta)
> p_include = p_include/pS_denom
> sum(p_include)
[1] 513.1844
> 
> # gg_dat = data.frame(Selection_Probability = p_include,
> #                    Pid = cces$recode_pid_3way,
> #                    age = cces$recode_age_bucket,
> #                    born = cces$recode_born,
> #                    Outcome_pD = NA)
> # ggplot(gg_dat) +
> #     geom_density(aes(x= Selection_Probability, color = age, linetype = Pid)) +
> #     ggtitle("Distribution of Selection Probabilities by Party") +
> #     theme_bw()
> 
> 
> #################### DESIGN OUTCOME MODEL ##################
> #p(D)
> #start perfectly negatively correlated
> coefs_outcome = -coefs
> #scale out as a starting point before adjusting to be in probability range
> coefs_outcome = coefs_outcome*1.5
> cor(p_include, cces_expanded %*% coefs_outcome)
           [,1]
[1,] -0.8991601
> #adjust starting point of intercept so we're in ball part of prob range
> #so the following adjustment of coefs doesn't take forever
> coefs_outcome[1] = 25
> cor(p_include, cces_expanded %*% coefs_outcome)
           [,1]
[1,] -0.8991601
> 
> cat(paste("Adding sd(outcome)*",round(noise, 3), "\n")) 
Adding sd(outcome)* 1 
> #reset seed so noise added is directly controllable/replicable without running in order above
> set.seed(1383904)
> bound = bound_outcome(outcome = cces_expanded %*% coefs_outcome,
+                       coefs = coefs_outcome,
+                       cces_expanded = cces_expanded,
+                       noise = noise, silent = F)
11 : -0.03 3.57 
12 : -0.1 3.08 
13 : -0.22 2.97 
14 : -0.16 2.74 
15 : -0.02 2.55 
16 : -0.05 2.34 
17 : -0.11 2.14 
18 : -0.23 2.13 
19 : -0.02 1.99 
20 : 
moving intercept down 24.99 
0.01 1.99 
20 : 
moving intercept down 24.98 
0.03 1.94 
20 : -0.03 1.85 
21 : 
moving intercept down 24.97 
0 1.79 
21 : -0.01 1.85 
22 : -0.1 1.76 
23 : -0.06 1.58 
24 : 0 1.63 
25 : 
moving intercept down 24.96 
0.03 1.48 
25 : 
moving intercept down 24.95 
0.01 1.53 
25 : -0.06 1.51 
26 : -0.04 1.46 
27 : -0.07 1.48 
28 : -0.04 1.36 
29 : -0.05 1.41 
30 : -0.06 1.31 
31 : -0.1 1.22 
32 : -0.01 1.16 
33 : -0.04 1.21 
34 : 
moving intercept down 24.94 
0.03 1.18 
34 : 
moving intercept down 24.93 
0.01 1.11 
34 : -0.13 1.1 
35 : -0.03 1.11 
36 : -0.09 1.03 
37 : -0.09 1.01 
38 : -0.07 1.02 
39 : 
moving intercept up 24.94 
-0.01 0.96 
39 : 
moving intercept up 24.95 
-0.03 0.97 
39 : 
moving intercept up 24.96 
-0.05 0.99 
39 : 
moving intercept up 24.97 
-0.01 0.96 
39 : -0.08 1.01 
40 : 
moving intercept up 24.98 
-0.07 0.96 
40 : 
moving intercept up 24.99 
-0.02 0.94 
40 : 
moving intercept up 25 
-0.03 0.93 
40 : 0.03 0.95 
Min denom: 40> coefs_outcome = bound$coefs
> xbeta_outcome = bound$outcome
> if(check_outcome(xbeta_outcome)) {
+     warning("Outcome beyond prob support for some units when noise is added",
+                                                             immediate. = T)
+ }
> 
> cat(paste("Mean outcome w/noise is", round(mean(xbeta_outcome)*100,3), "\n"))
Mean outcome w/noise is 49.143 
> cat(paste("Range of outcome w/noise is\n"))
Range of outcome w/noise is
> cat(paste(summary(xbeta_outcome), "\n"))
Min.   :0.02652   
 1st Qu.:0.40756   
 Median :0.49534   
 Mean   :0.49143   
 3rd Qu.:0.57859   
 Max.   :0.95386   
> s = summary(lm(update(selection_model, xbeta_outcome ~ .),data = cces))
> R2_outcome = s$adj.r.squared
> cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
R^2 outcome is 0.497 
> cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
Mean scaled outcome (target) is 49.143> cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))

Corr of sampling prob and outcome  -0.635> cces$outcome = xbeta_outcome
> 
> # #LA: 
> #plot(density(xbeta_outcome))
> # cor(xbeta_outcome, p_include)
> # cces[which(xbeta_outcome == min(xbeta_outcome)),
> #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
> # cces[which(xbeta_outcome == max(xbeta_outcome)),
> #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
> # 
> # gg_dat = data.frame(Selection_Probability = p_include,
> #                     Pid = cces$recode_pid_3way,
> #                     age = cces$recode_age_bucket,
> #                     Outcome_pD = cces$outcome)
> # gg_p_include_outcome = ggplot(gg_dat) +
> #     geom_point(aes(x= Selection_Probability, y= Outcome_pD, color = age, shape = Pid)) +
> #     theme_bw()
> # gg_p_include_outcome
> 
> 
> ######### Make STRATA variable in CCES ############
> #post-stratification gets true selection model
> formula_ps <- selection_model
> cces <- bind_cols(cces, cces %>%
+                       unite("strata", all.vars(formula_ps), remove = FALSE) %>%
+                       dplyr::select(strata))
> 
> 
> #################### Targets ###################
> cces_svy <- suppressWarnings(svydesign(ids = ~1, data = cces))
> margin_sim = svymean(~outcome, cces_svy)[1]* 100
> margin_sim
 outcome 
49.14268 
> targets_rake_demos_noeduc <- create_targets(cces_svy,
+                                             formula_rake_demos_noeduc)
> targets_rake_demos_weduc <- create_targets(cces_svy, formula_rake_demos_weduc)
> targets_rake_all_vars <- create_targets(cces_svy,
+                                         formula_rake_all_vars)
> targets_demo_truth <- create_targets(cces_svy, selection_model)
> 
> 
> ## Make table of Population Counts for post-stratification for manual ps function
> cces_counts <- cces %>%
+   group_by(strata) %>%
+   summarize(n = n()) %>%
+   ungroup() %>%
+   mutate(w = n / sum(n, na.rm = TRUE))
> 
> ########################### PREP SIMS ##################################
> 
> est_mean <- function(outcome, design) {
+   svymean(as.formula(paste0("~", outcome)), design, na.rm = TRUE)[1]
+ }
> 
> #########################################
> ############## variance calc ###########
> ## Variance functions
> var_fixed <- function(Y, weights, pop_size) {
+     ## note: needs weights that sum to population total
+     if(round(sum(weights)) != pop_size) { weights = weights*pop_size/sum(weights)}
+     return(Hmisc::wtd.var(Y, weights))
+ }
> 
> ## kott (14) (under poisson)
> var_quasi <- function(weights, residuals, pop_size) {
+     #moving from kott 14 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     #sum^n (w_i^2 - 1/N_pop w_i)e_i^2 
+     return(sum((weights^2 - (weights / pop_size))*residuals^2))
+ }
> 
> ## kott (15) linearization
> var_linear <- function(weights, residuals, sample_size) {
+     #moving from kott 15 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     # n/(n-1) sum^n (w_i*e_i)^2 - (1/n-1) [sum(w_i*e_i)^2] *using this nonapprox version (deriv in my notes)
+     # approx = sum^n (w_i*e_i)^2 - (1/n) sum(w_i*e_i)^2
+     n = sample_size
+     return((n/(n-1))*sum((weights * residuals)^2) - 1/(n-1) * sum(weights * residuals)^2)
+ }
> 
> ## chad
> var_chad <- function(weights, residuals) {
+     return(sum(weights^2 * residuals^2))
+ }
> 
> ## calculate all variances
> calc_SEs <- function(Y, residuals, pop_size, weights, sample_size) {
+     if(round(sum(weights)) != 1 ) {
+         weights = weights/sum(weights)
+     }
+     return(data.frame(SE_fixed = sqrt(var_fixed(Y, weights, pop_size) / length(Y)),
+                       SE_quasi = sqrt(var_quasi(weights, residuals, pop_size)),
+                       SE_linear = sqrt(var_linear(weights, residuals, sample_size)),
+                       SE_chad = sqrt(var_chad(weights, residuals))))
+ }
> 
> ########################### RUN SIMS ##################################
> 
> #if run sim sin rstudio and want print outs
> #rstudio is dumb:
> # rstudio_para_cat <- function(...){
> #     system(sprintf('echo "\n%s\n"', paste0(..., collapse="")))
> # }
> 
> 
> #double check params are as expected:
> nsims
[1] 1000
> sum(p_include)
[1] 513.1844
> SAVE
[1] TRUE
> eval_kpop
[1] TRUE
> kpop_constraints
[1] TRUE
> #save some memory by deleting some larger objects for the parallel run
> pryr::mem_used()
184 MB
> rm(bound, cces_expanded, s, xbeta, xbeta_outcome)
> pryr::mem_used()
175 MB
> system.time({
+   sims <- mclapply(1:nsims, function(nsim) {
+     #
+     system.time({
+     cat(paste("=====================  SIM:",nsim, 
+               "===================== \n"))
+ 
+     sample <- rbinom(nrow(cces), 1, p_include)
+     survey_sim <- cces[sample == 1, ]
+     
+     survey_design <- suppressWarnings(svydesign(ids = ~1, data = survey_sim))
+     
+     ########################### check sample ##########################
+     #checks to see if we got a sample that has no units in a strata in the selection model 
+     #this would indicate our selection probabilities are too small
+     check_s = check_sample(survey_sim, selection_model)
+     bad_sample = check_s$fail_bin
+     # if(bad_sample ==1) {
+     #     rstudio_para_cat(c("SAMPLING FAILURE: ", names(check_s$fail), " strata has ", as.character(check_s$fail), " sampled units\n"))
+     # }
+     #tracks the counts of units in each strata in the selection model
+     check_2 = check_sample_outcome(survey_sim, pop = cces, 
+                                    selection_model, interaction_cols = inter, interaction_cols_2 = inter_2)
+     check_nums = c(leq_5pp = check_2$bad,
+                    leq_1pp = check_2$v_bad, 
+                    fail = check_2$fail)
+     s = survey_sim %>% group_by(recode_pid_3way,recode_female, recode_age_bucket,
+                                 recode_educ_3way) %>% count() %>%
+         mutate(n_s = round(n/nrow(survey_sim), 3))
+     c = cces %>% group_by(recode_pid_3way, recode_female, recode_age_bucket,
+                           recode_educ_3way) %>% count() %>%
+         mutate(n_c = round(n/nrow(cces), 3))
+     count = nrow(c) - nrow(s)
+     
+     ############################################
+     ## Unweighted estimate
+     ############################################
+     
+     unweighted <- est_mean("outcome", survey_design)
+     ############################################
+     ## Sample size
+     ############################################\
+     n <- sum(sample) 
+     
+     ############################################
+     ## Raking on demographics (no education)
+     ############################################
+     rake_demos_noeduc_svyd <- try(calibrate(design = survey_design,
+                                         formula = formula_rake_demos_noeduc,
+                                         population = targets_rake_demos_noeduc,
+                                         calfun = "raking"), silent = T)
+     
+     rake_demos_noeduc <- tryCatch(est_mean("outcome", rake_demos_noeduc_svyd), error = function(e) NA)
+     
+     #SEs
+     if(class(rake_demos_noeduc_svyd)[1] == "try-error") {
+         rake_demos_noeduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY  = NA)
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_noeduc, outcome ~ .), 
+                                  data = rake_demos_noeduc_svyd$variables))
+ 
+         res_rake_demos_noeduc = data.frame(min = min(residuals), 
+                                            perc_25 = quantile(residuals, .25), 
+                                            mean = mean(residuals),
+                                            perc_75 = quantile(residuals, .75),
+                                            var = var(residuals))
+ 
+         rake_demos_noeduc_se <- calc_SEs(Y = rake_demos_noeduc_svyd$variables$outcome, 
+                                          residuals = residuals, 
+                                          pop_size = nrow(cces), 
+                                          sample_size = sum(sample),
+                                          weights = weights(rake_demos_noeduc_svyd))
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         
+         
+         rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY = data.frame(svymean(~outcome, 
+                                                                                             rake_demos_noeduc_svyd, 
+                                                                                             na.rm = TRUE))[1,2])
+         
+     }
+     ############################################
+     #### Raking on demographics (with education)
+     ############################################
+     
+     rake_demos_weduc_svyd <- try(calibrate(design = survey_design,
+                                        formula = formula_rake_demos_weduc,
+                                        population = targets_rake_demos_weduc,
+                                        calfun = "raking"), silent = T)
+ 
+     rake_demos_weduc <- tryCatch(est_mean("outcome", rake_demos_weduc_svyd), 
+                                 error = function(e) NA)
+     
+     
+     #SEs
+     if(class(rake_demos_weduc_svyd)[1] == "try-error") {
+         rake_demos_weduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_weduc, outcome ~ .), 
+                                  data = rake_demos_weduc_svyd$variables))
+         res_rake_demos_wedu = data.frame(min = min(residuals), 
+                                          perc_25 = quantile(residuals, .25), 
+                                          mean = mean(residuals),
+                                          perc_75 = quantile(residuals, .75),
+                                          var = var(residuals))
+         rake_demos_weduc_se <- calc_SEs(Y = rake_demos_weduc_svyd$variables$outcome, 
+                                         residuals = residuals, 
+                                         pop_size = nrow(cces),
+                                         sample_size = sum(sample),
+                                         weights = weights(rake_demos_weduc_svyd))
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     }
+     
+     ############################################
+     #### Raking on everything
+     ############################################
+     rake_all_svyd <- try(calibrate(design = survey_design,
+                                formula = formula_rake_all_vars,
+                                population = targets_rake_all_vars,
+                                calfun = "raking"))
+     
+     rake_all <- tryCatch(est_mean("outcome", rake_all_svyd), 
+                          error = function(e) NA)
+     #SEs
+     if(class(rake_all_svyd)[1] == "try-error") {
+         rake_all_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_all_vars, outcome ~ .), 
+                                  data = rake_all_svyd$variables))
+         res_rake_all = data.frame(min = min(residuals), 
+                                   perc_25 = quantile(residuals, .25), 
+                                   mean = mean(residuals),
+                                   perc_75 = quantile(residuals, .75),
+                                   var = var(residuals))
+         rake_all_se <- calc_SEs(Y = rake_all_svyd$variables$outcome, 
+                                 residuals = residuals, 
+                                 pop_size = nrow(cces), 
+                                 sample_size = sum(sample),
+                                 weights = weights(rake_all_svyd))
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     }
+     
+     ############################################
+     ## Post-stratification: Truth
+     ############################################
+     
+     #track empty cells:
+     #this subsets cces strata to only those in survey_sim
+     missing_strata <- unique(cces$strata)[!(unique(cces$strata) %in%
+                                                 unique(survey_sim$strata))]
+     cat(round(length(missing_strata)/ length(unique(cces$strata)),3),
+         "% cces original strata missing from sample, ",
+         " and", cces %>% filter(strata %in% missing_strata) %>% summarise(n()) %>% pull(), "/", nrow(cces), "units\n" )
+     dropped_cells = cces %>% filter(strata %in% missing_strata) %>% group_by(strata) %>% count()
+     #dropped_cells = data.frame(sum = sum(dropped_cells$n), strata = paste(dropped_cells$strata, collapse = " | "))
+     dropped_cells = sum(dropped_cells$n)
+     
+     post_stratification_svyd = svydesign(~1, data = postStrat(survey_sim, 
+                                                               cces_counts, "w", 
+                                                               strata_pass = "strata", 
+                                                               warn = F),
+                                          weights = ~w)
+     
+     post_stratification <- est_mean("outcome", post_stratification_svyd)
+     
+     #SEs can use svy pacakge
+     post_stratification_se <- data.frame(post_strat_SE_svy = data.frame(svymean(~outcome, post_stratification_svyd,
+                                                                                 na.rm = TRUE))[1,2])
+ 
+     
+     ############################################
+     #### Raking on true model
+     ############################################
+     #error catching just a stop gap to prevent from breaking sims run if truth doesnt converge
+     rake_truth_svyd <- try(calibrate(design = survey_design,
+                                      formula = selection_model,
+                                      population = targets_demo_truth,
+                                      maxit = 100,
+                                      calfun = "raking"), silent = T)
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+         rake_truth_svyd <- try(calibrate(design = survey_design,
+                                          formula = selection_model,
+                                          population = targets_demo_truth,
+                                          calfun = "raking",
+                                          maxit = 100,
+                                          epsilon = .009), silent = T)
+     }
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+     
+         rake_truth_se <- data.frame(SE_fixed = NA,
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA) 
+         names(rake_truth_se) = paste0("rake_truth_", names(rake_truth_se))
+         
+     } else {
+         lambdas <- 10^seq(3, -2, by = -.1)
+         x <- model.matrix(update(selection_model, outcome ~ .),
+                           data = rake_truth_svyd$variables)[, -1]
+         fit <- glmnet(x, 
+                       rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         cv_fit <- cv.glmnet(x, rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         opt_lambda <- cv_fit$lambda.min
+         fit <- cv_fit$glmnet.fit
+         
+         residuals = rake_truth_svyd$variables$outcome - predict(fit, s = opt_lambda, newx = x)
+         res_rake_truth = data.frame(min = min(residuals), 
+                                     perc_25 = quantile(residuals, .25), 
+                                     mean = mean(residuals),
+                                     perc_75 = quantile(residuals, .75),
+                                     var = var(residuals))
+         rake_truth_se <- tryCatch(calc_SEs(Y = rake_truth_svyd$variables$outcome,
+                                            residuals = residuals,
+                                            pop_size = nrow(cces),
+                                            sample_size = sum(sample),
+                                            weights = weights(rake_truth_svyd)), error = function(e) NA)
+         names(rake_truth_se) = paste0("rake_truth_", names(rake_truth_se))
+     }
+     
+     rake_truth <- tryCatch(est_mean("outcome", rake_truth_svyd), 
+                            error = function(e) NA)
+ 
+     #HT and Hayek
+     p_sample <- as.matrix((p_include[sample==1]))
+     
+     #HT
+     ht_truth = sum((cces[sample ==1, "outcome"]/p_sample))/nrow(cces) 
+     #Hayek
+     hayek_truth = sum((cces[sample ==1, "outcome"]/p_sample))/sum(1/p_sample)
+     
+     ############################################
+     ## Kpop: Categorical Data + b = argmax V(K)
+     ############################################
+     if(eval_kpop) {
+ 
+       # Select the covariates for use in Kbal: updated cat data no cont age
+       #one-hot coded for cat kernel
+       kbal_data <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                           recode_female,
+                                                           recode_race,
+                                                           recode_region,
+                                                           recode_pid_3way,
+                                                           recode_educ,
+                                                           recode_income_5way,
+                                                           recode_relig_6way,
+                                                           recode_born,
+                                                           recode_attndch_4way),
+                              cces %>% dplyr::select(recode_age_bucket,
+                                                     recode_female,
+                                                     recode_race,
+                                                     recode_region,
+                                                     recode_pid_3way,
+                                                     recode_educ,
+                                                     recode_income_5way,
+                                                     recode_relig_6way,
+                                                     recode_born,
+                                                     recode_attndch_4way))
+       
+       kbal_data_sampled <- c(rep(1, nrow(survey_sim)), rep(0, nrow(cces)))
+         
+         #### DEFAULT ######
+         #rstudio_para_cat(c("nsim: ", nsim, " DEFAULT"))
+         cat(paste("nsim:", nsim, "DEFAULT", "\n"))
+         kbal_est <- kbal(allx=kbal_data,
+                          sampled = kbal_data_sampled,
+                          cat_data = TRUE,
+                          incrementby = increment,
+                          meanfirst = FALSE,
+                          ebal.tol = tolerance,
+                          ebal.maxit = maxit,
+                          minnumdims = min_num_dims,
+                          maxnumdims = max_num_dims,
+                          sampledinpop = FALSE,
+                          fullSVD = TRUE)
+         
+         kpop_svyd <- svydesign(~1, data = survey_sim,
+                                weights = kbal_est$w[kbal_data_sampled ==1])
+         
+         kpop <- est_mean("outcome", kpop_svyd)
+         b_kpop = kbal_est$b
+         #save memory by saving only the svd to re use
+         svdK = kbal_est$svdK 
+         numdims = kbal_est$numdims
+         biasbound_r = kbal_est$biasbound_ratio
+         biasbound = kbal_est$biasbound_opt
+         
+         ##### Kpop SEs
+         kpop <- tryCatch(est_mean("outcome", kpop_svyd), error = function(e) NA)
+         
+         x <- as.matrix(data.frame(kbal_dims = kbal_est$svdK$v[, 1:kbal_est$numdims]))
+         cv_fit <- cv.glmnet(x, kpop_svyd$variables$outcome, alpha = 0)
+         lambda_pass = cv_fit$lambda.1se
+         
+         residuals = kpop_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                           s = lambda_pass, newx = x)
+         res_kpop = data.frame(min = min(residuals), 
+                               perc_25 = quantile(residuals, .25), 
+                               mean = mean(residuals),
+                               perc_75 = quantile(residuals, .75),
+                               var = var(residuals))
+         kpop_se <- tryCatch(calc_SEs(Y = kpop_svyd$variables$outcome,
+                                      residuals = residuals,
+                                      pop_size = nrow(cces),
+                                      sample_size = sum(sample),
+                                      weights = weights(kpop_svyd)), error = function(e) NA)
+         
+         if(length(kpop_se) == 1) {
+             kpop_se <- data.frame(SE_fixed = NA, 
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA)
+         }
+         names(kpop_se) = tryCatch(paste0("kpop_", names(kpop_se)), error = function(e) NA)
+         
+         #CONVERGED
+         dist_record = data.frame(t(kbal_est$dist_record))
+         min_converged = dist_record[which.min(dist_record[dist_record$Ebal.Convergence ==1,"BiasBound"]), "Dims"]
+         
+         rm(kbal_est, residuals, x, cv_fit)
+         
+         #CONVERGED
+         #### CONVG ####
+         cat(paste("nsim:", nsim, " CONV", "\n"))
+         if(is.null(min_converged) | length(min_converged) ==0) {
+           kpop_svyd_conv <- "dn converge"
+           kpop_conv <- "dn converge"
+           
+           numdims_conv = "dn converge"
+           biasbound_r_conv = "dn converge"
+           biasbound_conv = "dn converge"
+           kpop_conv_se = data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           res_kpop_conv = data.frame(min = NA, 
+                                      perc_25 = NA, 
+                                      mean =NA,
+                                      perc_75 = NA,
+                                      var = NA)
+           
+         } else {
+           kbal_est_conv <- kbal(allx=kbal_data,
+                                 K.svd = svdK,
+                                 sampled = kbal_data_sampled,
+                                 numdims = min_converged,
+                                 ebal.tol = tolerance,
+                                 ebal.maxit = maxit,
+                                 minnumdims = min_num_dims,
+                                 maxnumdims = max_num_dims,
+                                 scale_data = FALSE,
+                                 drop_MC = FALSE,
+                                 incrementby = increment,
+                                 meanfirst = FALSE,
+                                 sampledinpop = FALSE,
+                                 ebal.convergence = TRUE)
+           kpop_svyd_conv <- svydesign(~1, data = survey_sim,
+                                       weights = kbal_est_conv$w[kbal_data_sampled ==1])
+           kpop_conv <- est_mean("outcome", kpop_svyd_conv)
+           
+           numdims_conv = kbal_est_conv$numdims
+           biasbound_r_conv = kbal_est_conv$biasbound_ratio
+           biasbound_conv = kbal_est_conv$biasbound_opt
+           
+           #SEs
+           x <- as.matrix(data.frame(kbal_dims = kbal_est_conv$svdK$v[, 1:kbal_est_conv$numdims]))
+           cv_fit <- cv.glmnet(x, kpop_svyd_conv$variables$outcome, alpha = 0)
+           fit <- cv_fit$glmnet.fit
+           lambda_pass = cv_fit$lambda.1se
+           residuals = kpop_svyd_conv$variables$outcome - predict(cv_fit$glmnet.fit, 
+                                                                  s = lambda_pass, 
+                                                                            newx = x)
+           res_kpop_conv = data.frame(min = min(residuals), 
+                                      perc_25 = quantile(residuals, .25), 
+                                      mean = mean(residuals),
+                                      perc_75 = quantile(residuals, .75),
+                                      var = var(residuals))
+           kpop_conv_se <- tryCatch(calc_SEs(Y = kpop_svyd_conv$variables$outcome,
+                                        residuals = residuals,
+                                        pop_size = nrow(cces),
+                                        sample_size = sum(sample),
+                                        weights = weights(kpop_svyd_conv)), error = function(e) NA)
+           if(length(kpop_conv_se) == 1) {
+               kpop_conv_se <- data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           }
+           names(kpop_conv_se) = tryCatch(paste0("kpop_conv_", names(kpop_conv_se)), error = function(e) NA)
+          #KRLS SEs are exactly the same for coverged
+           rm(kbal_est_conv, residuals, x, cv_fit) 
+         }
+         
+         
+         ####### MF #######
+         #rstudio_para_cat(c("nsim: ", nsim, " + aMF"))
+         cat(paste("nsim:", nsim, "aMEANFIRST", "\n"))
+         
+         if(kpop_constraints) {
+             #########demos constraint method:
+             cat(paste("nsim:", nsim, "CONSTR (Demos)", "\n"))
+             kbal_demos_est <- kbal(K.svd = svdK,
+                                    allx=kbal_data,
+                                    cat_data = TRUE,
+                                    sampled = kbal_data_sampled,
+                                    ebal.tol = tolerance,
+                                    ebal.maxit = maxit,
+                                    minnumdims = min_num_dims,
+                                    maxnumdims = max_num_dims,
+                                    scale_data = FALSE,
+                                    drop_MC = FALSE,
+                                    incrementby = increment,
+                                    meanfirst = TRUE,
+                                    mf_columns = all.vars(formula_rake_demos_noeduc),
+                                    sampledinpop = FALSE)
+             
+             kpop_demos_svyd <- svydesign(~1, data = survey_sim, 
+                                          weights = kbal_demos_est$w[kbal_data_sampled ==1])
+             
+             kpop_demos <- est_mean("outcome", kpop_demos_svyd)
+             
+             numdims_demos = kbal_demos_est$numdims
+             mf_appended_dims_demos = kbal_demos_est$meanfirst_dims
+             if(is.null(numdims_demos)) {
+                 numdims_demos = c(NA) 
+                 kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                             SE_quasi = NA, 
+                                             SE_linear = NA, 
+                                             SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_demos_est$svdK$v[, c(1:kbal_demos_est$numdims)])
+                 X <- as.matrix(cbind(kbal_demos_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_demos_svyd$variables$outcome, alpha = 0, 
+                                     penalty.factor = c(rep(0, ncol(kbal_demos_est$appended_constraint_cols)), rep(1, kbal_demos_est$numdims)))
+                 
+                 lambda_pass = cv_fit$lambda.1se
+                 residuals =  kpop_demos_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                          s = lambda_pass, 
+                                                                          newx = X)
+                 res_kpop_demos = data.frame(min = min(residuals), 
+                                             perc_25 = quantile(residuals, .25), 
+                                             mean = mean(residuals),
+                                             perc_75 = quantile(residuals, .75),
+                                             var = var(residuals))
+                 
+                 kpop_demos_se <- tryCatch(calc_SEs(Y = kpop_demos_svyd$variables$outcome,
+                                                    residuals = residuals,
+                                                    pop_size = nrow(cces),
+                                                    sample_size = sum(sample),
+                                                    weights = weights(kpop_demos_svyd)), 
+                                           error = function(e) NA)
+                 if(length(kpop_demos_se) == 1) {
+                     kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                                 SE_quasi = NA, 
+                                                 SE_linear = NA, 
+                                                 SE_chad = NA)
+                 }
+                 names(kpop_demos_se) = tryCatch(paste0("kpop_demos_", names(kpop_demos_se)),
+                                                 error = function(e) NA)
+             }
+             biasbound_r_demos = kbal_demos_est$biasbound_ratio
+             biasbound_demos = kbal_demos_est$biasbound_opt
+             
+             rm(kbal_demos_est, residuals, X, V, cv_fit)
+             
+             
+             #########demos + educ constraint method:
+             cat(paste("nsim:", nsim, "CONSTR (D+Edu)", "\n"))
+             kbal_demos_wedu_est <- kbal(K.svd = svdK,
+                                         allx=kbal_data,
+                                         cat_data = TRUE,
+                                         sampled = kbal_data_sampled,
+                                         ebal.tol = tolerance,
+                                         ebal.maxit = maxit,
+                                         minnumdims = min_num_dims,
+                                         maxnumdims = max_num_dims,
+                                         scale_data = FALSE,
+                                         drop_MC = FALSE,
+                                         incrementby = increment,
+                                         meanfirst = TRUE,
+                                         mf_columns = all.vars(formula_rake_demos_weduc),
+                                         sampledinpop = FALSE)
+             kpop_demos_wedu_svyd <- svydesign(~1, data = survey_sim, 
+                                               weights = kbal_demos_wedu_est$w[kbal_data_sampled ==1])
+             
+             kpop_demos_wedu <- est_mean("outcome", kpop_demos_wedu_svyd)
+             
+             numdims_demos_wedu = kbal_demos_wedu_est$numdims
+             mf_appended_dims_demos_wedu = kbal_demos_wedu_est$meanfirst_dims
+             if(is.null(numdims_demos_wedu)) {
+                 numdims_demos_wedu = c(NA)
+                 kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                  SE_quasi = NA, 
+                                                  SE_linear = NA, 
+                                                  SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_demos_wedu_est$svdK$v[, c(1:kbal_demos_wedu_est$numdims)])
+                 X <- as.matrix(cbind(kbal_demos_wedu_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_demos_wedu_svyd$variables$outcome, alpha = 0,
+                                     penalty.factor = c(rep(0, ncol(kbal_demos_wedu_est$appended_constraint_cols)),
+                                                        rep(1, kbal_demos_wedu_est$numdims)))
+                 
+                 lambda_pass = cv_fit$lambda.1se
+                 residuals =  kpop_demos_wedu_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                               s = lambda_pass, 
+                                                                               newx = X)
+                 res_kpop_demos_wedu = data.frame(min = min(residuals), 
+                                                  perc_25 = quantile(residuals, .25), 
+                                                  mean = mean(residuals),
+                                                  perc_75 = quantile(residuals, .75),
+                                                  var = var(residuals))
+                 kpop_demos_wedu_se <- tryCatch(calc_SEs(Y = kpop_demos_wedu_svyd$variables$outcome,
+                                                         residuals = residuals,
+                                                         pop_size = nrow(cces),
+                                                         sample_size = sum(sample),
+                                                         weights = weights(kpop_demos_wedu_svyd)), 
+                                                error = function(e) NA)
+                 if(length(kpop_demos_wedu_se) == 1) {
+                     kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                      SE_quasi = NA, 
+                                                      SE_linear = NA, 
+                                                      SE_chad = NA)
+                 }
+                 names(kpop_demos_wedu_se) = tryCatch(paste0("kpop_demos_wedu_", names(kpop_demos_wedu_se)),
+                                                      error = function(e) NA)
+                 
+             }
+             biasbound_r_demos_wedu = kbal_demos_wedu_est$biasbound_ratio
+             biasbound_demos_wedu = kbal_demos_wedu_est$biasbound_opt
+             
+             rm(kbal_demos_wedu_est, residuals, X, V, cv_fit)
+             
+             
+             #########all constraint method:
+             cat(paste("nsim:", nsim, "CONSTR (All)", "\n"))
+             kbal_all_est <- kbal(K.svd = svdK,
+                                  allx=kbal_data,
+                                  cat_data = TRUE,
+                                  sampled = kbal_data_sampled,
+                                  ebal.tol = tolerance,
+                                  ebal.maxit = maxit,
+                                  minnumdims = min_num_dims,
+                                  maxnumdims = max_num_dims,
+                                  scale_data = FALSE,
+                                  drop_MC = FALSE,
+                                  incrementby = increment,
+                                  meanfirst = TRUE,
+                                  sampledinpop = FALSE)
+             kpop_all_svyd <- svydesign(~1, data = survey_sim, 
+                                        weights = kbal_all_est$w[kbal_data_sampled ==1])
+             
+             kpop_all <- est_mean("outcome", kpop_all_svyd)
+             
+             numdims_all = kbal_all_est$numdims
+             mf_appended_dims_all = kbal_all_est$meanfirst_dims
+             if(is.null(numdims_all)) {
+                 numdims_all = c(NA)
+                 numdims_all_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_all_est$svdK$v[, c(1:kbal_all_est$numdims)])
+                 X <- as.matrix(cbind(kbal_all_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_all_svyd$variables$outcome, alpha = 0, 
+                                     penalty.factor = c(rep(0, ncol(kbal_all_est$appended_constraint_cols)),
+                                                        rep(1, kbal_all_est$numdims)))
+                 
+                 lambda_pass = cv_fit$lambda.1se
+                 residuals =  kpop_all_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                        s = lambda_pass, 
+                                                                        newx = X)
+                 res_kpop_all = data.frame(min = min(residuals), 
+                                           perc_25 = quantile(residuals, .25), 
+                                           mean = mean(residuals),
+                                           perc_75 = quantile(residuals, .75),
+                                           var = var(residuals))
+                 kpop_all_se <- tryCatch(calc_SEs(Y = kpop_all_svyd$variables$outcome,
+                                                  residuals = residuals,
+                                                  pop_size = nrow(cces),
+                                                  sample_size = sum(sample),
+                                                  weights = weights(kpop_all_svyd)), 
+                                         error = function(e) NA)
+                 if(length(kpop_demos_wedu_se) == 1) {
+                     kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                      SE_quasi = NA, 
+                                                      SE_linear = NA, 
+                                                      SE_chad = NA)
+                 }
+                 names(kpop_all_se) = tryCatch(paste0("kpop_all_", names(kpop_all_se)),
+                                               error = function(e) NA)
+                 
+             }
+             
+             biasbound_r_all = kbal_all_est$biasbound_ratio
+             biasbound_all = kbal_all_est$biasbound_opt
+             
+             rm(kbal_all_est, residuals, X,V, cv_fit)
+         } else {
+             kpop_demos <- NA
+             numdims_demos = c(NA) 
+             kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                         SE_quasi = NA, 
+                                         SE_linear = NA, 
+                                         SE_chad = NA)
+             res_kpop_demos = data.frame(min = NA, 
+                                       perc_25 = NA, 
+                                       mean = NA,
+                                       perc_75 = NA,
+                                       var = NA)
+             biasbound_r_demos = NA
+             biasbound_demos = NA
+         
+             kpop_demos_wedu <- NA
+             numdims_demos_wedu = c(NA) 
+             kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             res_kpop_demos_wedu =  data.frame(min = NA, 
+                                               perc_25 = NA, 
+                                               mean = NA,
+                                               perc_75 = NA,
+                                               var = NA)
+             biasbound_r_demos_wedu = NA
+             biasbound_demos_wedu = NA
+             
+             kpop_all <- NA
+             numdims_all = c(NA) 
+             kpop_all_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             res_kpop_all =  data.frame(min = NA, 
+                                               perc_25 = NA, 
+                                               mean = NA,
+                                               perc_75 = NA,
+                                               var = NA)
+             biasbound_r_all = NA
+             biasbound_all = NA
+             
+             #for weights
+             kpop_demos_svyd = survey_design
+             kpop_demos_wedu_svyd = survey_design
+             kpop_all_svyd = survey_design
+         }
+         
+         
+         rm(svdK)
+   
+          ##### return
+         kpop_res = list()
+         kpop_res$sims = data.frame(b_out = b_kpop,
+                               kpop = kpop,
+                               kpop_conv = kpop_conv,
+                               kpop_demos = kpop_demos,
+                               kpop_demos_wedu = kpop_demos_wedu,
+                               kpop_all = kpop_all,
+                               bb = biasbound,
+                               bbr = biasbound_r,
+                               bb_conv = biasbound_conv,
+                               bbr_conv = biasbound_r_conv,
+                               bb_demos = biasbound_demos,
+                               bbr_demos = biasbound_r_demos,
+                               bb_demos_wedu = biasbound_demos_wedu,
+                               bbr_demos_wedu = biasbound_r_demos_wedu,
+                               bb_all = biasbound_all,
+                               bbr_all = biasbound_r_all,
+                               numdims,
+                               numdims_conv,
+                               numdims_demos,
+                               numdims_demos_wedu,
+                               numdims_all,
+                               mf_appended_dims_demos, 
+                               mf_appended_dims_demos_wedu, 
+                               mf_appended_dims_all)
+         
+         #Standard Errors:
+         kpop_res$SEs = data.frame(rake_demos_noeduc_se,
+                                   rake_demos_noeduc_se_SVY,
+                                   rake_demos_weduc_se,
+                                   rake_all_se,
+                                   post_stratification_se,
+                                   rake_truth_se,
+                                   kpop_se,
+                                   kpop_conv_se,
+                                   kpop_demos_se,
+                                   kpop_demos_wedu_se,
+                                   kpop_all_se)
+         
+         #weights
+         kpop_res$weights = list(b = b_kpop,
+                                 kpop_w = weights(kpop_svyd),
+                                 kpop_w_conv = weights(kpop_svyd_conv),
+                                 kpop_demos_w = weights(kpop_demos_svyd),
+                                 kpop_demos_wedu_w = weights(kpop_demos_wedu_svyd),
+                                 kpop_all_w = weights(kpop_all_svyd))
+         
+         #residuals
+         kpop_res$residuals = rbind(b = b_kpop,
+                                    kpop = res_kpop,
+                                    kpop_conv = res_kpop_conv,
+                                    kpop_demos = res_kpop_demos,
+                                    kpop_demos_wedu = res_kpop_demos_wedu,
+                                    kpop_all = res_kpop_all,
+                                    rake_truth = res_rake_truth,
+                                    rake_demos = res_rake_demos_noeduc,
+                                    rake_demos_wedu = res_rake_demos_wedu,
+                                    rake_all = res_rake_all)
+        
+         rm(kpop_svyd, 
+            kpop_svyd_conv,
+            kpop_demos_svyd,
+            kpop_demos_wedu_svyd, kpop_all_svyd)
+       
+     }
+     
+     ############################################ OUTPUT
+     out = list()
+     if(eval_kpop) {
+       out$sims = cbind(nsim,
+                        n,
+                        unweighted,
+                        rake_demos_noeduc,
+                        rake_demos_weduc,
+                        rake_all,
+                        post_stratification,
+                        rake_truth,
+                        ht_truth, 
+                        hayek_truth,
+                        kpop_res$sims)
+       
+       out$SEs = kpop_res$SEs
+       out$weights = kpop_res$weights
+       out$residuals = kpop_res$residuals
+       out$dropped_cells = c(dropped_cells = dropped_cells)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      #chaningn temporarily to the fuller view from check_nums
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+       
+     } else {
+         
+     
+       out$sims = data.frame(nsim,
+                             n,
+                             unweighted,
+                             rake_demos_noeduc,
+                             rake_demos_weduc,
+                             rake_all,
+                             post_stratification,
+                             rake_truth,
+                             ht_truth, 
+                             hayek_truth)
+       
+       out$SEs = data.frame(rake_demos_noeduc_se,
+                            rake_demos_weduc_se,
+                            rake_demos_noeduc_se_SVY,
+                            rake_all_se,
+                            rake_truth_se,
+                            post_stratification_se)
+                            
+       
+       out$residuals = rbind(b = NULL,
+                             rake_truth = res_rake_truth,
+                             rake_demos = res_rake_demos_noeduc,
+                             rake_demos_wedu = res_rake_demos_wedu ,
+                             rake_all = res_rake_all
+       )
+       
+       out$dropped_cells = c(dropped_cells = dropped_cells)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+     } 
+     
+     
+     })
+     return(out)
+     
+   }, mc.cores = detectCores() - cores_saved) 
+   
+ })
=====================  SIM: 1 ===================== 
=====================  SIM: 5 ===================== 
=====================  SIM: 3 ===================== 
=====================  SIM: 2 ===================== 
=====================  SIM: 4 ===================== 
=====================  SIM: 6 ===================== 
=====================  SIM: 7 ===================== 
=====================  SIM: 8 ===================== 
=====================  SIM: 9 ===================== 
0.654 % cces original strata missing from sample,   and 9350 / 44932 units
0.649 % cces original strata missing from sample,   and 10089 / 44932 units
0.66 % cces original strata missing from sample,   and 9966 / 44932 units
0.656 % cces original strata missing from sample,   and 9118 / 44932 units
0.681 % cces original strata missing from sample,   and 11085 / 44932 units
0.647 % cces original strata missing from sample,   and 10639 / 44932 units
0.645 % cces original strata missing from sample,   and 9398 / 44932 units
0.697 % cces original strata missing from sample,   and 11108 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
0.676 % cces original strata missing from sample,   and 10201 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
nsim: 4 DEFAULT 
nsim: 1 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 9 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 3 DEFAULT 
nsim: 8 DEFAULT 
nsim: 2 DEFAULT 
nsim: 5 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 7 DEFAULT 
nsim: 6 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: 5.537 selected 
Building kernel matrix
5.566 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
5.53 selected 
Building kernel matrix
5.539 selected 
Building kernel matrix
5.561 selected 
Building kernel matrix
5.577 selected 
Building kernel matrix
5.546 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
5.552 selected 
5.554 selected 
Building kernel matrix
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02217 and the L1 discrepancy is 0.028 
Without balancing, biasbound (norm=1) is 0.02151 and the L1 discrepancy is 0.026 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02268  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02233  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01467  
Without balancing, biasbound (norm=1) is 0.01808 and the L1 discrepancy is 0.018 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01261  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01368  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01813  
Without balancing, biasbound (norm=1) is 0.02809 and the L1 discrepancy is 0.038 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01116  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01538  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02721  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01154  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01337  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01693  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01316  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01106  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01025  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00773  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00843  
Without balancing, biasbound (norm=1) is 0.02233 and the L1 discrepancy is 0.025 
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.027 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02369  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02263  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0177  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01621  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01119  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01274  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
Without balancing, biasbound (norm=1) is 0.02408 and the L1 discrepancy is 0.029 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02448  
Without balancing, biasbound (norm=1) is 0.02683 and the L1 discrepancy is 0.034 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01149  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01632  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02744  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.009  
Without balancing, biasbound (norm=1) is 0.02392 and the L1 discrepancy is 0.029 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01715  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.012  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01558  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02462  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With With 16 dimensions of K, ebalance convergence is 41 dimensions of K, ebalance convergence isTRUE yielding biasbound (norm=1) of  TRUE 0.00939 yielding biasbound (norm=1) of 0.00922  
 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01517  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0135  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01351  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00864  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00925  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00748  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00773  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00857  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00976  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00926  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00932  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00892  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00971  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00874  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00888  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0093  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00977  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00852  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00957  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00777  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
nsim: 5  CONV 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00956  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00956  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01056  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00883  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
nsim: 9  CONV 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00919  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00896  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00992  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00939  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
nsim: 1  CONV 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00891  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00915  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.027 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00885  
With user-specified 26 dimensions, biasbound (norm=1) of  0.0076  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00833  
nsim: 5 aMEANFIRST 
nsim: 5 CONSTR (Demos) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00983  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00877  
nsim: 2  CONV 
Without balancing, biasbound (norm=1) is 0.02151 and the L1 discrepancy is 0.026 
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00823  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00926  
nsim: 9 aMEANFIRST 
nsim: 9 CONSTR (Demos) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01298  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00909  
Without balancing, biasbound (norm=1) is 0.01808 and the L1 discrepancy is 0.018 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00904  
nsim: 7  CONV 
With user-specified 31 dimensions, biasbound (norm=1) of  0.00772  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00789  
nsim: 1 aMEANFIRST 
nsim: 1 CONSTR (Demos) 
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00946  
Without balancing, biasbound (norm=1) is 0.02217 and the L1 discrepancy is 0.028 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00904  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00919  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00785  
nsim: 2 aMEANFIRST 
nsim: 2 CONSTR (Demos) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00811  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01013  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.027 
Without balancing, biasbound (norm=1) is 0.02809 and the L1 discrepancy is 0.038 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01531  
nsim: 6  CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00963  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01051  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00999  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With user-specified 46 dimensions, biasbound (norm=1) of  0.00887  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00889  
nsim: 7 aMEANFIRST 
nsim: 7 CONSTR (Demos) 
Without balancing, biasbound (norm=1) is 0.02151 and the L1 discrepancy is 0.026 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01195  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
Without balancing, biasbound (norm=1) is 0.01808 and the L1 discrepancy is 0.018 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01293  
nsim: 4  CONV 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01153  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00864  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0111  
Without balancing, biasbound (norm=1) is 0.02217 and the L1 discrepancy is 0.028 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0154  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0111  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
WithWith  41111  dimensions of K, ebalance convergence is dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896 FALSE yielding biasbound (norm=1) of 0.01092  
 
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01117  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00954  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
Without balancing, biasbound (norm=1) is 0.02392 and the L1 discrepancy is 0.029 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
nsim: 3  CONV 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01005  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00791  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00974  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
Without balancing, biasbound (norm=1) is 0.02233 and the L1 discrepancy is 0.025 
nsim: 6 aMEANFIRST 
nsim: 6 CONSTR (Demos) 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00949  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With user-specified 41 dimensions, biasbound (norm=1) of  0.00745  
Without balancing, biasbound (norm=1) is 0.02809 and the L1 discrepancy is 0.038 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00967  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02309  
nsim: 4 aMEANFIRST 
nsim: 4 CONSTR (Demos) 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01032  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01162  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00997  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00969  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 51 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01028  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02683 and the L1 discrepancy is 0.034 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01012  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01032  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00791  
nsim: 3 aMEANFIRST 
nsim: 3 CONSTR (Demos) 
nsim: 8  CONV 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01066  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01107  
Re-running at optimal choice of numdims, 11 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
Without balancing, biasbound (norm=1) is 0.02392 and the L1 discrepancy is 0.029 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00936  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01469  
Used 13 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
nsim: 9 CONSTR (D+Edu) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01223  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00918  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01022  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01063  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
Without balancing, biasbound (norm=1) is 0.02233 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00957  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01001  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0108  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00973  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
Without balancing, biasbound (norm=1) is 0.02408 and the L1 discrepancy is 0.029 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00773  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01132  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01049  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01044  
With user-specified 51 dimensions, biasbound (norm=1) of  0.00773  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
nsim: 8 aMEANFIRST 
nsim: 8 CONSTR (Demos) 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01  
Without balancing, biasbound (norm=1) is 0.02683 and the L1 discrepancy is 0.034 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02149  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01169  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0127  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01067  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01077  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01094  
Re-running at optimal choice of numdims, 26 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01027  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
Without balancing, biasbound (norm=1) is 0.02151 and the L1 discrepancy is 0.026 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01134  
nsim: 1 CONSTR (D+Edu) 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01076  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0113  
Re-running at optimal choice of numdims, 26 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
nsim: 2 CONSTR (D+Edu) 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01098  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00978  
Without balancing, biasbound (norm=1) is 0.02408 and the L1 discrepancy is 0.029 
WithWith  36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00967  
86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01055  
Re-running at optimal choice of numdims, 26 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01622  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01132  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00995  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00949  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
nsim: 5 CONSTR (D+Edu) 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00962  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01003  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01181  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01004  
Without balancing, biasbound (norm=1) is 0.01808 and the L1 discrepancy is 0.018 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00989  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00876  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00878  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01259  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
Without balancing, biasbound (norm=1) is 0.02217 and the L1 discrepancy is 0.028 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01396  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00865  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01032  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.027 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01078  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01414  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01285  
Re-running at optimal choice of numdims, 21 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00972  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00876  
Used 13 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01104  
Re-running at optimal choice of numdims, 16 
nsim: 7 CONSTR (D+Edu) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00996  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
nsim: 2 CONSTR (All) 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00945  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00963  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
Re-running at optimal choice of numdims, 6 
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00981  
nsim: 5 CONSTR (All) 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01015  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00942  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00903  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00993  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0117  
Re-running at optimal choice of numdims, 21 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01074  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
nsim: 6 CONSTR (D+Edu) 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00931  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01028  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00946  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01131  
Re-running at optimal choice of numdims, 11 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01065  
Without balancing, biasbound (norm=1) is 0.02809 and the L1 discrepancy is 0.038 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00882  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02334  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00964  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00964  
Used 17 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
nsim: 9 CONSTR (All) 
Without balancing, biasbound (norm=1) is 0.02217 and the L1 discrepancy is 0.028 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01046  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01008  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00862  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01046  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01132  
Re-running at optimal choice of numdims, 11 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01059  
Used 17 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
nsim: 1 CONSTR (All) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01126  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01074  
Re-running at optimal choice of numdims, 31 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01045  
Re-running at optimal choice of numdims, 26 
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0109  
Re-running at optimal choice of numdims, 1 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01158  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.027 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00885  
=====================  SIM: 11 ===================== 
0.642 % cces original strata missing from sample,   and 9369 / 44932 units
Joining with `by = join_by(strata)`
Without balancing, biasbound (norm=1) is 0.02392 and the L1 discrepancy is 0.029 
nsim: 11 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
nsim: 4 CONSTR (D+Edu) 
Used 13 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01264  
Searching for b value which maximizes the variance in K: With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01134  
nsim: 3 CONSTR (D+Edu) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
5.574 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01073  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00983  
Re-running at optimal choice of numdims, 1 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0092  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
Selected 35 dimensions of "allx" to use as mean balance constraints. 
Selected 19 dimensions of "allx" to use as mean balance constraints. 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 14 ===================== 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
0.67 % cces original strata missing from sample,   and 9790 / 44932 units
Joining with `by = join_by(strata)`
nsim: 14 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01108  
Searching for b value which maximizes the variance in K: With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
5.518 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01169  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
Without balancing, biasbound (norm=1) is 0.02151 and the L1 discrepancy is 0.026 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01065  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01245  
Re-running at optimal choice of numdims, 6 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01004  
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
nsim: 7 CONSTR (All) 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
Re-running at optimal choice of numdims, 1 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 18 ===================== 
Without balancing, biasbound (norm=1) is 0.01808 and the L1 discrepancy is 0.018 
Without balancing, biasbound (norm=1) is 0.02233 and the L1 discrepancy is 0.025 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01186  
Re-running at optimal choice of numdims, 66 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00773  
0.656 % cces original strata missing from sample,   and 9477 / 44932 units
Joining with `by = join_by(strata)`
nsim: 18 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00949  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0117  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
Searching for b value which maximizes the variance in K: Without balancing, biasbound (norm=1) is 0.02683 and the L1 discrepancy is 0.034 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
Without balancing, biasbound (norm=1) is 0.02272 and the L1 discrepancy is 0.03 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
Re-running at optimal choice of numdims, 1 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
Used 35 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02018  
=====================  SIM: 10 ===================== 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02325  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01273  
0.667 % cces original strata missing from sample,   and 10783 / 44932 units
Joining with `by = join_by(strata)`
nsim: 10 DEFAULT 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01027  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00991  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
5.56 selected 
Building kernel matrix
Searching for b value which maximizes the variance in K: Running full SVD on kernel matrix 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01068  
Re-running at optimal choice of numdims, 21 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
5.555 selected 
Building kernel matrix
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
Running full SVD on kernel matrix 
Used 12 dimensions of "allx" for mean balancing, and an additional 66 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
nsim: 8 CONSTR (D+Edu) 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
Used 17 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
nsim: 6 CONSTR (All) 
Without balancing, biasbound (norm=1) is 0.02557 and the L1 discrepancy is 0.035 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02492  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0158  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01135  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01097  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00818  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00966  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00819  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00866  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
nsim: 14  CONV 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00945  
Without balancing, biasbound (norm=1) is 0.02809 and the L1 discrepancy is 0.038 
Selected 32 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00989  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.009  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01077  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01168  
Re-running at optimal choice of numdims, 1 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
Without balancing, biasbound (norm=1) is 0.02636 and the L1 discrepancy is 0.031 
Without balancing, biasbound (norm=1) is 0.02408 and the L1 discrepancy is 0.029 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02842  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01499  
Without balancing, biasbound (norm=1) is 0.02557 and the L1 discrepancy is 0.035 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00962  
=====================  SIM: 16 ===================== 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01969  
Without balancing, biasbound (norm=1) is 0.02385 and the L1 discrepancy is 0.029 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00818  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01489  
nsim: 14 aMEANFIRST 
nsim: 14 CONSTR (Demos) 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0115  
Re-running at optimal choice of numdims, 21 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01337  
0.67 % cces original strata missing from sample,   and 10625 / 44932 units
Joining with `by = join_by(strata)`
nsim: 16 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.024  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01544  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00819  
Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01236  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
Without balancing, biasbound (norm=1) is 0.02392 and the L1 discrepancy is 0.029 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01132  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01014  
Re-running at optimal choice of numdims, 41 
Used 19 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
nsim: 3 CONSTR (All) 
5.521 selected 
Building kernel matrix
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
Used 18 dimensions of "allx" for mean balancing, and an additional 41 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
nsim: 4 CONSTR (All) 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01086  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00961  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0098  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
Re-running at optimal choice of numdims, 1 
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01051  
Used 32 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 15 ===================== 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
Without balancing, biasbound (norm=1) is 0.02557 and the L1 discrepancy is 0.035 
0.677 % cces original strata missing from sample,   and 10621 / 44932 units
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01828  
Joining with `by = join_by(strata)`
nsim: 15 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00985  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01116  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01045  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01049  
Searching for b value which maximizes the variance in K: Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00983  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00813  
5.527 selected 
Building kernel matrix
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01018  
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00972  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01227  
Re-running at optimal choice of numdims, 21 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01067  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00838  
Used 13 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
nsim: 14 CONSTR (D+Edu) 
Without balancing, biasbound (norm=1) is 0.02374 and the L1 discrepancy is 0.027 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02417  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01575  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01083  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01184  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01058  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00855  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01119  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
Without balancing, biasbound (norm=1) is 0.02233 and the L1 discrepancy is 0.025 
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
nsim: 10  CONV 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00858  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0097  
Without balancing, biasbound (norm=1) is 0.02683 and the L1 discrepancy is 0.034 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00873  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 6 dimensions of K, ebalance convergence is With TRUE yielding biasbound (norm=1) of51  dimensions of K, ebalance convergence is 0.00879 TRUE 
 yielding biasbound (norm=1) of 0.00779  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00959  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00908  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00901  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
Without balancing, biasbound (norm=1) is 0.02902 and the L1 discrepancy is 0.038 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02801  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01481  
Without balancing, biasbound (norm=1) is 0.02636 and the L1 discrepancy is 0.031 
Without balancing, biasbound (norm=1) is 0.02557 and the L1 discrepancy is 0.035 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0118  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0176  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01056  
Re-running at optimal choice of numdims, 1 
=====================  SIM: 13 ===================== 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01076  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01148  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00872  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
nsim: 10 aMEANFIRST 
nsim: 10 CONSTR (Demos) 
0.652 % cces original strata missing from sample,   and 8883 / 44932 units
Joining with `by = join_by(strata)`
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
nsim: 13 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01016  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00934  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 66 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00975  
Searching for b value which maximizes the variance in K: =====================  SIM: 12 ===================== 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
5.541 selected 
Building kernel matrix
0.676 % cces original strata missing from sample,   and 10644 / 44932 units
Running full SVD on kernel matrix 
Joining with `by = join_by(strata)`
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00966  
nsim: 12 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00767  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
Searching for b value which maximizes the variance in K: With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01086  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01332  
Re-running at optimal choice of numdims, 16 
5.559 selected 
Building kernel matrix
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Running full SVD on kernel matrix 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
nsim: 11  CONV 
Used 18 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 14 CONSTR (All) 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00978  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01008  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01035  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01055  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
nsim: 18  CONV 
Without balancing, biasbound (norm=1) is 0.02636 and the L1 discrepancy is 0.031 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01644  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01168  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01094  
Re-running at optimal choice of numdims, 6 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01005  
Without balancing, biasbound (norm=1) is 0.02272 and the L1 discrepancy is 0.03 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 8 CONSTR (All) 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01074  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With user-specified 66 dimensions, biasbound (norm=1) of  0.00769  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01121  
nsim: 11 aMEANFIRST 
nsim: 11 CONSTR (Demos) 
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01223  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
Re-running at optimal choice of numdims, 16 
Used 12 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 10 CONSTR (D+Edu) 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02255 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02333  
Without balancing, biasbound (norm=1) is 0.02385 and the L1 discrepancy is 0.029 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01367  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01116  
Without balancing, biasbound (norm=1) is 0.02538 and the L1 discrepancy is 0.03 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00778  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00841  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02572  
Without balancing, biasbound (norm=1) is 0.02557 and the L1 discrepancy is 0.035 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01752  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
nsim: 18 aMEANFIRST 
nsim: 18 CONSTR (Demos) 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01519  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01275  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01074  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01043  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01124  
Re-running at optimal choice of numdims, 1 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01072  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.02272 and the L1 discrepancy is 0.03 
=====================  SIM: 23 ===================== 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01678  
0.661 % cces original strata missing from sample,   and 9925 / 44932 units
Joining with `by = join_by(strata)`
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00819  
nsim: 23 DEFAULT 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00966  
Searching for b value which maximizes the variance in K: With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01111  
5.559 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01085  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00944  
Without balancing, biasbound (norm=1) is 0.02408 and the L1 discrepancy is 0.029 
Without balancing, biasbound (norm=1) is 0.02636 and the L1 discrepancy is 0.031 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01582  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01066  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00917  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01043  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00991  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00906  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01047  
Without balancing, biasbound (norm=1) is 0.02385 and the L1 discrepancy is 0.029 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01095  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01565  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01055  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0115  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01051  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01061  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 66 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01285  
Re-running at optimal choice of numdims, 16 
=====================  SIM: 17 ===================== 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00991  
0.64 % cces original strata missing from sample,   and 9697 / 44932 units
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
Joining with `by = join_by(strata)`
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 17 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 10 CONSTR (All) 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
Searching for b value which maximizes the variance in K: With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01063  
5.566 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02152 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0225  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00979  
nsim: 16  CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01372  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01219  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01164  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00997  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01222  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00844  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00885  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00905  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01042  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
WithWith  51 dimensions of K, ebalance convergence is71  TRUE dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of yielding biasbound (norm=1) of 0.01245  
0.00884  
Without balancing, biasbound (norm=1) is 0.02374 and the L1 discrepancy is 0.027 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00876  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00983  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00885  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00858  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01326  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00881  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
nsim: 12  CONV 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0096  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00858  
Without balancing, biasbound (norm=1) is 0.02279 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02323  
Without balancing, biasbound (norm=1) is 0.02636 and the L1 discrepancy is 0.031 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01538  
With user-specified 66 dimensions, biasbound (norm=1) of  0.00757  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.009  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00947  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00767  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
nsim: 16 aMEANFIRST 
nsim: 16 CONSTR (Demos) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00755  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01094  
Re-running at optimal choice of numdims, 1 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01053  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00896  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
=====================  SIM: 19 ===================== 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00865  
0.654 % cces original strata missing from sample,   and 8553 / 44932 units
Joining with `by = join_by(strata)`
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
nsim: 19 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.02538 and the L1 discrepancy is 0.03 
Searching for b value which maximizes the variance in K: With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00765  
5.586 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01096  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00931  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With user-specified 26 dimensions, biasbound (norm=1) of  0.01043  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0097  
nsim: 12 aMEANFIRST 
nsim: 12 CONSTR (Demos) 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00872  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01153  
Re-running at optimal choice of numdims, 26 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
nsim: 18 CONSTR (D+Edu) 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00963  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00912  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00921  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01007  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00985  
Without balancing, biasbound (norm=1) is 0.02516 and the L1 discrepancy is 0.037 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02545  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00794  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01476  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01219  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01054  
Without balancing, biasbound (norm=1) is 0.02374 and the L1 discrepancy is 0.027 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00997  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00949  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01616  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00764  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01013  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01128  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
Without balancing, biasbound (norm=1) is 0.02538 and the L1 discrepancy is 0.03 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01115  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01042  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0206  
Without balancing, biasbound (norm=1) is 0.02385 and the L1 discrepancy is 0.029 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01351  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01027  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 61 
nsim: 23  CONV 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01099  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0082  
nsim: 15  CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01117  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01098  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01071  
Re-running at optimal choice of numdims, 66 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00949  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
nsim: 13  CONV 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01081  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00897  
Without balancing, biasbound (norm=1) is 0.02152 and the L1 discrepancy is 0.025 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01183  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
Without balancing, biasbound (norm=1) is 0.02902 and the L1 discrepancy is 0.038 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With user-specified 36 dimensions, biasbound (norm=1) of  0.00872  
nsim: 23 aMEANFIRST 
nsim: 23 CONSTR (Demos) 
Used 12 dimensions of "allx" for mean balancing, and an additional 66 dimensions of "K" from kernel balancing.
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01269  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00828  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
nsim: 11 CONSTR (D+Edu) 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00922  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 66 
Without balancing, biasbound (norm=1) is 0.02255 and the L1 discrepancy is 0.028 
nsim: 15 aMEANFIRST 
nsim: 15 CONSTR (Demos) 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0127  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01292  
nsim: 17  CONV 
With user-specified 61 dimensions, biasbound (norm=1) of  0.00787  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
nsim: 13 aMEANFIRST 
nsim: 13 CONSTR (Demos) 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
Without balancing, biasbound (norm=1) is 0.02152 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01691  
Selected 20 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0108  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0098  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00907  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00936  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01014  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01257  
Without balancing, biasbound (norm=1) is 0.02279 and the L1 discrepancy is 0.028 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
Without balancing, biasbound (norm=1) is 0.02272 and the L1 discrepancy is 0.03 
With user-specified 66 dimensions, biasbound (norm=1) of  0.0073  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01527  
nsim: 17 aMEANFIRST 
nsim: 17 CONSTR (Demos) 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00953  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01019  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01173  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00994  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00876  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00992  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0129  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01064  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
Without balancing, biasbound (norm=1) is 0.02279 and the L1 discrepancy is 0.028 
Without balancing, biasbound (norm=1) is 0.02902 and the L1 discrepancy is 0.038 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01565  
Without balancing, biasbound (norm=1) is 0.02255 and the L1 discrepancy is 0.028 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01831  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02104  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01313  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01106  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.009  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01031  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00966  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00998  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01169  
Re-running at optimal choice of numdims, 11 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0139  
Re-running at optimal choice of numdims, 26 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00944  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01037  
Used 17 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
nsim: 18 CONSTR (All) 
nsim: 12 CONSTR (D+Edu) 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00974  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01008  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01257  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 21 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
nsim: 19  CONV 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00938  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01018  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0098  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00912  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00946  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00819  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00904  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01077  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00982  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00934  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0108  
Without balancing, biasbound (norm=1) is 0.02516 and the L1 discrepancy is 0.037 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With user-specified 21 dimensions, biasbound (norm=1) of  0.00835  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00922  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01054  
nsim: 19 aMEANFIRST 
nsim: 19 CONSTR (Demos) 
Without balancing, biasbound (norm=1) is 0.02538 and the L1 discrepancy is 0.03 
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00995  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01053  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01974  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01098  
Without balancing, biasbound (norm=1) is 0.02385 and the L1 discrepancy is 0.029 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01151  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00864  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01062  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00966  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01128  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00991  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01129  
Re-running at optimal choice of numdims, 6 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01164  
Re-running at optimal choice of numdims, 61 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01136  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
Re-running at optimal choice of numdims, 1 
Used 13 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01019  
nsim: 15 CONSTR (D+Edu) 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01193  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 27 ===================== 
0.67 % cces original strata missing from sample,   and 10366 / 44932 units
Joining with `by = join_by(strata)`
nsim: 27 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01275  
Searching for b value which maximizes the variance in K: With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01094  
Re-running at optimal choice of numdims, 21 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01082  
Re-running at optimal choice of numdims, 31 
5.546 selected 
Building kernel matrix
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
Running full SVD on kernel matrix 
Used 12 dimensions of "allx" for mean balancing, and an additional 61 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01214  
Without balancing, biasbound (norm=1) is 0.02516 and the L1 discrepancy is 0.037 
nsim: 23 CONSTR (D+Edu) 
Used 20 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02027  
nsim: 11 CONSTR (All) 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01188  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01058  
Used 13 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 17 CONSTR (D+Edu) 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01229  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01039  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
Re-running at optimal choice of numdims, 61 
Without balancing, biasbound (norm=1) is 0.02195 and the L1 discrepancy is 0.024 
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0225  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01634  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01356  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
WithSelected  16 17 dimensions of K, ebalance convergence is dimensions of "allx" to use as mean balance constraints. 
TRUE yielding biasbound (norm=1) of 0.01051  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0126  
Used 13 dimensions of "allx" for mean balancing, and an additional 61 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01047  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
nsim: 13 CONSTR (D+Edu) 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01117  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01102  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01117  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01261  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
Without balancing, biasbound (norm=1) is 0.02272 and the L1 discrepancy is 0.03 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01123  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
Without balancing, biasbound (norm=1) is 0.02152 and the L1 discrepancy is 0.025 
Without balancing, biasbound (norm=1) is 0.02279 and the L1 discrepancy is 0.028 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01267  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01104  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01429  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01443  
Without balancing, biasbound (norm=1) is 0.02902 and the L1 discrepancy is 0.038 
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01158  
Re-running at optimal choice of numdims, 66 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01056  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02001  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00925  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00831  
Used 13 dimensions of "allx" for mean balancing, and an additional 66 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01008  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00977  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01426  
nsim: 16 CONSTR (D+Edu) 
Re-running at optimal choice of numdims, 11 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01041  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0083  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01064  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
Used 18 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00962  
nsim: 12 CONSTR (All) 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01081  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00875  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01124  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0104  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0098  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01026  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01013  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01017  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01059  
Without balancing, biasbound (norm=1) is 0.02255 and the L1 discrepancy is 0.028 
Selected 19 dimensions of "allx" to use as mean balance constraints. 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00891  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00888  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01034  
Re-running at optimal choice of numdims, 11 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01043  
Re-running at optimal choice of numdims, 6 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00995  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01614  
Used 18 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
nsim: 17 CONSTR (All) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00946  
Used 18 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
nsim: 15 CONSTR (All) 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01142  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00987  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00939  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00964  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00876  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01003  
Without balancing, biasbound (norm=1) is 0.02374 and the L1 discrepancy is 0.027 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01183  
Re-running at optimal choice of numdims, 11 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01446  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00976  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01068  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
Re-running at optimal choice of numdims, 1 
Used 12 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
Selected 31 dimensions of "allx" to use as mean balance constraints. 
nsim: 19 CONSTR (D+Edu) 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01105  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00917  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01051  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00906  
=====================  SIM: 20 ===================== 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
0.668 % cces original strata missing from sample,   and 10488 / 44932 units
Joining with `by = join_by(strata)`
nsim: 20 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01206  
nsim: 27  CONV 
Re-running at optimal choice of numdims, 16 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
Searching for b value which maximizes the variance in K: Without balancing, biasbound (norm=1) is 0.02279 and the L1 discrepancy is 0.028 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00974  
nsim: 23 CONSTR (All) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01015  
Without balancing, biasbound (norm=1) is 0.02195 and the L1 discrepancy is 0.024 
5.555 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
Re-running at optimal choice of numdims, 1 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00821  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01042  
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
nsim: 27 aMEANFIRST 
nsim: 27 CONSTR (Demos) 
=====================  SIM: 26 ===================== 
0.672 % cces original strata missing from sample,   and 11108 / 44932 units
Joining with `by = join_by(strata)`
Without balancing, biasbound (norm=1) is 0.02516 and the L1 discrepancy is 0.037 
Without balancing, biasbound (norm=1) is 0.02538 and the L1 discrepancy is 0.03 
nsim: 26 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01034  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01969  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
Without balancing, biasbound (norm=1) is 0.02902 and the L1 discrepancy is 0.038 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
5.569 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00974  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
Selected 31 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02607 and the L1 discrepancy is 0.031 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01011  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02562  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01787  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00962  
Re-running at optimal choice of numdims, 1 
Without balancing, biasbound (norm=1) is 0.02152 and the L1 discrepancy is 0.025 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01098  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01277  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0118  
Re-running at optimal choice of numdims, 1 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01092  
Re-running at optimal choice of numdims, 16 
Without balancing, biasbound (norm=1) is 0.02195 and the L1 discrepancy is 0.024 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01018  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0145  
Without balancing, biasbound (norm=1) is 0.02531 and the L1 discrepancy is 0.033 
Used 31 With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02572  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01109  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
Used 19 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
=====================  SIM: 24 ===================== 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01529  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00989  
0.654 % cces original strata missing from sample,   and 10242 / 44932 units
Joining with `by = join_by(strata)`
nsim: 16 CONSTR (All) 
nsim: 24 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01205  
Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01114  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.011  
Re-running at optimal choice of numdims, 1 
=====================  SIM: 21 ===================== 
WithWith  16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
0.658 % cces original strata missing from sample,   and 8968 / 44932 units
Joining with `by = join_by(strata)`
nsim: 21 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
Searching for b value which maximizes the variance in K: With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00815  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00963  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01085  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00843  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00971  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
5.57 selected 
Building kernel matrix
Running full SVD on kernel matrix 
5.536 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01087  
=====================  SIM: 32 ===================== 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
0.677 % cces original strata missing from sample,   and 9558 / 44932 units
Joining with `by = join_by(strata)`
nsim: 32 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
Searching for b value which maximizes the variance in K: With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01147  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0114  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01055  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0113  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01115  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.028 
5.547 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02332  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00885  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0111  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01573  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0113  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01341  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
Without balancing, biasbound (norm=1) is 0.02173 and the L1 discrepancy is 0.024 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0227  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0102  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0175  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01058  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01141  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01388  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01182  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0097  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01132  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01094  
Without balancing, biasbound (norm=1) is 0.0264 and the L1 discrepancy is 0.037 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00878  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02683  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00964  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01274  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01219  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01165  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01052  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0107  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01004  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01113  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00949  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
nsim: 20  CONV 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00989  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00936  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01046  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00956  
Without balancing, biasbound (norm=1) is 0.02607 and the L1 discrepancy is 0.031 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01069  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01014  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01191  
Re-running at optimal choice of numdims, 6 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00797  
nsim: 20 aMEANFIRST 
nsim: 20 CONSTR (Demos) 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 19 CONSTR (All) 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01192  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00935  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01266  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 21 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01019  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00981  
nsim: 26  CONV 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
Re-running at optimal choice of numdims, 26 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
Without balancing, biasbound (norm=1) is 0.02607 and the L1 discrepancy is 0.031 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02098  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00884  
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01158  
Without balancing, biasbound (norm=1) is 0.02531 and the L1 discrepancy is 0.033 
nsim: 27 CONSTR (D+Edu) 
With user-specified 21 dimensions, biasbound (norm=1) of  0.00963  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01041  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00878  
Without balancing, biasbound (norm=1) is 0.02374 and the L1 discrepancy is 0.027 
nsim: 26 aMEANFIRST 
nsim: 26 CONSTR (Demos) 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00951  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01048  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00874  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00969  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00887  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
Without balancing, biasbound (norm=1) is 0.02516 and the L1 discrepancy is 0.037 
Without balancing, biasbound (norm=1) is 0.02531 and the L1 discrepancy is 0.033 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02111  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01018  
Re-running at optimal choice of numdims, 1 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00965  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0087  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01013  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01269  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 28 ===================== 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00966  
With 0.64 % cces original strata missing from sample,   and 8012 / 44932 units
111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00994  
Joining with `by = join_by(strata)`
nsim: 28 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00925  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01219  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01008  
5.547 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02195 and the L1 discrepancy is 0.024 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01267  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0106  
Re-running at optimal choice of numdims, 1 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01196  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0121  
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00968  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01248  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00892  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01305  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01153  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 61 
=====================  SIM: 25 ===================== 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01276  
0.684 % cces original strata missing from sample,   and 10882 / 44932 units
Joining with `by = join_by(strata)`
nsim: 25 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.02354 and the L1 discrepancy is 0.033 
Searching for b value which maximizes the variance in K: With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01242  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0236  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01031  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01011  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01221  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01249  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00989  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00767  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00977  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
5.527 selected 
Building kernel matrix
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01311  
Running full SVD on kernel matrix 
nsim: 24  CONV 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00682  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01303  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0108  
Re-running at optimal choice of numdims, 26 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0107  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01346  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01025  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01022  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
Without balancing, biasbound (norm=1) is 0.02173 and the L1 discrepancy is 0.024 
Used 18 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
nsim: 32  CONV 
nsim: 13 CONSTR (All) 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00748  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00896  
Without balancing, biasbound (norm=1) is 0.02159 and the L1 discrepancy is 0.024 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01061  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0134  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02262  
nsim: 24 aMEANFIRST 
nsim: 24 CONSTR (Demos) 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00773  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0103  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01601  
Without balancing, biasbound (norm=1) is 0.0264 and the L1 discrepancy is 0.037 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01188  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01123  
Re-running at optimal choice of numdims, 21 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00853  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0104  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01112  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01404  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
nsim: 32 aMEANFIRST 
nsim: 32 CONSTR (Demos) 
Used 18 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
nsim: 27 CONSTR (All) 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01062  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01388  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01311  
Re-running at optimal choice of numdims, 31 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01058  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01015  
Without balancing, biasbound (norm=1) is 0.02255 and the L1 discrepancy is 0.028 
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 20 CONSTR (D+Edu) 
Without balancing, biasbound (norm=1) is 0.02173 and the L1 discrepancy is 0.024 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0101  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01893  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01019  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0135  
Without balancing, biasbound (norm=1) is 0.0264 and the L1 discrepancy is 0.037 
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01211  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0117  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01984  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01148  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01167  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01044  
Selected 33 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01099  
nsim: 21  CONV 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01169  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01094  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01032  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.011  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01276  
Without balancing, biasbound (norm=1) is 0.02195 and the L1 discrepancy is 0.024 
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00777  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01034  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.028 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00867  
nsim: 21 aMEANFIRST 
nsim: 21 CONSTR (Demos) 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00984  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01043  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00771  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
Without balancing, biasbound (norm=1) is 0.02607 and the L1 discrepancy is 0.031 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00995  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01971  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01202  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00998  
Re-running at optimal choice of numdims, 1 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01035  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01005  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
Used 33 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 36 ===================== 
0.674 % cces original strata missing from sample,   and 10361 / 44932 units
Joining with `by = join_by(strata)`
nsim: 36 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01002  
Searching for b value which maximizes the variance in K: With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01055  
Re-running at optimal choice of numdims, 1 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00968  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01012  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.028 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00795  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0161  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0101  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01195  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00917  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01026  
5.548 selected 
Building kernel matrix
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01098  
Running full SVD on kernel matrix 
=====================  SIM: 22 ===================== 
0.66 % cces original strata missing from sample,   and 10152 / 44932 units
Joining with `by = join_by(strata)`
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00878  
nsim: 22 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01103  
Searching for b value which maximizes the variance in K: With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01025  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00927  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01188  
nsim: 28  CONV 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01081  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00927  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
5.542 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01066  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01115  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
Without balancing, biasbound (norm=1) is 0.02462 and the L1 discrepancy is 0.032 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0249  
Without balancing, biasbound (norm=1) is 0.02354 and the L1 discrepancy is 0.033 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01425  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00947  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0116  
Re-running at optimal choice of numdims, 26 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00682  
nsim: 28 aMEANFIRST 
nsim: 28 CONSTR (Demos) 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01289  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01096  
Used 18 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01256  
nsim: 20 CONSTR (All) 
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01205  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01008  
Without balancing, biasbound (norm=1) is 0.0217 and the L1 discrepancy is 0.025 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01034  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02264  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0143  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00967  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0124  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01059  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0117  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00972  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01163  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00977  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00982  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00958  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0101  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01238  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01018  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01154  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
Selected 32 dimensions of "allx" to use as mean balance constraints. 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01184  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00971  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00937  
Without balancing, biasbound (norm=1) is 0.02354 and the L1 discrepancy is 0.033 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01172  
Re-running at optimal choice of numdims, 31 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01635  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
nsim: 25  CONV 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01182  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01282  
Without balancing, biasbound (norm=1) is 0.02607 and the L1 discrepancy is 0.031 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
nsim: 24 CONSTR (D+Edu) 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00983  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
Without balancing, biasbound (norm=1) is 0.02159 and the L1 discrepancy is 0.024 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00938  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01239  
Re-running at optimal choice of numdims, 31 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01019  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01167  
Re-running at optimal choice of numdims, 6 
nsim: 25 aMEANFIRST 
nsim: 25 CONSTR (Demos) 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
Used 32 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
=====================  SIM: 29 ===================== 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01007  
0.674 % cces original strata missing from sample,   and 10659 / 44932 units
Joining with `by = join_by(strata)`
nsim: 29 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00935  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
5.533 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01351  
nsim: 32 CONSTR (D+Edu) 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00946  
Without balancing, biasbound (norm=1) is 0.02159 and the L1 discrepancy is 0.024 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01575  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02652 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02638  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01052  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01105  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01864  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01118  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01568  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01038  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01432  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01415  
Re-running at optimal choice of numdims, 16 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01117  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01165  
Used 14 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01147  
nsim: 26 CONSTR (D+Edu) 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01092  
Re-running at optimal choice of numdims, 26 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01147  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01155  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0113  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01143  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
nsim: 21 CONSTR (D+Edu) 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.012  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01234  
Re-running at optimal choice of numdims, 16 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00982  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0117  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 71 
Used 14 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 25 CONSTR (D+Edu) 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00947  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00919  
Without balancing, biasbound (norm=1) is 0.0264 and the L1 discrepancy is 0.037 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01049  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01886  
Without balancing, biasbound (norm=1) is 0.02173 and the L1 discrepancy is 0.024 
Without balancing, biasbound (norm=1) is 0.02531 and the L1 discrepancy is 0.033 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01096  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01664  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01891  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.028 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0158  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01102  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
nsim: 22  CONV 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01158  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01179  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01014  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00901  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01377  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01092  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
nsim: 29  CONV 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01046  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00864  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01324  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01043  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01051  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01004  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00934  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01063  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
Without balancing, biasbound (norm=1) is 0.02652 and the L1 discrepancy is 0.032 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01191  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00879  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
Without balancing, biasbound (norm=1) is 0.0217 and the L1 discrepancy is 0.025 
nsim: 29 aMEANFIRST 
nsim: 29 CONSTR (Demos) 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.011  
Re-running at optimal choice of numdims, 26 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01156  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00834  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01171  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With user-specified 41 dimensions, biasbound (norm=1) of  0.00971  
Used 18 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
nsim: 21 CONSTR (All) 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01118  
Re-running at optimal choice of numdims, 11 
nsim: 22 aMEANFIRST 
nsim: 22 CONSTR (Demos) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00975  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01114  
Without balancing, biasbound (norm=1) is 0.02652 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01837  
Used 17 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01163  
nsim: 32 CONSTR (All) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01154  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01015  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00861  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00995  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01092  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01173  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01115  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01108  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01121  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01183  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01159  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
nsim: 36  CONV 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01151  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01218  
Without balancing, biasbound (norm=1) is 0.02159 and the L1 discrepancy is 0.024 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00939  
Re-running at optimal choice of numdims, 31 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01091  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01257  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01082  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
Without balancing, biasbound (norm=1) is 0.02333 and the L1 discrepancy is 0.028 
WithWith  11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 56 dimensions of K, ebalance convergence is 0.00925FALSE   
yielding biasbound (norm=1) of 0.01248  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.0217 and the L1 discrepancy is 0.025 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01467  
nsim: 28 CONSTR (D+Edu) 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01064  
Without balancing, biasbound (norm=1) is 0.02462 and the L1 discrepancy is 0.032 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01196  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01059  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0138  
Re-running at optimal choice of numdims, 16 
With user-specified 31 dimensions, biasbound (norm=1) of  0.00821  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00977  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01137  
nsim: 36 aMEANFIRST 
nsim: 36 CONSTR (Demos) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
Re-running at optimal choice of numdims, 1 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01179  
Re-running at optimal choice of numdims, 11 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01096  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01022  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01315  
Re-running at optimal choice of numdims, 16 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
nsim: 26 CONSTR (All) 
=====================  SIM: 30 ===================== 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0106  
Used 18 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
Selected 18 dimensions of "allx" to use as mean balance constraints. 
0.676 % cces original strata missing from sample,   and 9998 / 44932 units
Joining with `by = join_by(strata)`
nsim: 25 CONSTR (All) 
nsim: 30 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Used 12 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 29 CONSTR (D+Edu) 
Searching for b value which maximizes the variance in K: With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01044  
5.557 selected 
Building kernel matrix
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01022  
Running full SVD on kernel matrix 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
Without balancing, biasbound (norm=1) is 0.0264 and the L1 discrepancy is 0.037 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0101  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00982  
Without balancing, biasbound (norm=1) is 0.02462 and the L1 discrepancy is 0.032 
Without balancing, biasbound (norm=1) is 0.02354 and the L1 discrepancy is 0.033 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01067  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01817  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01574  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01104  
Re-running at optimal choice of numdims, 1 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01188  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0072  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01041  
Without balancing, biasbound (norm=1) is 0.02159 and the L1 discrepancy is 0.024 
Without balancing, biasbound (norm=1) is 0.02676 and the L1 discrepancy is 0.036 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
Without balancing, biasbound (norm=1) is 0.02652 and the L1 discrepancy is 0.032 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0107  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01697  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02622  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00988  
=====================  SIM: 41 ===================== 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01051  
0.663 % cces original strata missing from sample,   and 10734 / 44932 units
Joining with `by = join_by(strata)`
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01549  
nsim: 41 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01085  
Without balancing, biasbound (norm=1) is 0.02531 and the L1 discrepancy is 0.033 
Searching for b value which maximizes the variance in K: With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00876  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01106  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01264  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00961  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00754  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01105  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01081  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01187  
Re-running at optimal choice of numdims, 1 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01115  
Re-running at optimal choice of numdims, 1 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 35 ===================== 
0.672 % cces original strata missing from sample,   and 10378 / 44932 units
Joining with `by = join_by(strata)`
nsim: 35 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
=====================  SIM: 34 ===================== 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
0.644 % cces original strata missing from sample,   and 9117 / 44932 units
Joining with `by = join_by(strata)`
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
nsim: 34 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
Searching for b value which maximizes the variance in K: 5.568 selected 
Building kernel matrix
Searching for b value which maximizes the variance in K: 5.529 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01071  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01076  
5.564 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00917  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01112  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00961  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01104  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01083  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0116  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00949  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00935  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01137  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.011  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
Re-running at optimal choice of numdims, 6 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01003  
Used 18 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
Without balancing, biasbound (norm=1) is 0.0247 and the L1 discrepancy is 0.032 
nsim: 28 CONSTR (All) 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02508  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01278  
Without balancing, biasbound (norm=1) is 0.02352 and the L1 discrepancy is 0.029 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01116  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02444  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01125  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01533  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0118  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01352  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00896  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
Without balancing, biasbound (norm=1) is 0.02618 and the L1 discrepancy is 0.034 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0115  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00997  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02618  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0153  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01218  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00968  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01072  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.009  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01207  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01108  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00805  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00953  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0108  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01242  
Re-running at optimal choice of numdims, 11 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00815  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01147  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
Used 18 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
nsim: 29 CONSTR (All) 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01049  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00995  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00945  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01258  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
Without balancing, biasbound (norm=1) is 0.02354 and the L1 discrepancy is 0.033 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01226  
Re-running at optimal choice of numdims, 41 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00992  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00928  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01054  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00853  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
Used 12 dimensions of "allx" for mean balancing, and an additional 41 dimensions of "K" from kernel balancing.
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01088  
nsim: 22 CONSTR (D+Edu) 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00894  
Re-running at optimal choice of numdims, 1 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00908  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01127  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 37 ===================== 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
0.663 % cces original strata missing from sample,   and 10200 / 44932 units
Joining with `by = join_by(strata)`
nsim: 37 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01267  
Re-running at optimal choice of numdims, 16 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00856  
Searching for b value which maximizes the variance in K: With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01131  
5.539 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
Used 18 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00999  
nsim: 24 CONSTR (All) 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01184  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 51 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01001  
Selected 33 dimensions of "allx" to use as mean balance constraints. 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01121  
Re-running at optimal choice of numdims, 31 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01085  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00834  
nsim: 41  CONV 
nsim: 35  CONV 
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 36 CONSTR (D+Edu) 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0108  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 71 
Without balancing, biasbound (norm=1) is 0.0217 and the L1 discrepancy is 0.025 
Selected 33 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01328  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00978  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0087  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01042  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0106  
Without balancing, biasbound (norm=1) is 0.02248 and the L1 discrepancy is 0.03 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02343  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01401  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01187  
Without balancing, biasbound (norm=1) is 0.02652 and the L1 discrepancy is 0.032 
Without balancing, biasbound (norm=1) is 0.02352 and the L1 discrepancy is 0.029 
Without balancing, biasbound (norm=1) is 0.0247 and the L1 discrepancy is 0.032 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01125  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01101  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
nsim: 30  CONV 
With user-specified 26 dimensions, biasbound (norm=1) of  0.00854  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
nsim: 35 aMEANFIRST 
nsim: 35 CONSTR (Demos) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00919  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01158  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00932  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00954  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
nsim: 41 aMEANFIRST 
nsim: 41 CONSTR (Demos) 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01166  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.011  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01155  
Without balancing, biasbound (norm=1) is 0.02173 and the L1 discrepancy is 0.024 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01198  
Re-running at optimal choice of numdims, 1 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00985  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01068  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Used 33 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
=====================  SIM: 38 ===================== 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
0.66 % cces original strata missing from sample,   and 10730 / 44932 units
Joining with `by = join_by(strata)`
nsim: 34  CONV 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
nsim: 38 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.02676 and the L1 discrepancy is 0.036 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00983  
Searching for b value which maximizes the variance in K: With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01121  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
Re-running at optimal choice of numdims, 1 
Without balancing, biasbound (norm=1) is 0.02462 and the L1 discrepancy is 0.032 
5.554 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0102  
Used 33 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01614  
=====================  SIM: 33 ===================== 
0.647 % cces original strata missing from sample,   and 8946 / 44932 units
Without balancing, biasbound (norm=1) is 0.0247 and the L1 discrepancy is 0.032 
Joining with `by = join_by(strata)`
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01062  
nsim: 33 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With user-specified 71 dimensions, biasbound (norm=1) of  0.00813  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00978  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01876  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00982  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01066  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00882  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01044  
nsim: 30 aMEANFIRST 
nsim: 30 CONSTR (Demos) 
5.557 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
Without balancing, biasbound (norm=1) is 0.02618 and the L1 discrepancy is 0.034 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
Without balancing, biasbound (norm=1) is 0.02352 and the L1 discrepancy is 0.029 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01919  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01175  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00926  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01229  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00766  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01117  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
nsim: 34 aMEANFIRST 
nsim: 34 CONSTR (Demos) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01126  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01014  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01049  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01058  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01145  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01084  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00924  
Without balancing, biasbound (norm=1) is 0.02546 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02508  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01087  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01501  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01097  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01051  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01196  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00942  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00759  
Without balancing, biasbound (norm=1) is 0.0209 and the L1 discrepancy is 0.025 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02169  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01289  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01144  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01231  
Re-running at optimal choice of numdims, 6 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01048  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01086  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01056  
Without balancing, biasbound (norm=1) is 0.02676 and the L1 discrepancy is 0.036 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02082  
Without balancing, biasbound (norm=1) is 0.02618 and the L1 discrepancy is 0.034 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
nsim: 22 CONSTR (All) 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01894  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01012  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00985  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01109  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01045  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00954  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00968  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01024  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00814  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0097  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01005  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01068  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01185  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01015  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
nsim: 37  CONV 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00977  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01257  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01108  
Re-running at optimal choice of numdims, 26 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01261  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01058  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
nsim: 35 CONSTR (D+Edu) 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00888  
Without balancing, biasbound (norm=1) is 0.02248 and the L1 discrepancy is 0.03 
Without balancing, biasbound (norm=1) is 0.0217 and the L1 discrepancy is 0.025 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00808  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01305  
Re-running at optimal choice of numdims, 31 
With user-specified 26 dimensions, biasbound (norm=1) of  0.00826  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0083  
nsim: 37 aMEANFIRST 
nsim: 37 CONSTR (Demos) 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01079  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
nsim: 41 CONSTR (D+Edu) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0084  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01015  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00937  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01105  
Re-running at optimal choice of numdims, 1 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0109  
=====================  SIM: 31 ===================== 
0.652 % cces original strata missing from sample,   and 11974 / 44932 units
Joining with `by = join_by(strata)`
nsim: 31 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01061  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00868  
Searching for b value which maximizes the variance in K: With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0087  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0083  
5.572 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Selected 18 dimensions of "allx" to use as mean balance constraints. 
nsim: 33  CONV 
Without balancing, biasbound (norm=1) is 0.02248 and the L1 discrepancy is 0.03 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01643  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00901  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01098  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01127  
Re-running at optimal choice of numdims, 16 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01127  
Without balancing, biasbound (norm=1) is 0.0247 and the L1 discrepancy is 0.032 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01059  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01708  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00988  
nsim: 38  CONV 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00835  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
nsim: 36 CONSTR (All) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00866  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00975  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00981  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00966  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00866  
Without balancing, biasbound (norm=1) is 0.0209 and the L1 discrepancy is 0.025 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01047  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0089  
Without balancing, biasbound (norm=1) is 0.02352 and the L1 discrepancy is 0.029 
With user-specified 31 dimensions, biasbound (norm=1) of  0.00761  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
nsim: 33 aMEANFIRST 
nsim: 33 CONSTR (Demos) 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01872  
Without balancing, biasbound (norm=1) is 0.02546 and the L1 discrepancy is 0.032 
Without balancing, biasbound (norm=1) is 0.02336 and the L1 discrepancy is 0.027 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02425  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01768  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0108  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0103  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01403  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00739  
nsim: 38 aMEANFIRST 
nsim: 38 CONSTR (Demos) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01053  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01346  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01032  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01056  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01084  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01027  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00975  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01048  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01069  
Re-running at optimal choice of numdims, 26 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01155  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Used 17 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01185  
nsim: 35 CONSTR (All) 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01042  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0111  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0118  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01086  
Re-running at optimal choice of numdims, 41 
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01079  
Re-running at optimal choice of numdims, 71 
Without balancing, biasbound (norm=1) is 0.02462 and the L1 discrepancy is 0.032 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01006  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01034  
Used 12 dimensions of "allx" for mean balancing, and an additional 41 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.0209 and the L1 discrepancy is 0.025 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
nsim: 30 CONSTR (D+Edu) 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01251  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01462  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01097  
Re-running at optimal choice of numdims, 16 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01088  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01091  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01114  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
Used 12 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
nsim: 37 CONSTR (D+Edu) 
Used 12 dimensions of "allx" for mean balancing, and an additional 71 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
Without balancing, biasbound (norm=1) is 0.02546 and the L1 discrepancy is 0.032 
nsim: 34 CONSTR (D+Edu) 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01131  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01822  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00972  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01234  
Re-running at optimal choice of numdims, 1 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01204  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01126  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00935  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
=====================  SIM: 45 ===================== 
0.658 % cces original strata missing from sample,   and 9688 / 44932 units
Joining with `by = join_by(strata)`
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
nsim: 45 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01051  
nsim: 31  CONV 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
Searching for b value which maximizes the variance in K: Without balancing, biasbound (norm=1) is 0.0247 and the L1 discrepancy is 0.032 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
5.579With  selected 
41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
Building kernel matrix
Running full SVD on kernel matrix 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
Without balancing, biasbound (norm=1) is 0.02676 and the L1 discrepancy is 0.036 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00963  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0203  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01048  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
Without balancing, biasbound (norm=1) is 0.02248 and the L1 discrepancy is 0.03 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00967  
Re-running at optimal choice of numdims, 1 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01586  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
Without balancing, biasbound (norm=1) is 0.02336 and the L1 discrepancy is 0.027 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
0.01004  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00972  
=====================  SIM: 44 ===================== 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01065  
0.681 % cces original strata missing from sample,   and 10436 / 44932 units
Joining with `by = join_by(strata)`
With user-specified 26 dimensions, biasbound (norm=1) of  0.00924  
nsim: 44 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
Re-running at optimal choice of numdims, 16 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
nsim: 31 aMEANFIRST 
nsim: 31 CONSTR (Demos) 
Searching for b value which maximizes the variance in K: With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
Without balancing, biasbound (norm=1) is 0.02618 and the L1 discrepancy is 0.034 
Used 13 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
5.521 selected 
Building kernel matrix
nsim: 38 CONSTR (D+Edu) 
Running full SVD on kernel matrix 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01098  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01705  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0098  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00925  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01062  
Re-running at optimal choice of numdims, 16 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0101  
Used 18 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 37 CONSTR (All) 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01148  
Without balancing, biasbound (norm=1) is 0.02403 and the L1 discrepancy is 0.028 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00894  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0239  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01706  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01257  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0117  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01061  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01208  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0092  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0125  
Without balancing, biasbound (norm=1) is 0.02412 and the L1 discrepancy is 0.03 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02496  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01015  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01297  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00977  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01064  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
Without balancing, biasbound (norm=1) is 0.02336 and the L1 discrepancy is 0.027 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
Without balancing, biasbound (norm=1) is 0.02546 and the L1 discrepancy is 0.032 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02059  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01592  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
Without balancing, biasbound (norm=1) is 0.02248 and the L1 discrepancy is 0.03 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0094  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00947  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0112  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01353  
Re-running at optimal choice of numdims, 31 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01027  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01138  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00843  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00936  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01119  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01199  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01011  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00936  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01227  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01157  
Re-running at optimal choice of numdims, 1 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
Used 18 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
=====================  SIM: 46 ===================== 
0.681 % cces original strata missing from sample,   and 10500 / 44932 units
Joining with `by = join_by(strata)`
nsim: 41 CONSTR (All) 
nsim: 46 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
Searching for b value which maximizes the variance in K: With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01213  
5.54 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01143  
Re-running at optimal choice of numdims, 31 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00943  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01251  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00984  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
Re-running at optimal choice of numdims, 6 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01142  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0077  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00867  
Used 17 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 38 CONSTR (All) 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00933  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 33 CONSTR (D+Edu) 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01228  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01065  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00956  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01258  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00829  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01131  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00888  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01326  
Without balancing, biasbound (norm=1) is 0.01905 and the L1 discrepancy is 0.02 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01927  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01134  
Re-running at optimal choice of numdims, 16 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01487  
nsim: 44  CONV 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01162  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00924  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
Used 18 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
nsim: 30 CONSTR (All) 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00947  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0151  
Re-running at optimal choice of numdims, 6 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00809  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
Used 12 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
nsim: 31 CONSTR (D+Edu) 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
Without balancing, biasbound (norm=1) is 0.02412 and the L1 discrepancy is 0.03 
WithWith user-specified 31 dimensions, biasbound (norm=1) of   0.00839  
66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01052  
Re-running at optimal choice of numdims, 11 
nsim: 44 aMEANFIRST 
Without balancing, biasbound (norm=1) isnsim: 44 CONSTR (Demos) 
 0.0209 and the L1 discrepancy is 0.025 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00944  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01257  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01056  
Without balancing, biasbound (norm=1) is 0.02546 and the L1 discrepancy is 0.032 
Without balancing, biasbound (norm=1) is 0.02352 and the L1 discrepancy is 0.029 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
Selected 18 dimensions of "allx" to use as mean balance constraints. 
Used 17 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
nsim: 34 CONSTR (All) 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00878  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0106  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00961  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00993  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 71 
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00983  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00771  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02412 and the L1 discrepancy is 0.03 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00981  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01853  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0106  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00953  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01079  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01045  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00764  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01008  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
nsim: 45  CONV 
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01101  
Re-running at optimal choice of numdims, 1 
=====================  SIM: 47 ===================== 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
Without balancing, biasbound (norm=1) is 0.02336 and the L1 discrepancy is 0.027 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00963  
0.665 % cces original strata missing from sample,   and 10888 / 44932 units
Joining with `by = join_by(strata)`
nsim: 47 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01935  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
Searching for b value which maximizes the variance in K: =====================  SIM: 50 ===================== 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0105  
0.656 % cces original strata missing from sample,   and 9876 / 44932 units
Joining with `by = join_by(strata)`
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
nsim: 50 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

5.552 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01067  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00962  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00798  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
Searching for b value which maximizes the variance in K: Without balancing, biasbound (norm=1) is 0.02676 and the L1 discrepancy is 0.036 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01174  
5.574 selected 
Building kernel matrix
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01019  
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01243  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01251  
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01107  
Re-running at optimal choice of numdims, 16 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
Without balancing, biasbound (norm=1) is 0.02403 and the L1 discrepancy is 0.028 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
Used 17 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01264  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01062  
Re-running at optimal choice of numdims, 1 
nsim: 33 CONSTR (All) 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00829  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01301  
Re-running at optimal choice of numdims, 6 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
=====================  SIM: 39 ===================== 
Used 18 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 31 CONSTR (All) 
0.661 % cces original strata missing from sample,   and 10103 / 44932 units
Joining with `by = join_by(strata)`
nsim: 39 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01049  
Searching for b value which maximizes the variance in K: With user-specified 71 dimensions, biasbound (norm=1) of  0.00763  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00894  
nsim: 45 aMEANFIRST 
nsim: 45 CONSTR (Demos) 
5.536 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02543 and the L1 discrepancy is 0.031 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02542  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01658  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01063  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
Without balancing, biasbound (norm=1) is 0.02618 and the L1 discrepancy is 0.034 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.024 and the L1 discrepancy is 0.029 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00916  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 66 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02443  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01511  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01397  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01156  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00966  
Re-running at optimal choice of numdims, 1 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0094  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
nsim: 46  CONV 
=====================  SIM: 43 ===================== 
0.681 % cces original strata missing from sample,   and 9147 / 44932 units
Joining with `by = join_by(strata)`
nsim: 43 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.0209 and the L1 discrepancy is 0.025 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
Searching for b value which maximizes the variance in K: With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
Without balancing, biasbound (norm=1) is 0.02407 and the L1 discrepancy is 0.03 
5.524 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02528  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01415  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01162  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01069  
Without balancing, biasbound (norm=1) is 0.02403 and the L1 discrepancy is 0.028 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00949  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01786  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00814  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01153  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01182  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01129  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
Without balancing, biasbound (norm=1) is 0.01905 and the L1 discrepancy is 0.02 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
Without balancing, biasbound (norm=1) is 0.02336 and the L1 discrepancy is 0.027 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0103  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01131  
Re-running at optimal choice of numdims, 1 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
With user-specified 66 dimensions, biasbound (norm=1) of  0.00732  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
nsim: 46 aMEANFIRST 
nsim: 46 CONSTR (Demos) 
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00952  
=====================  SIM: 42 ===================== 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00864  
0.663 % cces original strata missing from sample,   and 10224 / 44932 units
Joining with `by = join_by(strata)`
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
nsim: 42 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Without balancing, biasbound (norm=1) is 0.02327 and the L1 discrepancy is 0.027 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01157  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02249  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.008  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01628  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01191  
Re-running at optimal choice of numdims, 31 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01232  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01084  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0113  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 44 CONSTR (D+Edu) 
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00944  
Without balancing, biasbound (norm=1) is 0.01905 and the L1 discrepancy is 0.02 
5.547 selected 
Building kernel matrix
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
Running full SVD on kernel matrix 
WithWith  56 dimensions of K, ebalance convergence is TRUE 21 yielding biasbound (norm=1) of 0.00868  
dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01291  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
Re-running at optimal choice of numdims, 1 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01398  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01074  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00969  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00924  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
=====================  SIM: 40 ===================== 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
0.667 % cces original strata missing from sample,   and 11834 / 44932 units
Joining with `by = join_by(strata)`
nsim: 40 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00858  
Searching for b value which maximizes the variance in K: With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01077  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 66 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
Without balancing, biasbound (norm=1) is 0.02412 and the L1 discrepancy is 0.03 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01731  
Without balancing, biasbound (norm=1) is 0.02455 and the L1 discrepancy is 0.029 
5.567 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0084  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00991  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00892  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02485  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01741  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
nsim: 50  CONV 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01371  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0099  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00878  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00963  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00919  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00898  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00971  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00921  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.034 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02606  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
Without balancing, biasbound (norm=1) is 0.024 and the L1 discrepancy is 0.029 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01593  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01011  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 16 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00985  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01048  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01257  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00958  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01054  
nsim: 47  CONV 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01036  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00944  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01072  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0089  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With user-specified 56 dimensions, biasbound (norm=1) of  0.00821  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
nsim: 50 aMEANFIRST 
nsim: 50 CONSTR (Demos) 
nsim: 39  CONV 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01112  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01033  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
Without balancing, biasbound (norm=1) is 0.02543 and the L1 discrepancy is 0.031 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00958  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With user-specified 16 dimensions, biasbound (norm=1) of  0.00793  
nsim: 47 aMEANFIRST 
nsim: 47 CONSTR (Demos) 
Without balancing, biasbound (norm=1) is 0.02407 and the L1 discrepancy is 0.03 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00998  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
With user-specified 31 dimensions, biasbound (norm=1) of  0.00794  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00843  
nsim: 39 aMEANFIRST 
nsim: 39 CONSTR (Demos) 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01109  
Re-running at optimal choice of numdims, 26 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01044  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00935  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01001  
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
nsim: 45 CONSTR (D+Edu) 
Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00806  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01076  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
Without balancing, biasbound (norm=1) is 0.024 and the L1 discrepancy is 0.029 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01892  
Without balancing, biasbound (norm=1) is 0.02407 and the L1 discrepancy is 0.03 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01019  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00945  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01785  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
nsim: 43  CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01018  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01025  
Without balancing, biasbound (norm=1) is 0.02543 and the L1 discrepancy is 0.031 
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00858  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0094  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00884  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00957  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01732  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00971  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01031  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00829  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00819  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0093  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00975  
Selected 21 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00933  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00972  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00963  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00974  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00802  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00985  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01085  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0098  
Without balancing, biasbound (norm=1) is 0.02327 and the L1 discrepancy is 0.027 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00855  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00986  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00842  
nsim: 43 aMEANFIRST 
nsim: 43 CONSTR (Demos) 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01129  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01002  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 71 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00934  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01003  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00959  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01189  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00938  
Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01035  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00961  
Without balancing, biasbound (norm=1) is 0.02327 and the L1 discrepancy is 0.027 
Without balancing, biasbound (norm=1) is 0.02403 and the L1 discrepancy is 0.028 
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01463  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01014  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 91 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01206  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01059  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01084  
nsim: 42  CONV 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01041  
Re-running at optimal choice of numdims, 26 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01208  
Re-running at optimal choice of numdims, 26 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00945  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01502  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00962  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01115  
Re-running at optimal choice of numdims, 31 
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
Used 17 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00926  
nsim: 46 CONSTR (D+Edu) 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
nsim: 44 CONSTR (All) 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
Without balancing, biasbound (norm=1) is 0.02455 and the L1 discrepancy is 0.029 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00885  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
Used 14 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 39 CONSTR (D+Edu) 
nsim: 40  CONV 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
With user-specified 26 dimensions, biasbound (norm=1) of  0.00907  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00945  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00896  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
Selected 17 dimensions of "allx" to use as mean balance constraints. 
nsim: 42 aMEANFIRST 
nsim: 42 CONSTR (Demos) 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01033  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
Selected 17 dimensions of "allx" to use as mean balance constraints. 
Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
Without balancing, biasbound (norm=1) is 0.01905 and the L1 discrepancy is 0.02 
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.034 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00906  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01059  
Without balancing, biasbound (norm=1) is 0.02412 and the L1 discrepancy is 0.03 
Without balancing, biasbound (norm=1) is 0.02407 and the L1 discrepancy is 0.03 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01683  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01069  
Re-running at optimal choice of numdims, 26 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0098  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0097  
Without balancing, biasbound (norm=1) is 0.02455 and the L1 discrepancy is 0.029 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01033  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01908  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01082  
Re-running at optimal choice of numdims, 1 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00976  
With user-specified 81 dimensions, biasbound (norm=1) of  0.00823  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00997  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
=====================  SIM: 53 ===================== 
0.663 % cces original strata missing from sample,   and 10767 / 44932 units
Joining with `by = join_by(strata)`
nsim: 53 DEFAULT 
nsim: 50 CONSTR (D+Edu) 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 40 aMEANFIRST 
nsim: 40 CONSTR (Demos) 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00951  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01006  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00983  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
5.567 selected 
Building kernel matrix
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
Running full SVD on kernel matrix 
Selected 20 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00972  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.034 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01023  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01011  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00955  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01694  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00963  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01108  
Without balancing, biasbound (norm=1) is 0.024 and the L1 discrepancy is 0.029 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
Without balancing, biasbound (norm=1) is 0.02403 and the L1 discrepancy is 0.03 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00966  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00967  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02501  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01591  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01596  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01398  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00965  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00946  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00974  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01268  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00987  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0104  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00872  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01024  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00936  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00954  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00909  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00998  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00957  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01009  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00902  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00975  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00879  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01056  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00999  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00959  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00883  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01038  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01011  
With With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 36 dimensions of K, ebalance convergence is0.01029   
TRUE yielding biasbound (norm=1) of 0.00999  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01018  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01064  
Re-running at optimal choice of numdims, 26 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00882  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01003  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01135  
With 71 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01037  
Used 14 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01026  
nsim: 43 CONSTR (D+Edu) 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01026  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00899  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01051  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0088  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01136  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0106  
Re-running at optimal choice of numdims, 11 
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00928  
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00875  
WithWith  81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01144  
66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
Re-running at optimal choice of numdims, 31 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00913  
Used 12 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01153  
nsim: 47 CONSTR (D+Edu) 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01118  
Re-running at optimal choice of numdims, 16 
