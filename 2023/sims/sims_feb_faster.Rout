
R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ### Packages
> library(MASS)
Warning message:
package ‘MASS’ was built under R version 3.6.2 
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.2.1     ✔ dplyr   1.1.1
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ dplyr::select() masks MASS::select()
Warning messages:
1: package ‘tidyverse’ was built under R version 3.6.2 
2: package ‘ggplot2’ was built under R version 3.6.2 
3: package ‘tidyr’ was built under R version 3.6.2 
4: package ‘readr’ was built under R version 3.6.2 
5: package ‘purrr’ was built under R version 3.6.2 
6: package ‘forcats’ was built under R version 3.6.2 
> library(survey)
Loading required package: grid
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

Loading required package: survival

Attaching package: ‘survey’

The following object is masked from ‘package:graphics’:

    dotchart

Warning message:
package ‘Matrix’ was built under R version 3.6.2 
> #devtools::install_github("csterbenz1/KBAL", ref = "cat_kernel")
> library(kbal)
> library(parallel)
> library(knitr)
Warning message:
package ‘knitr’ was built under R version 3.6.2 
> library(glmnet)
Loaded glmnet 4.1-1
Warning message:
package ‘glmnet’ was built under R version 3.6.2 
> library(tictoc)
Warning message:
package ‘tictoc’ was built under R version 3.6.2 
> 
> ###### SET PARAMS  ###############
> set.seed(9345876)
> 
> if(detectCores() > 10) {
+   path_data = "/home/csterbenz/Data/"
+   cores_saved = 10
+   eval_kpop = T
+ } else if(detectCores() != 4) {
+   path_data= "/Users/Ciara_1/Dropbox/kpop/Updated/application/data/"
+   cores_saved = 3
+ } else {
+     path_data= "/Users/Ciara/Dropbox/kpop/Updated/application/data/"
+     cores_saved = 2
+ }
> options(dplyr.print_max = 1e9)
> #fit kpop and others to cces with populations weights?
> POPW = FALSE
> # to run with a linear kernel so it's way faster; UPDATE: errors catch this as mistake and prevent
> TEST = FALSE 
> #ebal tolerance and max iterations for kpop
> tolerance = 1e-4
> maxit = 500
> #adjust these both for runtime
> increment = 5
> min_num_dims = NULL
> max_num_dims = NULL
> eval_kpop = T
> SAVE = TRUE #save .Rdata results?
> #Need to adjust accordingly to machine for adequate number of sims
> #for cluster
> #nsims = (detectCores()-cores_saved)*14*2
> #First test with 10 sims
> #test runtime w/  1 iteration per core
> #nsims = (detectCores()-cores_saved)*1
> #full run nsims approx 1000
> nsims = (detectCores()-cores_saved)*60
> nsims
[1] 420
> 
> ##### Central Params to adjust
> #T=selection and outcome model are identical and directly specified w OLS with noise added after the fact;F= legacy; selection model is specified and regularized w/ lasso; outcome is previously run lasso model of cces three way vote choice fitted values = p(D) - p(R) 
> coverage_eval = TRUE 
> #if coverage_eval=T: use linear or nonlinear model  
> linear_model = FALSE 
> #if coverage_eval=T: add bernoulli noise by drawing binary from p(S=1)?
> bern = FALSE 
> #if coverage_eval=T: sd(y)*noise; 1-> r^2 = .5; sqrt(2) -> r^2 = .33; 1/2*sqrt(2) -> r^2 = .66;
> noise = 1 
> #if coverage_eval= T: adjusts sample size by dividing p(S) by scalar pS_denom (i.e. pS = plogis(XBeta)/pS_denom)
> pS_denom = 30
> #use the manually specified range of lambdas in the ridge residualization or allow glmnet to choose internally?
> manual_lambda = FALSE 
> #T=lambda as that which minimizes cverror in residualization; F= 1 sd from min choice
> lambda_min = FALSE 
> 
> #if coverage_eval=F: Legacy arg for coverage_eval = F that adj sample size as p(S)*(n_sample/sum(p(S)) + intercept_shift
> n_sample = 500 
> #if coverage_eval=F: Legacy arg for coverage_eval = F that flips selection model
> simple_selection_model = TRUE 
> 
> 
> ###################### Formulas ################
> formula_rake_demos_noeduc <- ~recode_age_bucket + recode_female + recode_race +
+   recode_region + recode_pid_3way
> 
> #updated to include 6 way edu
> formula_rake_demos_weduc <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_educ + recode_pid_3way
> 
> formula_rake_all_vars <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_pid_3way + recode_educ +
+   recode_income_5way + recode_relig_6way + recode_born + recode_attndch_4way
> 
> formula_ps <- ~recode_age_3way + recode_female + recode_race +
+   recode_region + recode_educ_wh_3way + recode_pid_3way
> 
> # if(coverage_eval &!linear_model) {
> #     formula_ps <- ~recode_age_3way + recode_female + recode_pid_3way
> # } else if(coverage_eval) {
> #     formula_ps <- ~recode_age_3way + recode_pid_3way
> # }
> 
> formula_ps_reduc <- ~recode_age_3way + recode_female +
+   recode_race + recode_region + recode_pid_3way  +
+   recode_income_3way + recode_born + recode_educ_wh_3way
> 
> # #let's coarsen income and religion and attend church for all:
> formula_ps_all <- ~recode_age_3way + recode_female +
+   recode_race + recode_region + recode_pid_3way + recode_educ_3way +
+   recode_income_3way + recode_relig_6way + recode_born + recode_attndch_bin
> 
> create_targets <- function (target_design, target_formula) {
+   target_mf <- model.frame(target_formula, model.frame(target_design))
+   target_mm <- model.matrix(target_formula, target_mf)
+   wts <- weights(target_design)
+ 
+   return(colSums(target_mm * wts) / sum(wts))
+ }
> 
> manual_rescale <- function(dat, a =0, b= 1) {
+     rescaled = (b-a)*( dat - min(dat))/(max(dat) - min(dat)) + a
+     return(rescaled)
+ }
> 
> ### Post-stratification function
> ## For now assumes that strata variable is already created and in
> ## the data set and called "strata"
> postStrat <- function(survey, pop_counts, pop_w_col, strata_pass, warn = T) {
+   survey_counts <- survey %>%
+     group_by(!!as.symbol(strata_pass)) %>%
+     summarize(n = n()) %>%
+     ungroup() %>%
+     mutate(w_survey = n / sum(n))
+ 
+   pop_counts <- pop_counts %>%
+     rename(w_pop = matches(pop_w_col))
+   
+   if(warn == T & nrow(survey_counts) !=  nrow(pop_counts)) {
+       missing_strat = pop_counts[! (( pop_counts[, strata_pass]%>% pull()) %in% (survey_counts[, strata_pass]%>% pull() )), strata_pass]
+       warning(paste("Strata in Pop not found in Sample. Dropping", 
+                     sum(pop_counts[(pop_counts[, strata_pass] %>% pull()) %in% 
+                                        (missing_strat %>% pull()),"n" ]), 
+                     "empty cells\n"), immediate.  =T )
+   } 
+   post_strat <- pop_counts %>%
+     left_join(survey_counts, by = strata_pass) %>%
+     filter(!is.na(w_survey)) %>%
+     ## Normalizes back to 1 after dropping
+     ## empty cells
+     mutate(w_pop = w_pop * 1/sum(w_pop),
+            w = w_pop / w_survey) %>%
+     dplyr::select(!!as.symbol(strata_pass), w)
+ 
+   survey <- survey %>%
+     left_join(post_strat)
+ 
+   return(survey)
+ }
> 
> check_sample <- function(sample, selection_model) {
+     check = model.matrix(selection_model, data = sample)
+     check = colSums(check)
+     fail = check[which(check ==0)]
+     fail_bin = length(check[which(check ==0)]) > 0 
+     check_prop = check/nrow(sample)
+     
+     return(list(samp_prop = check_prop,
+                 fail  = fail, 
+                 fail_bin = fail_bin,
+                 counts = check))
+ }
> 
> check_sample_outcome <- function(sample, selection_model, interaction_cols, interaction_cols_2 = NULL) {
+     
+     vars = all.vars(selection_model)
+     var = NULL
+     counts = NULL
+     prop = NULL
+     u_outcome = NULL
+     #uninteracted variables
+     for(i in 1:length(vars)) {
+         t = sample %>% group_by_at(vars[i]) %>%
+             summarise(n = n(), 
+                       avg_outcome = mean(outcome)) %>%
+             mutate(prop = round(n/nrow(sample),4))
+         var = c(var, as.character(t[,1] %>% pull()))
+         counts = c(counts, t$n)
+         prop = c(prop,  t$prop)
+         u_outcome = c(u_outcome, t$avg_outcome)
+     }
+     #interactions
+     t = suppressMessages(sample %>% group_by_at(interaction_cols) %>% 
+         summarise(n = n(),
+                   avg_outcome = mean(outcome)) %>%
+         mutate(prop = round(n/nrow(sample), 4)))
+     interaction = apply(t, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+     counts = data.frame(var  = c(var, interaction),
+                n = c(counts, t$n), 
+                prop = c(prop, t$prop),
+                avg_outcome = c(u_outcome, t$avg_outcome))
+     
+     if(!is.null(interaction_cols_2)) {
+         t2 = suppressMessages(sample %>% group_by_at(interaction_cols_2) %>% 
+                                  summarise(n = n(),
+                                            avg_outcome = mean(outcome)) %>%
+                                  mutate(prop = round(n/nrow(sample), 4)))
+         interaction = apply(t2, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+         append = cbind(data.frame(var = interaction), t2[, - c(1,2)])
+         counts = rbind(counts, append)
+     }
+     
+     fail = sum(counts$n == 0)
+     bad = sum(counts$prop <= 0.05)
+     bad_strata = data.frame(strata = as.character(counts$var[counts$prop <= 0.05]), prop = counts$prop[counts$prop <= 0.05])
+     v_bad = sum(counts$prop <= 0.01)
+     v_bad_strata = data.frame(strata = as.character(counts$var[counts$prop <= 0.01]), prop = counts$prop[counts$prop <= 0.01])
+     counts$var[counts$prop <= 0.01]
+     
+     counts = counts %>% mutate(leq_5pp = as.numeric(prop <= 0.05),
+                                leq_1pp = as.numeric(prop <= 0.01))
+     
+    
+     return(list(counts = counts,
+                 fail = fail, 
+                 bad = bad, 
+                 v_bad = v_bad, 
+                 bad_strata = bad_strata,
+                 v_bad_strata = v_bad_strata))
+ }
> 
> check_outcome <- function(outcome) {
+     beyond_support = (min(outcome) <0 | max(outcome) > 1)
+     return(beyond_support)
+ }
> 
> bound_outcome <- function(outcome, coefs, increment = 1, increment_intercept = .01, noise, cces_expanded, silent = T) {
+     denom = 10
+     fail = check_outcome(outcome)
+     while(fail) {
+         denom = denom + increment
+         if(!silent) { cat(denom, ": ") }
+         coefs_use = coefs/denom
+         outcome = cces_expanded %*% coefs_use
+         
+         outcome = outcome + rnorm(nrow(cces_expanded), mean = 0, sd = sd(outcome)*noise)
+         summary(outcome)
+         if(max(outcome) <=1 & min(outcome) <0 ) {
+             coefs[1] = coefs[1] + increment_intercept
+             if(!silent) { cat("\nmoving intercept up", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         if(max(outcome) >1 & min(outcome) >=0 ) {
+             coefs[1] = coefs[1] - increment_intercept
+             if(!silent) { cat("\nmoving intercept down", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         fail = check_outcome(outcome)
+         if(!silent) { cat(round(min(outcome),2), round(max(outcome),2),  "\n") }
+     }
+     if(!silent) { cat(paste("Min denom:", denom)) }
+     return(list(outcome = outcome, coefs = coefs_use, denom = denom))
+     
+ }
> 
> 
> ############# Load Data #####################
> #these data have been cleaned already see app_modeled for how it was done
> ## Load Pew
> pew <- readRDS(paste0(path_data, "pew_lasso_061021.rds"))
> pew$recode_age_bucket = as.character(pew$recode_age_bucket)
> pew$recode_age_3way= as.character(pew$recode_age_3way)
> ### Load Target Data
> cces <- readRDS(paste0(path_data, "cces_lasso_061021.rds"))
> cces$recode_age_bucket = as.character(cces$recode_age_bucket)
> cces$recode_age_3way= as.character(cces$recode_age_3way)
> 
> cces <- cces %>% mutate(recode_agesq = recode_age^2/ mean(recode_age^2),
+                         recode_agecubed = recode_age^3/ mean(recode_age^3))
> 
>  ##################### LASSO: Selection #############################
> 
> 
> #income, religion, pid x race
> if(!coverage_eval) {
+ 
+     if(simple_selection_model) {
+         #first attempt to make this worse
+         # selection_model = as.formula(~recode_pid_3way:poly(recode_age, 2) +
+         #                                  recode_female:recode_pid_3way)
+         #first attempt to make this worse
+         #center age then square
+         pew = pew %>% mutate(centered_age = scale(recode_age, scale = F))
+         cces = cces %>% mutate(centered_age = scale(recode_age, scale = F))
+ 
+         selection_model = as.formula(~recode_pid_3way*poly(centered_age, 2) +
+                                          #recode_age:recode_pid_3way +
+                                          recode_female*recode_pid_3way)
+ 
+         #selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
+ 
+     } else {
+         selection_model = as.formula(~recode_female:recode_pid_3way +
+                                          recode_age:recode_pid_3way +
+                                          #adds a bit mroe bias to edu+d
+                                          recode_pid_race +
+                                          recode_race_reg_wh_educ +
+                                          recode_educ_wh_3way +
+                                          poly(recode_age, 3))
+     }
+     # Stack data with S = 1 indicating Pew
+     stack_data <- data.frame(bind_rows(pew, cces),
+                              S = c(rep(1, nrow(pew)), rep(0, nrow(cces))))
+ 
+ 
+     mod <- model.matrix(selection_model, data = stack_data)
+     nrow(mod)
+     ## Remove columns where Pew missing strata
+     ncol(mod)
+     mod <- mod[, apply(mod[stack_data$S == 1, ], 2, sum) != 0]
+     ## Remove columns where CCES missing Strata
+     mod <- mod[, apply(mod[stack_data$S == 0, ], 2, sum) != 0]
+     ncol(mod)
+     lasso_lambda <- cv.glmnet(x= mod[,-1],
+                               y = as.matrix(stack_data$S),
+                               alpha = 1,
+                               family = "binomial",
+                               intercept = TRUE)
+     lasso_lambda$lambda.min
+     lasso_include <- glmnet(x= mod[,-1],
+                             y = as.matrix(stack_data$S),
+                             alpha = 1,
+                             lambda = lasso_lambda$lambda.min,
+                             weights = if(POPW){ c(rep(1, nrow(pew)),
+                                                   cces$commonweight_vv_post)} else {NULL},
+                             family = "binomial",
+                             intercept = TRUE)
+ 
+     lasso_include_coefs <- coef(lasso_include)
+     res <- as.matrix(lasso_include_coefs)
+     #View(res)
+     ncol(mod)
+     sum(lasso_include_coefs == 0)
+ 
+     lasso_include_coefs
+     lasso_pinclude = predict(lasso_include,
+                              s= lasso_lambda$lambda.min,
+                              type = "response",
+                              newx = mod[stack_data$S == 0,-1])
+ 
+     p_include <- lasso_pinclude
+     sum(p_include)
+     cor(p_include, cces$outcome)
+     cor(p_include, cces$mod_cces_on_cces_pD)
+     cor(p_include, cces$mod_cces_on_cces_pR)
+ 
+     ########### Sampling Inclusion Results
+     summary(p_include)
+     sum(p_include)
+     mean(p_include)
+     p_include_raw = p_include
+ 
+ 
+     intercept_shift = 0
+     p_include = p_include*(n_sample/sum(p_include)) + intercept_shift
+     mean(p_include)
+     summary(p_include)
+     sum(p_include)
+ 
+     #################### Define Outcome #########
+     cces$outcome = cces$diff_cces_on_cces
+ } else {
+     if(linear_model) {
+         ########## DESIGN SELECTION MODEL: Specifying Coefs Directly
+         #coefs: pid, age, gender
+         selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
+         inter = NULL
+         cces_expanded = model.matrix(selection_model, data = cces)
+         #needs to be n x p X p x 1 -> coef matrix is p x 1
+         coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
+         rownames(coefs) = colnames(cces_expanded)
+         coefs[,1] = c(-5.2, #intercept
+                       .1, #selection of indep pos
+                       .4, #selection of R pos
+                       .1, #36-50,
+                       .3, #51-64,
+                       .8, #65+,
+                       .4 #male pos
+         )
+         
+         xbeta = cces_expanded %*% coefs
+         p_include = plogis(xbeta)
+         sum(p_include)
+         summary(p_include)
+         
+         #################### DESIGN OUTCOME MODEL ##################
+         coefs_outcome = coefs
+         
+         coefs_outcome[,1] = c(6.1, #intercept
+                               -.3, #  indep pos #decreasing lowers mean, increases corr #try 1
+                               -1.1, #  R pos #empirically
+                               -.2 ,#36-50, #empirically in cces lean dem 50% #.55
+                               -.3, #51-64, #empirically lean rep slightly 50%
+                               -.7, #65+, #empirically lean rep 51%
+                               #base cat: 18-35 lean strongly dem 58%
+                               -.7 #male #empirically women lean dem 53%
+         )
+         coefs_outcome = coefs_outcome/10
+         
+         xbeta_outcome = cces_expanded %*% coefs_outcome
+         #cat(paste("Orig mean scaled outcome", round(mean(xbeta_outcome)*100, 3) , "\n"))
+         #summary(xbeta_outcome)
+         if(!bern) {
+             cat(paste("Adding sd(outcome)*",round(noise, 3), "\n"))   
+             xbeta_outcome = xbeta_outcome + rnorm(nrow(cces), mean = 0, sd = sd(xbeta_outcome)*noise)
+         }
+         
+         # summary(xbeta_outcome)
+         # cor(xbeta_outcome, p_include)
+         # cces[which(xbeta_outcome == min(xbeta_outcome)),
+         #      c("recode_pid_3way", "recode_age_bucket", "recode_female" )]
+         # cces[which(xbeta_outcome == max(xbeta_outcome)),
+         #      c("recode_pid_3way", "recode_age_bucket", "recode_female" )]
+         
+         #plot(density(xbeta_outcome))
+         #cat(paste("Mean outcome w/noise is", round(mean(xbeta_outcome)*100,3), "\n"))
+         if(bern) {
+             cat(paste("Adding bernoulli",round(noise, 3), "\n"))  
+             xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
+         }
+         cat(paste("Range of outcome w/noise is\n"))
+         cat(paste(summary(xbeta_outcome), "\n"))
+         if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {warning("outcome beyond prob support for some units when noise is added", immediate. = T)}
+         s = summary(lm(xbeta_outcome ~ recode_pid_3way + recode_age_bucket + recode_female,data = cces))
+         R2_outcome = s$adj.r.squared
+         cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
+         cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
+         cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))
+         #bernoulli draw
+         
+         cces$outcome = xbeta_outcome
+     } else {
+         
+         selection_model = as.formula(~recode_pid_3way + recode_female + recode_age_bucket 
+                                      + recode_educ_3way 
+                                      + recode_race
+                                      + recode_born 
+                                      + recode_born:recode_age_bucket
+                                      + recode_pid_3way:recode_age_bucket
+         )
+         inter = c("recode_pid_3way", "recode_age_bucket")
+         inter_2 = c("recode_born", "recode_age_bucket")
+         cces_expanded = model.matrix(selection_model, data = cces)
+         coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
+         rownames(coefs) = colnames(cces_expanded)
+         coefs
+         #(p(S)) for negative bias select non dem voters
+         coefs[,1] = c(-2, #intercept -5 w race
+                       2, #selection of indep pos
+                       2, #selection of R pos
+                       .5, #male
+                       .15, #36-50,
+                       .2, #51-64,
+                       .2, #65+,
+                       .7, #college
+                       -1 , #post-grad
+                       .5,#hispanic
+                       .3,#other
+                       .7,#white
+                        2, #bornagain
+                       1,#bornagain x 36-50
+                       1.5, #bornagain x 51-64
+                       2, #bornagain x 65+
+                       .3,#ind x 36-50
+                       .5, #rep x 36-50,
+                       1, #ind x 51-64,
+                       1, #rep x 51-64,
+                       -.2, #ind x 65+
+                       2 #rep x 65+
+         )
+         
+         xbeta = cces_expanded %*% coefs
+         p_include = plogis(xbeta)
+         p_include = p_include/pS_denom
+         sum(p_include)
+         
+         # LA:tinkering
+         # sample <- rbinom(nrow(cces), 1, p_include)
+         # sum(sample)
+         # survey_sim <- cces[sample == 1, ]
+         # 
+         # check_sample_outcome(survey_sim, selection_model, inter)
+         # #check_sample_outcome(survey_sim, selection_model, inter_2)
+         # n_distinct(cces$strata) - n_distinct(survey_sim$strata)
+         # dropped_strata = unique(cces$strata)[which(!(unique(cces$strata) %in%
+         #                                                  unique(survey_sim$strata)))]
+         # cces %>% filter(strata %in% dropped_strata) %>% count()
+         # 
+         # la = data.frame(pS = p_include, pid = cces$recode_pid_3way) %>%
+         #     group_by(pid) %>% summarise(max = round(max(pS)*100,2 ))
+         # # la
+         # #
+         # # check_sample_outcome(survey_sim,selection_model, inter)
+         # gg_dat = data.frame(Selection_Probability = p_include,
+         #                     Pid = cces$recode_pid_3way,
+         #                     Outcome_pD = NA)
+         # gg_p_include_pid = ggplot(gg_dat) +
+         #     geom_density(aes(x= Selection_Probability, color = Pid)) +
+         #     annotate(geom = "label", x=quantile(p_include,.25),y=Inf, vjust = 1,
+         #              color = "red",
+         #              label =  paste0("Dem Max P(S)= ", la[la$pid =="Dem", "max"], "%")) +
+         #     annotate(geom = "label",x=quantile(p_include,.25), y=Inf, vjust =3,
+         #              label= paste0("Ind Max P(S)= ", la[la$pid =="Ind", "max"], "%" ),
+         #              color = "green") +
+         #     annotate(geom = "label",x=quantile(p_include,.25), y=Inf, vjust = 5,
+         #              label= paste0("Rep Max P(S)= ", la[la$pid =="Rep", "max"], "%" ),
+         #              color = "blue") +
+         #     # geom_text(x=.10, y=250, label= paste0("Dem Max P(S)=", la[la$pid =="Dem", "max"] ),
+         #     #           color = "red") +
+         #     #  geom_text(x=.10, y=200, label= paste0("Ind Max P(S)=", la[la$pid =="Ind", "max"] ),
+         #     #           color = "green") +
+         #     #  geom_text(x=.10, y=150, label= paste0("Rep Max P(S)=", la[la$pid =="Rep", "max"] ),
+         #     #           color = "Blue") +
+         #     ggtitle("Distribution of Seleciton Probabilities by Party") +
+         #     theme_bw()
+         # gg_p_include_pid
+         # 
+         #most likely to select: rep, 65+, male, protestants
+         # cces[which(p_include == max(p_include)),
+         #      c("recode_pid_3way", "recode_age_bucket", "recode_female", "recode_relig_6way")]
+         # #summary(p_include)
+         
+         #################### DESIGN OUTCOME MODEL ##################
+         coefs_outcome = coefs
+         #p(D)
+         # coefs_outcome[,1] = c(37, #intercept
+         #                       -3,#-.5, #selection of indep pos
+         #                       -5,# -.8, #selection of R pos
+         #                       -.3, #male
+         #                       -.5, #36-50,
+         #                       -.1, #51-64,
+         #                       -.2, #65+,
+         #                       .8, #college
+         #                       .9,  #post-grad
+         #                       -.5,#hispanic
+         #                       -.7,#other
+         #                       -3,#white
+         #                        -5, #bornagain
+         #                       # -3,#bornagain x 36-50
+         #                       # -3.5, #bornagain x 51-64
+         #                       # -4, #bornagain x 65+
+         #                       -3,#ind x 36-50
+         #                       -4, #rep x 36-50,
+         #                       -5, #ind x 51-64,
+         #                       -6, #rep x 51-64,
+         #                       3.5, #ind x 65+
+         #                       -4.5 #rep x 65+
+         # )
+         #this -coefs method gets good bias on ps and other raking but rake all is p good :/
+         coefs_outcome = -coefs
+         cor(p_include, cces_expanded %*% coefs_outcome)
+         coefs_outcome = coefs_outcome*1.5
+         cor(p_include, cces_expanded %*% coefs_outcome)
+         coefs_outcome[1] = 25
+         cor(p_include, cces_expanded %*% coefs_outcome)
+         if(!bern) {
+             cat(paste("Adding sd(outcome)*",round(noise, 3), "\n")) 
+             set.seed(1383904)
+             bound = bound_outcome(outcome = cces_expanded %*% coefs_outcome,
+                                   coefs = coefs_outcome,
+                                   cces_expanded = cces_expanded,
+                                   noise = noise, silent = F)
+             coefs_outcome = bound$coefs
+             xbeta_outcome = bound$outcome
+             beyond_support = check_outcome(xbeta_outcome)
+         
+             if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {
+                 warning("Outcome beyond prob support for some units when noise is added",
+                                                                         immediate. = T)
+             }
+         } else {
+             cat(paste("Adding bernoulli noise",noise, "\n"))  
+             set.seed(1383904)
+             xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
+             cat(paste("Corr of S (one sample draw) and Y", round(cor(sample, xbeta_outcome),3)))
+         }
+         # summary(xbeta_outcome)
+         # cor(xbeta_outcome, p_include)
+         # cces[which(xbeta_outcome == min(xbeta_outcome)),
+         #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
+         # cces[which(xbeta_outcome == max(xbeta_outcome)),
+         #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
+         # 
+         #plot(density(xbeta_outcome))
+         #cat(paste("Mean outcome w/noise is", round(mean(xbeta_outcome)*100,3), "\n"))
+         
+         cat(paste("Range of outcome w/noise is\n"))
+         cat(paste(summary(xbeta_outcome), "\n"))
+         s = summary(lm(update(selection_model, xbeta_outcome ~ .),data = cces))
+         R2_outcome = s$adj.r.squared
+         cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
+         cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
+         cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))
+         cces$outcome = xbeta_outcome
+         
+         # #LA: 
+         # gg_dat = data.frame(Selection_Probability = p_include,
+         #                     Pid = cces$recode_pid_3way,
+         #                     Outcome_pD = cces$outcome)
+         # gg_p_include_outcome = ggplot(gg_dat) +
+         #     geom_point(aes(x= Selection_Probability, y= Outcome_pD, color = Pid)) +
+         #     theme_bw()
+         # gg_p_include_outcome
+         
+     }
+     
+ }
Adding sd(outcome)* 1 
11 : -0.03 3.57 
12 : -0.1 3.08 
13 : -0.22 2.97 
14 : -0.16 2.74 
15 : -0.02 2.55 
16 : -0.05 2.34 
17 : -0.11 2.14 
18 : -0.23 2.13 
19 : -0.02 1.99 
20 : 
moving intercept down 24.99 
0.01 1.99 
20 : 
moving intercept down 24.98 
0.03 1.94 
20 : -0.03 1.85 
21 : 
moving intercept down 24.97 
0 1.79 
21 : -0.01 1.85 
22 : -0.1 1.76 
23 : -0.06 1.58 
24 : 0 1.63 
25 : 
moving intercept down 24.96 
0.03 1.48 
25 : 
moving intercept down 24.95 
0.01 1.53 
25 : -0.06 1.51 
26 : -0.04 1.46 
27 : -0.07 1.48 
28 : -0.04 1.36 
29 : -0.05 1.41 
30 : -0.06 1.31 
31 : -0.1 1.22 
32 : -0.01 1.16 
33 : -0.04 1.21 
34 : 
moving intercept down 24.94 
0.03 1.18 
34 : 
moving intercept down 24.93 
0.01 1.11 
34 : -0.13 1.1 
35 : -0.03 1.11 
36 : -0.09 1.03 
37 : -0.09 1.01 
38 : -0.07 1.02 
39 : 
moving intercept up 24.94 
-0.01 0.96 
39 : 
moving intercept up 24.95 
-0.03 0.97 
39 : 
moving intercept up 24.96 
-0.05 0.99 
39 : 
moving intercept up 24.97 
-0.01 0.96 
39 : -0.08 1.01 
40 : 
moving intercept up 24.98 
-0.07 0.96 
40 : 
moving intercept up 24.99 
-0.02 0.94 
40 : 
moving intercept up 25 
-0.03 0.93 
40 : 0.03 0.95 
Min denom: 40Range of outcome w/noise is
Min.   :0.02652   
 1st Qu.:0.40756   
 Median :0.49534   
 Mean   :0.49143   
 3rd Qu.:0.57859   
 Max.   :0.95386   
R^2 outcome is 0.497 
Mean scaled outcome (target) is 49.143
Corr of sampling prob and outcome  -0.635> 
> if(coverage_eval) {
+     formula_ps <- selection_model
+ } else {
+     formula_ps <- ~recode_age_3way + recode_pid_3way + recode_female
+ }
> 
> ######### Make STRATA variable in CCES and Pew ############
> cces <- bind_cols(cces, cces %>%
+                       unite("strata", all.vars(formula_ps), remove = FALSE) %>%
+                       unite("strata_reduc", all.vars(formula_ps_reduc),
+                             remove = FALSE) %>%
+                       unite("strata_all", all.vars(formula_ps_all)) %>%
+                       dplyr::select(strata, strata_reduc, strata_all))
> 
> 
> #################### Targets ###################
> if(POPW) {
+   cces_svy <- svydesign(ids = ~1, weights = ~commonweight_vv_post, data = cces)
+ } else {
+   cces_svy <- suppressWarnings(svydesign(ids = ~1, data = cces))
+ }
> margin_sim = svymean(~outcome, cces_svy)[1]* 100
> margin_sim
 outcome 
49.14268 
> targets_rake_demos_noeduc <- create_targets(cces_svy,
+                                             formula_rake_demos_noeduc)
> targets_rake_demos_weduc <- create_targets(cces_svy, formula_rake_demos_weduc)
> targets_rake_all_vars <- create_targets(cces_svy,
+                                         formula_rake_all_vars)
> targets_demo_truth <- create_targets(cces_svy, selection_model)
> 
> 
> ## Make table of Population Counts for post-stratification for manual ps function
> cces_counts <- cces %>%
+   group_by(strata) %>%
+   summarize(n = if(!POPW) {n()} else {sum(commonweight_vv_post, na.rm = TRUE)}) %>%
+   ungroup() %>%
+   mutate(w = n / sum(n, na.rm = TRUE))
> 
> cces_reduc_counts <- cces %>%
+   group_by(strata_reduc) %>%
+   summarize(n = if(!POPW) {n()} else {sum(commonweight_vv_post, na.rm = TRUE)}) %>%
+   ungroup() %>%
+   mutate(w_reduc = n / sum(n, na.rm = TRUE)) 
> 
> 
> cces_all_counts <- cces %>% 
+   group_by(strata_all) %>% 
+   summarize(n = if(!POPW) {n()} else {sum(commonweight_vv_post, na.rm = TRUE)}) %>%
+   ungroup() %>%
+   mutate(w_all = n / sum(n, na.rm = TRUE)) 
> 
> 
> ########################### RUN SIMS ##################################
> #save mem
> rm(pew)
> #rm(pew, lasso_pinclude, lasso_include,lasso_lambda,
> #   stack_data, mod)
> 
> 
> margins_formula <- ~recode_vote_2016 + 
+   mod_cces_on_cces_pR + mod_cces_on_cces_pD + mod_cces_on_cces_pO+
+   diff_cces_on_cces + margin_cces_on_cces + 
+   recode_pid_3way + 
+   recode_female + recode_race +recode_region + recode_educ + recode_relig_6way + 
+   recode_born + recode_attndch_4way + recode_income_5way +
+   recode_age_bucket + recode_age + recode_agesq + recode_agecubed + recode_age_factor + 
+   recode_race_educ_reg + recode_educ_wh_3way + 
+   recode_educ_pid_race +
+   recode_pid_race + 
+   recode_educ_pid +
+   recode_race_reg_wh_educ + 
+   recode_midwest_edu_race +
+   recode_midwest_wh_edu
> 
> est_mean <- function(outcome, design) {
+   svymean(as.formula(paste0("~", outcome)), design, na.rm = TRUE)[1]
+ }
> 
> #########################################
> ############## variance calc ###########
> ## Variance functions
> var_fixed <- function(Y, weights, pop_size) {
+     ## note: needs weights that sum to population total
+     if(round(sum(weights)) != pop_size) { weights = weights*pop_size/sum(weights)}
+     return(Hmisc::wtd.var(Y, weights))
+ }
> 
> ## kott (14) (under poisson)
> var_quasi <- function(weights, residuals, pop_size) {
+     #moving from kott 14 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     #sum^n (w_i^2 - 1/N_pop w_i)e_i^2 
+     return(sum((weights^2 - (weights / pop_size))*residuals^2))
+ }
> 
> ## kott (15) linearization
> var_linear <- function(weights, residuals, sample_size) {
+     #moving from kott 14 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     # n/(n-1) sum^n (w_i*e_i)^2 - (1/n-1) [sum^n] *using this for now
+     # approx = sum^n (w_i*e_i)^2 - (1/n) [sum^n]
+     n = sample_size
+     return((n/(n-1))*sum((weights * residuals)^2) - 1/n * sum(weights * residuals)^2)
+ }
> 
> ## chad
> var_chad <- function(weights, residuals) {
+     return(sum(weights^2 * residuals^2))
+ }
> 
> ## calculate all variances
> calc_SEs <- function(Y, residuals, pop_size, weights, sample_size) {
+     if(round(sum(weights)) != 1 ) {
+         weights = weights/sum(weights)
+     }
+     return(data.frame(SE_fixed = sqrt(var_fixed(Y, weights, pop_size) / length(Y)),
+                       SE_quasi = sqrt(var_quasi(weights, residuals, pop_size)),
+                       SE_linear = sqrt(var_linear(weights, residuals, sample_size)),
+                       SE_chad = sqrt(var_chad(weights, residuals))))
+ }
> 
> 
> if(detectCores() < 20) {
+     nsims = 10
+     eval_kpop = T
+     SAVE = F
+ }
> #double check params are as expected:
> nsims
[1] 10
> sum(p_include)
[1] 1026.369
> SAVE
[1] FALSE
> coverage_eval
[1] TRUE
> eval_kpop
[1] TRUE
> manual_lambda
[1] FALSE
> pryr::mem_used()
180 MB
> rm(bound, cces_expanded, s, xbeta, xbeta_outcome)
> pryr::mem_used()
171 MB
> # lambda_min
> system.time({
+   sims <- mclapply(1:nsims, function(nsim) {
+     #
+     system.time({
+     cat(paste("=====================  SIM:",nsim, 
+               "===================== \n"))
+ 
+     sample <- rbinom(nrow(cces), 1, p_include)
+     survey_sim <- cces[sample == 1, ]
+     
+     survey_design <- suppressWarnings(svydesign(ids = ~1, data = survey_sim))
+     
+     ########################### check sample ##########################
+     check_s = check_sample(survey_sim, selection_model)
+     bad_sample = check_s$fail_bin
+     check_2 = check_sample_outcome(survey_sim, selection_model, interaction_cols = inter, interaction_cols_2 = inter_2)
+     
+     check_nums = c(leq_5pp = check_2$bad,
+                    leq_1pp = check_2$v_bad, 
+                    fail = check_2$fail)
+     s = survey_sim %>% group_by(recode_pid_3way,recode_female, recode_age_bucket,
+                                 recode_educ_3way) %>% count() %>%
+         mutate(n_s = round(n/nrow(survey_sim), 3))
+     c = cces %>% group_by(recode_pid_3way, recode_female, recode_age_bucket,
+                           recode_educ_3way) %>% count() %>%
+         mutate(n_c = round(n/nrow(cces), 3))
+     count = nrow(c) - nrow(s)
+     
+     ############################################
+     ## Unweighted estimate
+     ############################################
+     
+     unweighted <- est_mean("outcome", survey_design)
+     ############################################
+     ## Sample size
+     ############################################\
+     n <- sum(sample) 
+     
+     ############################################
+     ## Raking on demographics (no education)
+     ############################################
+     rake_demos_noeduc_svyd <- try(calibrate(design = survey_design,
+                                         formula = formula_rake_demos_noeduc,
+                                         population = targets_rake_demos_noeduc,
+                                         calfun = "raking"), silent = T)
+     
+     rake_demos_noeduc <- tryCatch(est_mean("outcome", rake_demos_noeduc_svyd), error = function(e) NA)
+     
+     #SEs
+     if(class(rake_demos_noeduc_svyd)[1] == "try-error") {
+         rake_demos_noeduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY  = NA)
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_noeduc, outcome ~ .), 
+                                  data = rake_demos_noeduc_svyd$variables))
+ 
+         res_rake_demos_noeduc = data.frame(min = min(residuals), 
+                                            perc_25 = quantile(residuals, .25), 
+                                            mean = mean(residuals),
+                                            perc_75 = quantile(residuals, .75),
+                                            var = var(residuals))
+ 
+         rake_demos_noeduc_se <- calc_SEs(Y = rake_demos_noeduc_svyd$variables$outcome, 
+                                          residuals = residuals, 
+                                          pop_size = nrow(cces), 
+                                          sample_size = sum(sample),
+                                          weights = weights(rake_demos_noeduc_svyd))
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         
+         if(coverage_eval) {
+             rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY = data.frame(svymean(~outcome, rake_demos_noeduc_svyd, na.rm = TRUE))[1,2])
+         }
+     }
+     ############################################
+     #### Raking on demographics (with education)
+     ############################################
+     
+     rake_demos_weduc_svyd <- try(calibrate(design = survey_design,
+                                        formula = formula_rake_demos_weduc,
+                                        population = targets_rake_demos_weduc,
+                                        calfun = "raking"), silent = T)
+ 
+     rake_demos_weduc <- tryCatch(est_mean("outcome", rake_demos_weduc_svyd), 
+                                 error = function(e) NA)
+     
+     
+     #SEs
+     if(class(rake_demos_weduc_svyd)[1] == "try-error") {
+         rake_demos_weduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_weduc, outcome ~ .), 
+                                  data = rake_demos_weduc_svyd$variables))
+         res_rake_demos_wedu = data.frame(min = min(residuals), 
+                                          perc_25 = quantile(residuals, .25), 
+                                          mean = mean(residuals),
+                                          perc_75 = quantile(residuals, .75),
+                                          var = var(residuals))
+         rake_demos_weduc_se <- calc_SEs(Y = rake_demos_weduc_svyd$variables$outcome, 
+                                         residuals = residuals, 
+                                         pop_size = nrow(cces),
+                                         sample_size = sum(sample),
+                                         weights = weights(rake_demos_weduc_svyd))
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     }
+     
+     ############################################
+     #### Raking on everything
+     ############################################
+     rake_all_svyd <- try(calibrate(design = survey_design,
+                                formula = formula_rake_all_vars,
+                                population = targets_rake_all_vars,
+                                calfun = "raking"))
+     
+     rake_all <- tryCatch(est_mean("outcome", rake_all_svyd), 
+                          error = function(e) NA)
+     #SEs
+     if(class(rake_all_svyd)[1] == "try-error") {
+         rake_all_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_all_vars, outcome ~ .), 
+                                  data = rake_all_svyd$variables))
+         res_rake_all = data.frame(min = min(residuals), 
+                                   perc_25 = quantile(residuals, .25), 
+                                   mean = mean(residuals),
+                                   perc_75 = quantile(residuals, .75),
+                                   var = var(residuals))
+         rake_all_se <- calc_SEs(Y = rake_all_svyd$variables$outcome, 
+                                 residuals = residuals, 
+                                 pop_size = nrow(cces), 
+                                 sample_size = sum(sample),
+                                 weights = weights(rake_all_svyd))
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     }
+     
+     ############################################
+     ## Post-stratification: Old Formula
+     ############################################
+     
+     #track empty cells:
+     #this subsets cces strata to only those in survey_sim
+     missing_strata <- unique(cces$strata)[!(unique(cces$strata) %in%
+                                                 unique(survey_sim$strata))]
+     cat(round(length(missing_strata)/ length(unique(cces$strata)),3),
+         "% cces original strata missing from sample, ",
+         " and", cces %>% filter(strata %in% missing_strata) %>% summarise(n()) %>% pull(), "/", nrow(cces), "units\n" )
+     dropped_cells = cces %>% filter(strata %in% missing_strata) %>% group_by(strata) %>% count()
+     #dropped_cells = data.frame(sum = sum(dropped_cells$n), strata = paste(dropped_cells$strata, collapse = " | "))
+     dropped_cells = sum(dropped_cells$n)
+     missing_strata_reduc <- unique(cces$strata_reduc)[!(unique(cces$strata_reduc) %in%
+                                                             unique(survey_sim$strata_reduc))]
+     cat(round(length(missing_strata_reduc)/ length( unique(cces$strata_reduc)),3),
+         "% cces reduc strata missing from sample, ",
+         " and", cces %>% filter(strata_reduc %in% missing_strata_reduc) %>% summarise(n()) %>% pull(), "/", nrow(cces), "units\n" )
+     dropped_cells_reduc = cces %>% 
+         filter(strata_reduc %in% missing_strata_reduc) %>% group_by(strata) %>% count()
+     dropped_cells_reduc = sum(dropped_cells_reduc$n)
+     #dropped_cells_reduc = data.frame(sum = sum(dropped_cells_reduc$n), strata = paste(dropped_cells_reduc$strata, collapse = " | "))
+ 
+     missing_strata_all <- unique(cces$strata_all)[!(unique(cces$strata_all) %in%
+                                                         unique(survey_sim$strata_all))]
+     cat(round(length(missing_strata_all)/ length( unique(cces$strata_all)),3),
+         "% cces all strata missing from sample, ",
+         "and", cces %>% filter(strata_all %in% missing_strata_all) %>% summarise(n()) %>% pull(), "/", nrow(cces), "units\n")
+     dropped_cells_all = cces %>% 
+         filter(strata_all %in% missing_strata_all) %>% group_by(strata) %>% count() 
+     dropped_cells_all = sum(dropped_cells_all$n)
+     #dropped_cells_all = data.frame(sum = sum(dropped_cells_all$n), strata = paste(dropped_cells_all$strata, collapse = " | "))
+ 
+     #note that we no longer have the issue of pew having strata that cces doesn bc
+     #we use survey_sim a sample of cces so it will never have diff strata
+     
+     post_stratification_svyd = svydesign(~1, data = postStrat(survey_sim, 
+                                                               cces_counts, "w", 
+                                                               strata_pass = "strata", 
+                                                               warn = F),
+                                          weights = ~w)
+     
+     post_stratification <- est_mean("outcome", post_stratification_svyd)
+     
+     #SEs
+     post_stratification_se <- data.frame(post_strat_SE_svy = data.frame(svymean(~outcome, post_stratification_svyd, na.rm = TRUE))[1,2])
+     
+     ############################################
+     ## Post-stratification: Reduced
+     ############################################
+     post_strat_reduc_svyd = svydesign(~1, data = postStrat(survey_sim, 
+                                                            cces_reduc_counts, "w_reduc", 
+                                                            strata_pass = "strata_reduc", 
+                                                            warn  = F),
+                                       weights = ~w)
+     
+     post_strat_reduc <- est_mean("outcome", post_strat_reduc_svyd)
+     
+     #SEs
+     post_strat_reduc_se <- data.frame(post_strat_reduc_SE_svy = data.frame(svymean(~outcome, post_strat_reduc_svyd, na.rm = TRUE))[1,2])
+ 
+     ############################################
+     ## Post-stratification: All
+     ############################################
+     post_strat_all_svyd = svydesign(~1, data = postStrat(survey_sim, 
+                                                          cces_all_counts, "w_all", 
+                                                          strata_pass = "strata_all", 
+                                                          warn = F),
+                                     weights = ~w)
+     
+     post_strat_all <- est_mean("outcome", post_strat_all_svyd)
+     
+     #SEs
+     post_strat_all_se <- data.frame(post_strat_all_SE_svy = data.frame(svymean(~outcome, post_strat_all_svyd, na.rm = TRUE))[1,2])
+     
+     ############################################
+     #### Raking on true model
+     ############################################
+     #very messy error catching for the moment just a stop gap to see how things look
+     rake_truth_svyd <- try(calibrate(design = survey_design,
+                                      formula = selection_model,
+                                      population = targets_demo_truth,
+                                      maxit = 100,
+                                      calfun = "raking"), silent = T)
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+         rake_truth_svyd <- try(calibrate(design = survey_design,
+                                          formula = selection_model,
+                                          population = targets_demo_truth,
+                                          calfun = "raking",
+                                          maxit = 100,
+                                          epsilon = .009), silent = T)
+     }
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+         rake_truth_se <- data.frame(SE_fixed = NA,
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA) 
+         names(rake_truth_se) = paste0("rake_all_", names(rake_all_se))
+         
+     } else {
+         lambdas <- 10^seq(3, -2, by = -.1)
+         x <- model.matrix(update(selection_model, outcome ~ .),
+                           data = rake_truth_svyd$variables)[, -1]
+         fit <- glmnet(x, 
+                       rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         cv_fit <- cv.glmnet(x, rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         opt_lambda <- cv_fit$lambda.min
+         fit <- cv_fit$glmnet.fit
+         
+         residuals = rake_truth_svyd$variables$outcome - predict(fit, s = opt_lambda, newx = x)
+         res_rake_truth = data.frame(min = min(residuals), 
+                                     perc_25 = quantile(residuals, .25), 
+                                     mean = mean(residuals),
+                                     perc_75 = quantile(residuals, .75),
+                                     var = var(residuals))
+     }
+     
+     rake_truth <- tryCatch(est_mean("outcome", rake_truth_svyd), 
+                            error = function(e) NA)
+     truth_margins <- tryCatch(svymean(margins_formula, rake_truth_svyd), 
+                            error = function(e) NA)
+     
+     rake_truth_se <- tryCatch(calc_SEs(Y = rake_truth_svyd$variables$outcome,
+                                        residuals = residuals,
+                                        pop_size = nrow(cces),
+                                        sample_size = sum(sample),
+                                        weights = weights(rake_truth_svyd)), error = function(e) NA)
+     
+     if(length(rake_truth_se) == 1) {
+         rake_truth_se <- data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+     }
+     names(rake_truth_se) = tryCatch(paste0("rake_truth_", names(rake_truth_se)), error = function(e) NA)
+     
+     #HT and Hayek
+     p_sample <- as.matrix((p_include[sample==1]))
+     
+     #HT
+     ht_truth = sum((cces[sample ==1, "outcome"]/p_sample))/nrow(cces) 
+     #Hayek
+     hayek_truth = sum((cces[sample ==1, "outcome"]/p_sample))/sum(1/p_sample)
+     
+     ############################################
+     ## Kpop: Categorical Data + b = argmax V(K)
+     ############################################
+     if(eval_kpop) {
+ 
+       # Select the covariates for use in Kbal: updated cat data no cont age
+       #one-hot coded for cat kernel
+       kbal_data <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                           recode_female,
+                                                           recode_race,
+                                                           recode_region,
+                                                           recode_pid_3way,
+                                                           recode_educ,
+ 
+                                                           recode_income_5way,
+                                                           recode_relig_6way,
+                                                           recode_born,
+                                                           recode_attndch_4way),
+                              cces %>% dplyr::select(recode_age_bucket,
+                                                     recode_female,
+                                                     recode_race,
+                                                     recode_region,
+                                                     recode_pid_3way,
+                                                     recode_educ,
+ 
+                                                     recode_income_5way,
+                                                     recode_relig_6way,
+                                                     recode_born,
+                                                     recode_attndch_4way))
+       
+       kbal_data_reduc <- bind_rows(survey_sim %>% dplyr::select(all.vars(selection_model)),
+                              cces %>% dplyr::select(all.vars(selection_model)))
+       
+       kbal_data_sampled <- c(rep(1, nrow(survey_sim)), rep(0, nrow(cces)))
+       ##### Demos Constraint
+       rake_demos_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                       recode_female,
+                                                                       recode_race,
+                                                                       recode_region,
+                                                                       recode_pid_3way),
+                                          cces %>% dplyr::select(recode_age_bucket,
+                                                                 recode_female,
+                                                                 recode_race,
+                                                                 recode_region,
+                                                                 recode_pid_3way))%>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_demos_constraint <- rake_demos_constraint[,-1]
+       rake_demos_constraint <- scale(rake_demos_constraint)
+ 
+ 
+       rake_demos_wedu_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                            recode_female,
+                                                                            recode_race,
+                                                                            recode_region,
+                                                                            recode_pid_3way,
+                                                                            recode_educ),
+                                               cces %>% dplyr::select(recode_age_bucket,
+                                                                      recode_female,
+                                                                      recode_race,
+                                                                      recode_region,
+                                                                      recode_pid_3way,
+                                                                      recode_educ))%>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_demos_wedu_constraint <- rake_demos_wedu_constraint[,-1]
+       rake_demos_wedu_constraint <- scale(rake_demos_wedu_constraint)
+ 
+ 
+       rake_all_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                     recode_female,
+                                                                     recode_race,
+                                                                     recode_region,
+                                                                     recode_pid_3way,
+                                                                     recode_educ,
+ 
+                                                                     recode_income_5way,
+                                                                     recode_relig_6way,
+                                                                     recode_born,
+                                                                     recode_attndch_4way),
+                                        cces %>% dplyr::select(recode_age_bucket,
+                                                               recode_female,
+                                                               recode_race,
+                                                               recode_region,
+                                                               recode_pid_3way,
+                                                               recode_educ,
+                                                               recode_income_5way,
+                                                               recode_relig_6way,
+                                                               recode_born,
+                                                               recode_attndch_4way)) %>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_all_constraint <- rake_all_constraint[,-1]
+       rake_all_constraint <- scale(rake_all_constraint)
+       
+       
+         #cat("===================== Running Kbal ==================\n")
+         
+         #### DEFAULT ######
+         cat(paste("nsim:", nsim, "DEFAULT", "\n"))
+         kbal_est <- kbal(allx=kbal_data,
+                          sampled = kbal_data_sampled,
+                          #b = b_manual[i],
+                          cat_data = TRUE,
+                          incrementby = increment,
+                          meanfirst = FALSE,
+                          ebal.tol = tolerance,
+                          ebal.maxit = maxit,
+                          minnumdims = min_num_dims,
+                          maxnumdims = max_num_dims,
+                          linkernel = if(TEST){TRUE} else{ FALSE},
+                          sampledinpop = FALSE,
+                          fullSVD = TRUE)
+         
+         kpop_svyd <- svydesign(~1, data = survey_sim,
+                                weights = kbal_est$w[kbal_data_sampled ==1])
+         
+         kpop <- est_mean("outcome", kpop_svyd)
+         b_kpop = kbal_est$b
+         #save memory by saving only the svd to re use
+         svdK = kbal_est$svdK 
+         numdims = kbal_est$numdims
+         biasbound_r = kbal_est$biasbound_ratio
+         biasbound = kbal_est$biasbound_opt
+         
+         ##### Kpop SEs
+         kpop <- tryCatch(est_mean("outcome", kpop_svyd), error = function(e) NA)
+         
+         lambdas <- if(manual_lambda) { 10^seq(3, -2, by = -.1) } else {NULL}
+         
+         x <- as.matrix(data.frame(kbal_dims = kbal_est$svdK$v[, 1:kbal_est$numdims]))
+         cv_fit <- cv.glmnet(x, kpop_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+         
+         residuals = kpop_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                           s = lambda_pass, newx = x)
+         res_kpop = data.frame(min = min(residuals), 
+                               perc_25 = quantile(residuals, .25), 
+                               mean = mean(residuals),
+                               perc_75 = quantile(residuals, .75),
+                               var = var(residuals))
+         kpop_se <- tryCatch(calc_SEs(Y = kpop_svyd$variables$outcome,
+                                      residuals = residuals,
+                                      pop_size = nrow(cces),
+                                      sample_size = sum(sample),
+                                      weights = weights(kpop_svyd)), error = function(e) NA)
+         
+         if(length(kpop_se) == 1) {
+             kpop_se <- data.frame(SE_fixed = NA, 
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA)
+         }
+         names(kpop_se) = tryCatch(paste0("kpop_", names(kpop_se)), error = function(e) NA)
+         
+         
+         # krls_kpop = KRLS::krls(X = kbal_est$onehot_data[kbal_data_sampled==1,],
+         #                        y = kpop_svyd$variables$outcome)
+         # residuals_krls = kpop_svyd$variables$outcome - krls_kpop$fitted
+         # kpop_krls_se <- tryCatch(calc_SEs(Y = kpop_svyd$variables$outcome,
+         #                              residuals = residuals_krls,
+         #                              pop_size = nrow(cces),
+         #                              sample_size = sum(sample),
+         #                              weights = weights(kpop_svyd)), error = function(e) NA)
+         # if(length(kpop_krls_se) == 1) {
+         #     kpop_krls_se <- data.frame(SE_fixed = NA, 
+         #                              SE_quasi = NA, 
+         #                               SE_linear = NA, 
+         #                               SE_chad = NA)
+         # }
+         # names(kpop_krls_se) = tryCatch(paste0("kpop_", names(kpop_krls_se)), error = function(e) NA)
+         
+     
+         #CONVERGED
+         dist_record = data.frame(t(kbal_est$dist_record))
+         min_converged = dist_record[which.min(dist_record[dist_record$Ebal.Convergence ==1,"BiasBound"]), "Dims"]
+         
+         rm(kbal_est, residuals, x, cv_fit)
+         
+         #CONVERGED
+         #### CONVG ####
+         cat(paste("nsim:", nsim, "CONV", "\n"))
+         if(is.null(min_converged) | length(min_converged) ==0) {
+           kpop_svyd_conv <- "dn converge"
+           kpop_conv <- "dn converge"
+           
+           numdims_conv = "dn converge"
+           biasbound_r_conv = "dn converge"
+           biasbound_conv = "dn converge"
+           kpop_conv_se = data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           
+         } else {
+           kbal_est_conv <- kbal(allx=kbal_data,
+                                 K.svd = svdK,
+                                 sampled = kbal_data_sampled,
+                                 numdims = min_converged,
+                                 ebal.tol = tolerance,
+                                 ebal.maxit = maxit,
+                                 minnumdims = min_num_dims,
+                                 maxnumdims = max_num_dims,
+                                 scale_data = FALSE,
+                                 drop_MC = FALSE,
+                                 incrementby = increment,
+                                 meanfirst = FALSE,
+                                 sampledinpop = FALSE,
+                                 ebal.convergence = TRUE)
+           kpop_svyd_conv <- svydesign(~1, data = survey_sim,
+                                       weights = kbal_est_conv$w[kbal_data_sampled ==1])
+           kpop_conv <- est_mean("outcome", kpop_svyd_conv)
+           
+           numdims_conv = kbal_est_conv$numdims
+           biasbound_r_conv = kbal_est_conv$biasbound_ratio
+           biasbound_conv = kbal_est_conv$biasbound_opt
+           
+           #SEs
+           x <- as.matrix(data.frame(kbal_dims = kbal_est_conv$svdK$v[, 1:kbal_est_conv$numdims]))
+           cv_fit <- cv.glmnet(x, kpop_svyd_conv$variables$outcome, alpha = 0, 
+                               lambda = lambdas)
+           fit <- cv_fit$glmnet.fit
+           lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+           residuals = kpop_svyd_conv$variables$outcome - predict(cv_fit$glmnet.fit, 
+                                                                  s = lambda_pass, 
+                                                                            newx = x)
+           res_kpop_conv = data.frame(min = min(residuals), 
+                                      perc_25 = quantile(residuals, .25), 
+                                      mean = mean(residuals),
+                                      perc_75 = quantile(residuals, .75),
+                                      var = var(residuals))
+           kpop_conv_se <- tryCatch(calc_SEs(Y = kpop_svyd_conv$variables$outcome,
+                                        residuals = residuals,
+                                        pop_size = nrow(cces),
+                                        sample_size = sum(sample),
+                                        weights = weights(kpop_svyd_conv)), error = function(e) NA)
+           if(length(kpop_conv_se) == 1) {
+               kpop_conv_se <- data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           }
+           names(kpop_conv_se) = tryCatch(paste0("kpop_conv_", names(kpop_conv_se)), error = function(e) NA)
+          #KRLS SEs are exactly the same for coverged
+           rm(kbal_est_conv, residuals, x, cv_fit) 
+         }
+         
+         
+         ####### MF #######
+         cat(paste("nsim:", nsim, "MEANFIRST", "\n"))
+         #### BROKEN HERE
+         kbal_mf_est <- kbal(K.svd = svdK,
+                             cat_data = T,
+                             allx=kbal_data,
+                             sampled = kbal_data_sampled,
+                             ebal.tol = tolerance,
+                             ebal.maxit = maxit,
+                             minnumdims = min_num_dims,
+                             maxnumdims = max_num_dims,
+                             incrementby = increment,
+                             meanfirst = TRUE,
+                             sampledinpop = FALSE)
+         
+         kpop_mf_svyd <- svydesign(~1, data = survey_sim, 
+                                   weights = kbal_mf_est$w[kbal_data_sampled ==1])
+         
+         kpop_mf <- est_mean("outcome", kpop_mf_svyd)
+         
+         mfnumdims = kbal_mf_est$numdims
+         mf_appended_dims = kbal_mf_est$meanfirst_dims
+         if(is.null(mf_appended_dims)) {mf_appended_dims = c(NA)}
+         biasbound_r_mf = kbal_mf_est$biasbound_ratio
+         biasbound_mf = kbal_mf_est$biasbound_opt
+         
+         if(is.null(mfnumdims)) {
+             mfnumdims = c(NA) 
+             kpop_mf_se = data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+         } else {
+             #after much hair puling i have determined the following
+             #1. it is best to allow glmnet to add the intercept (it's not receptive to turning it off if you have it already bc you use model matrix; also bc the penalty.factor will always regularize the intercept when it's intercept when its internally aded)
+             #2. glmnet is entirely redundant when you run cv.glmnet, but it is very important to predict with cv_glmnet$glmnet.fit + s = lambda.min otherwise you get different answers, BUT no errors (eg predict on the raw cv.glmnet obj)
+             #3. penalty.factor will rescale the lambdas internally except when the user specifies the lambda sequence directly
+             #4. you do not need to anticipate the addition of the intercept in penalty.factor when it's added internally
+             #5. FOR GODS SAKE dont be an idiot and subset with only a vector name (kbal_data_sampled), make it a logical ==1!!!!! 
+             #SEs X = V[, 1:numdims]
+             V <-  data.frame(kbal_dims = kbal_mf_est$svdK$v[, c(1:kbal_mf_est$numdims)])
+             #binding mf cols for sample units to V
+             X <- as.matrix(cbind(kbal_mf_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+ 
+             cv_fit <- cv.glmnet(X, kpop_mf_svyd$variables$outcome, alpha = 0,
+                                 lambda = lambdas, 
+                                 penalty.factor = c(rep(0, kbal_mf_est$meanfirst_dims), 
+                                                    rep(1, kbal_mf_est$numdims)))
+             lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+             residuals = kpop_mf_svyd$variables$outcome - predict(cv_fit$glmnet.fit, 
+                                                                  s = lambda_pass, 
+                                                                  newx = X)
+             res_kpop_mf = data.frame(min = min(residuals), 
+                                      perc_25 = quantile(residuals, .25), 
+                                      mean = mean(residuals),
+                                      perc_75 = quantile(residuals, .75),
+                                      var = var(residuals))
+             
+             kpop_mf_se <- tryCatch(calc_SEs(Y = kpop_mf_svyd$variables$outcome,
+                                               residuals = residuals,
+                                               pop_size = nrow(cces),
+                                               sample_size = sum(sample),
+                                               weights = weights(kpop_mf_svyd)), 
+                                    error = function(e) NA)
+             if(length(kpop_mf_se) == 1) {
+                 kpop_mf_se <- data.frame(SE_fixed = NA, 
+                                       SE_quasi = NA, 
+                                       SE_linear = NA, 
+                                       SE_chad = NA)
+             }
+             names(kpop_mf_se) = tryCatch(paste0("kpop_mf_", names(kpop_mf_se)),
+                                          error = function(e) NA)
+             
+             
+             
+         }
+ 
+         rm(kbal_mf_est, residuals,X, V, cv_fit)
+         
+         #########demos constraint method:
+         cat(paste("nsim:", nsim, "CONSTR", "\n"))
+         kbal_demos_est <- kbal(K.svd = svdK,
+                                allx=kbal_data,
+                                #cat_data = TRUE,
+                                sampled = kbal_data_sampled,
+                                ebal.tol = tolerance,
+                                ebal.maxit = maxit,
+                                minnumdims = min_num_dims,
+                                maxnumdims = max_num_dims,
+                                scale_data = FALSE,
+                                drop_MC = FALSE,
+                                incrementby = increment,
+                                #scaling these
+                                constraint = rake_demos_constraint,
+                                meanfirst = FALSE,
+                                sampledinpop = FALSE)
+         kpop_demos_svyd <- svydesign(~1, data = survey_sim, 
+                                      weights = kbal_demos_est$w[kbal_data_sampled ==1])
+         
+         kpop_demos <- est_mean("outcome", kpop_demos_svyd)
+         
+         numdims_demos = kbal_demos_est$numdims
+         if(is.null(numdims_demos)) {
+             numdims_demos = c(NA) 
+             kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                      SE_quasi = NA, 
+                                      SE_linear = NA, 
+                                      SE_chad = NA)
+         } else {
+             V <-  data.frame(kbal_dims = kbal_demos_est$svdK$v[, c(1:kbal_demos_est$numdims)])
+             X <- as.matrix(cbind(kbal_demos_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+             
+             cv_fit <- cv.glmnet(X, kpop_demos_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                 penalty.factor = c(rep(0, ncol(kbal_demos_est$appended_constraint_cols)), rep(1, kbal_demos_est$numdims)))
+             
+             lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+             residuals =  kpop_demos_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                      s = lambda_pass, 
+                                                                  newx = X)
+             res_kpop_demos = data.frame(min = min(residuals), 
+                                         perc_25 = quantile(residuals, .25), 
+                                         mean = mean(residuals),
+                                         perc_75 = quantile(residuals, .75),
+                                         var = var(residuals))
+             
+             kpop_demos_se <- tryCatch(calc_SEs(Y = kpop_demos_svyd$variables$outcome,
+                                                residuals = residuals,
+                                                pop_size = nrow(cces),
+                                                sample_size = sum(sample),
+                                                weights = weights(kpop_demos_svyd)), 
+                                       error = function(e) NA)
+             if(length(kpop_demos_se) == 1) {
+                 kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                             SE_quasi = NA, 
+                                             SE_linear = NA, 
+                                             SE_chad = NA)
+             }
+             names(kpop_demos_se) = tryCatch(paste0("kpop_demos_", names(kpop_demos_se)),
+                                             error = function(e) NA)
+         }
+         biasbound_r_demos = kbal_demos_est$biasbound_ratio
+         biasbound_demos = kbal_demos_est$biasbound_opt
+         
+         rm(kbal_demos_est, residuals, X, V, cv_fit)
+         
+         
+         #########demos + educ constraint method:
+         cat(paste("nsim:", nsim, "CONSTR", "\n"))
+         kbal_demos_wedu_est <- kbal(K.svd = svdK,
+                                     allx=kbal_data,
+                                     cat_data = TRUE,
+                                     sampled = kbal_data_sampled,
+                                     ebal.tol = tolerance,
+                                     ebal.maxit = maxit,
+                                     minnumdims = min_num_dims,
+                                     maxnumdims = max_num_dims,
+                                     scale_data = FALSE,
+                                     drop_MC = FALSE,
+                                     incrementby = increment,
+                                     #scaling these
+                                     constraint = rake_demos_wedu_constraint,
+                                     meanfirst = FALSE,
+                                     sampledinpop = FALSE)
+         kpop_demos_wedu_svyd <- svydesign(~1, data = survey_sim, 
+                                           weights = kbal_demos_wedu_est$w[kbal_data_sampled ==1])
+         
+         kpop_demos_wedu <- est_mean("outcome", kpop_demos_wedu_svyd)
+         
+         numdims_demos_wedu = kbal_demos_wedu_est$numdims
+         if(is.null(numdims_demos_wedu)) {
+             numdims_demos_wedu = c(NA)
+             kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                         SE_quasi = NA, 
+                                         SE_linear = NA, 
+                                         SE_chad = NA)
+         } else {
+             V <-  data.frame(kbal_dims = kbal_demos_wedu_est$svdK$v[, c(1:kbal_demos_wedu_est$numdims)])
+             X <- as.matrix(cbind(kbal_demos_wedu_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+             
+             cv_fit <- cv.glmnet(X, kpop_demos_wedu_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                 penalty.factor = c(rep(0, ncol(kbal_demos_wedu_est$appended_constraint_cols)),
+                                                    rep(1, kbal_demos_wedu_est$numdims)))
+             
+             lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+             residuals =  kpop_demos_wedu_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                           s = lambda_pass, 
+                                                                      newx = X)
+             res_kpop_demos_wedu = data.frame(min = min(residuals), 
+                                              perc_25 = quantile(residuals, .25), 
+                                              mean = mean(residuals),
+                                              perc_75 = quantile(residuals, .75),
+                                              var = var(residuals))
+             kpop_demos_wedu_se <- tryCatch(calc_SEs(Y = kpop_demos_wedu_svyd$variables$outcome,
+                                                residuals = residuals,
+                                                pop_size = nrow(cces),
+                                                sample_size = sum(sample),
+                                                weights = weights(kpop_demos_wedu_svyd)), 
+                                       error = function(e) NA)
+             if(length(kpop_demos_wedu_se) == 1) {
+                 kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                             SE_quasi = NA, 
+                                             SE_linear = NA, 
+                                             SE_chad = NA)
+             }
+             names(kpop_demos_wedu_se) = tryCatch(paste0("kpop_demos_wedu_", names(kpop_demos_wedu_se)),
+                                             error = function(e) NA)
+             
+         }
+         biasbound_r_demos_wedu = kbal_demos_wedu_est$biasbound_ratio
+         biasbound_demos_wedu = kbal_demos_wedu_est$biasbound_opt
+         
+         rm(kbal_demos_wedu_est, residuals, X, V, cv_fit)
+         
+         
+         #########all constraint method:
+         cat(paste("nsim:", nsim, "CONSTR", "\n"))
+         kbal_all_est <- kbal(K.svd = svdK,
+                              allx=kbal_data,
+                              #cat_data = TRUE,
+                              sampled = kbal_data_sampled,
+                              ebal.tol = tolerance,
+                              ebal.maxit = maxit,
+                              minnumdims = min_num_dims,
+                              maxnumdims = max_num_dims,
+                              scale_data = FALSE,
+                              drop_MC = FALSE,
+                              incrementby = increment,
+                              #scaling these
+                              constraint = rake_all_constraint,
+                              meanfirst = FALSE,
+                              sampledinpop = FALSE)
+         kpop_all_svyd <- svydesign(~1, data = survey_sim, 
+                                    weights = kbal_all_est$w[kbal_data_sampled ==1])
+         
+         kpop_all <- est_mean("outcome", kpop_all_svyd)
+         
+         numdims_all = kbal_all_est$numdims
+         if(is.null(numdims_all)) {
+             numdims_all = c(NA)
+             numdims_all_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+         } else {
+             V <-  data.frame(kbal_dims = kbal_all_est$svdK$v[, c(1:kbal_all_est$numdims)])
+             X <- as.matrix(cbind(kbal_all_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+             
+             cv_fit <- cv.glmnet(X, kpop_all_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                 penalty.factor = c(rep(0, ncol(kbal_all_est$appended_constraint_cols)),
+                                                    rep(1, kbal_all_est$numdims)))
+             
+             lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+             residuals =  kpop_all_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                    s = lambda_pass, 
+                                                                           newx = X)
+             res_kpop_all = data.frame(min = min(residuals), 
+                                       perc_25 = quantile(residuals, .25), 
+                                       mean = mean(residuals),
+                                       perc_75 = quantile(residuals, .75),
+                                       var = var(residuals))
+             kpop_all_se <- tryCatch(calc_SEs(Y = kpop_all_svyd$variables$outcome,
+                                                     residuals = residuals,
+                                                     pop_size = nrow(cces),
+                                                     sample_size = sum(sample),
+                                                     weights = weights(kpop_all_svyd)), 
+                                            error = function(e) NA)
+             if(length(kpop_demos_wedu_se) == 1) {
+                 kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                  SE_quasi = NA, 
+                                                  SE_linear = NA, 
+                                                  SE_chad = NA)
+             }
+             names(kpop_all_se) = tryCatch(paste0("kpop_all_", names(kpop_all_se)),
+                                                  error = function(e) NA)
+             
+         }
+         
+         biasbound_r_all = kbal_all_est$biasbound_ratio
+         biasbound_all = kbal_all_est$biasbound_opt
+         
+         rm(kbal_all_est, residuals, X,V, cv_fit)
+         
+         rm(svdK)
+   
+          ##### return
+         kpop_res = list()
+         b_out = b_kpop
+        # b_reduc = b_kpop_reduc
+         b = b_out
+         kpop_res$sims = data.frame(b_out,
+                               kpop = kpop,
+                               kpop_mf = kpop_mf,
+                               kpop_conv = kpop_conv,
+                               kpop_demos = kpop_demos,
+                               kpop_demos_wedu = kpop_demos_wedu,
+                               kpop_all = kpop_all,
+                               bb = biasbound,
+                               bbr = biasbound_r,
+                               bb_conv = biasbound_conv,
+                               bbr_conv = biasbound_r_conv,
+                               bb_mf = biasbound_mf,
+                               bbr_mf = biasbound_r_mf,
+                               bb_demos = biasbound_demos,
+                               bbr_demos = biasbound_r_demos,
+                               bb_demos_wedu = biasbound_demos_wedu,
+                               bbr_demos_wedu = biasbound_r_demos_wedu,
+                               bb_all = biasbound_all,
+                               bbr_all = biasbound_r_all,
+                               numdims,
+                               numdims_conv,
+                               mfnumdims, 
+                               mf_appended_dims, 
+                               numdims_demos,
+                               numdims_demos_wedu,
+                               numdims_all)
+         
+         #Starndard Errors:
+         if(coverage_eval) {
+             kpop_res$SEs = data.frame(rake_demos_noeduc_se,
+                                  rake_demos_noeduc_se_SVY,
+                                  rake_demos_weduc_se,
+                                  rake_all_se,
+                                  post_stratification_se,
+                                  post_strat_reduc_se,
+                                  post_strat_all_se,
+                                  rake_truth_se,
+                                  kpop_se,
+                                  kpop_conv_se,
+                                  kpop_mf_se,
+                                  kpop_demos_se,
+                                  kpop_demos_wedu_se,
+                                  kpop_all_se)
+         } else {
+             kpop_res$SEs = data.frame(rake_demos_noeduc_se,
+                                  rake_demos_weduc_se,
+                                  rake_all_se,
+                                  post_stratification_se,
+                                  post_strat_reduc_se,
+                                  post_strat_all_se,
+                                  rake_truth_se,
+                                  kpop_se,
+                                  kpop_conv_se,
+                                  kpop_mf_se,
+                                  kpop_demos_se,
+                                  kpop_demos_wedu_se,
+                                  kpop_all_se)
+         }
+         
+         
+         #weights
+         kpop_res$weights = list(b = b_out,
+                            kpop_w = weights(kpop_svyd),
+                            #kpop_w_reduc = weights(kpop_svyd_reduc),
+                            kpop_w_conv = weights(kpop_svyd_conv),
+                            kpop_mf_w = weights(kpop_mf_svyd), 
+                            kpop_demos_w = weights(kpop_demos_svyd),
+                            kpop_demos_wedu_w = weights(kpop_demos_wedu_svyd),
+                            kpop_all_w = weights(kpop_all_svyd))
+         
+         #residuals
+         kpop_res$residuals = rbind(b = b_out,
+                            kpop = res_kpop,
+                           # kpop_w_reduc = res_kpop_reduc,
+                            kpop_conv = res_kpop_conv,
+                            kpop_mf = res_kpop_mf,
+                            kpop_demos = res_kpop_demos,
+                            kpop_demos_wedu = res_kpop_demos_wedu,
+                            kpop_all = res_kpop_all,
+                            rake_truth = res_rake_truth,
+                            rake_demos = res_rake_demos_noeduc,
+                            rake_demos_wedu = res_rake_demos_wedu ,
+                            rake_all = res_rake_all
+                           )
+         
+         ######## Kpop Margins ########
+         
+         kpop_res$km <- round(cbind(b = b_out/100,
+                               kpop = svymean(margins_formula, kpop_svyd),
+                              # kpop_reduc = svymean(margins_formula, kpop_svyd_reduc),
+                               kpop_conv = svymean(margins_formula, kpop_svyd_conv),
+                               kpop_mf = svymean(margins_formula, kpop_mf_svyd),
+                               kpop_demos = svymean(margins_formula, kpop_demos_svyd),
+                               kpop_demos_wedu = svymean(margins_formula, kpop_demos_wedu_svyd),
+                               kpop_all = svymean(margins_formula, kpop_all_svyd)) * 100,
+                         4)
+         
+         rm(kpop_svyd, 
+            #kpop_svyd_reduc,
+            kpop_mf_svyd, kpop_svyd_conv, kpop_demos_svyd,
+            kpop_demos_wedu_svyd, kpop_all_svyd)
+       
+     }
+     
+     ############################################ OUTPUT
+     out = list()
+     if(eval_kpop) {
+       out$sims = cbind(nsim,
+                        n,
+                        unweighted,
+                        rake_demos_noeduc,
+                        rake_demos_weduc,
+                        rake_all,
+                        post_stratification,
+                        post_strat_reduc,
+                        post_strat_all,
+                        rake_truth,
+                        ht_truth, 
+                        hayek_truth,
+                        kpop_res$sims)
+       
+       out$SEs = kpop_res$SEs
+       out$weights = kpop_res$weights
+       out$residuals = kpop_res$residuals
+       out$dropped_cells = c(dropped_cells = dropped_cells,
+                             dropped_cells_reduc = dropped_cells_reduc,
+                             dropped_cells_all = dropped_cells_all)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      #chaningn temporarily to the fuller view from check_nums
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+       
+       margin <- round(cbind(sample = svymean(margins_formula, survey_design),
+                             cces =  svymean(margins_formula, cces_svy),
+                             rake_demos_noeduc = svymean(margins_formula,
+                                                         rake_demos_noeduc_svyd),
+                             rake_demos_weduc = svymean(margins_formula,
+                                                        rake_demos_weduc_svyd),
+                             rake_all = svymean(margins_formula,
+                                                rake_all_svyd),
+                             post_stratification = svymean(margins_formula,
+                                                           post_stratification_svyd),
+                             post_strat_reduc = svymean(margins_formula,
+                                                        post_strat_reduc_svyd),
+                             post_strat_all = svymean(margins_formula,
+                                                      post_strat_all_svyd),
+                             rake_truth = truth_margins) * 100, 5)
+       
+       #these are just means so let's not multiply by 100
+       margin["recode_age",] <- margin["recode_age",]/100 
+       margin["recode_agesq",] <- margin["recode_agesq",]/100
+       margin["recode_agecubed",] <- margin["recode_agecubed",]/100
+       
+       margin = cbind(margin,
+                      kpop_res$km)
+       
+       margin <- margin[,grepl("km.kpop_res", colnames(margin))|
+                          !grepl("km.", colnames(margin)) ]
+       
+       # colnames(margin)[grepl("km.kpop_res", colnames(margin))] <- unlist(lapply(b_out, function(x) c(paste0("kpop_b", round(x,3)),
+       #                                                                                        paste0("kpop_cvg_b", round(x,3)), 
+       #                                                                                        paste0("kpop_mf_b", round(x,3)), 
+       #                                                                                        paste0("kpop_demos_b", round(x,3)),
+       #                                                                                        paste0("kpop_demos_wedu_b", round(x,3)) ,
+       #                                                                                        paste0("kpop_all_b", round(x,3)) ) ))
+     } else {
+      ##### NO KPOP EVAL
+     if(!coverage_eval) {
+         margin <- round(cbind(sample = svymean(margins_formula, survey_design),
+                               cces =  svymean(margins_formula, cces_svy),
+                               rake_demos_noeduc = svymean(margins_formula,
+                                                           rake_demos_noeduc_svyd),
+                               rake_demos_weduc = svymean(margins_formula,
+                                                          rake_demos_weduc_svyd),
+                               rake_all = svymean(margins_formula,
+                                                  rake_all_svyd),
+                               post_stratification = svymean(margins_formula,
+                                                             post_stratification_svyd),
+                               post_strat_reduc = svymean(margins_formula,
+                                                          post_strat_reduc_svyd),
+                               post_strat_all = svymean(margins_formula,
+                                                        post_strat_all_svyd),
+ 
+                               rake_truth = truth_margins) * 100, 5)
+ 
+         #these are just means so let's not multiply by 100
+         margin["recode_age",] <- margin["recode_age",]/100
+         margin["recode_agesq",] <- margin["recode_agesq",]/100
+         margin["recode_agecubed",] <- margin["recode_agecubed",]/100
+     } else {
+         margin = NULL
+     }
+       out$sims = data.frame(nsim, n,
+                             unweighted,
+                             rake_demos_noeduc,
+                             rake_demos_weduc,
+                             rake_all,
+                             post_stratification,
+                             post_strat_reduc,
+                             post_strat_all,
+                             rake_truth,
+                             ht_truth, 
+                             hayek_truth)
+       
+       out$SEs = data.frame(rake_demos_noeduc_se,
+                            rake_demos_weduc_se,
+                            rake_all_se,
+                            post_stratification_se,
+                            post_strat_reduc_se,
+                            post_strat_all_se,
+                            rake_truth_se)
+       out$residuals = rbind(b = NULL,
+                             rake_truth = res_rake_truth,
+                             rake_demos = res_rake_demos_noeduc,
+                             rake_demos_wedu = res_rake_demos_wedu ,
+                             rake_all = res_rake_all
+       )
+       
+       out$dropped_cells = c(dropped_cells = dropped_cells ,
+                             dropped_cells_reduc = dropped_cells_reduc, 
+                             dropped_cells_all= dropped_cells_all)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      #chaningn temporarily to the fuller view from check_nums
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+       
+      
+       
+       if(coverage_eval) {
+           out$SEs = data.frame(rake_demos_noeduc_se,
+                      rake_demos_weduc_se,
+                      rake_demos_noeduc_se_SVY,
+                      rake_all_se,
+                      post_stratification_se,
+                      post_strat_reduc_se,
+                      post_strat_all_se,
+                      rake_truth_se)
+       }
+     } 
+     
+     out$margins = margin
+     })
+     return(out)
+     
+   }, mc.cores = detectCores() - cores_saved) 
+ })
=====================  SIM: 5 ===================== 
=====================  SIM: 4 ===================== 
=====================  SIM: 3 ===================== 
=====================  SIM: 6 ===================== 
=====================  SIM: 2 ===================== 
=====================  SIM: 7 ===================== 
=====================  SIM: 1 ===================== 
0.52 % cces original strata missing from sample,   and 6093 / 44932 units
0.532 % cces original strata missing from sample,   and 5616 / 44932 units
0.528 % cces original strata missing from sample,   and 5847 / 44932 units
0.511 % cces original strata missing from sample,   and 5278 / 44932 units
0.557 % cces original strata missing from sample,   and 7242 / 44932 units
0.53 % cces original strata missing from sample,   and 6055 / 44932 units
0.569 % cces original strata missing from sample,   and 6525 / 44932 units
0.766 % cces reduc strata missing from sample,   and 21365 / 44932 units
0.762 % cces reduc strata missing from sample,   and 21172 / 44932 units
0.768 % cces reduc strata missing from sample,   and 20990 / 44932 units
0.759 % cces reduc strata missing from sample,   and 20420 / 44932 units
0.752 % cces reduc strata missing from sample,   and 20680 / 44932 units
0.759 % cces reduc strata missing from sample,   and 20260 / 44932 units
0.767 % cces reduc strata missing from sample,   and 21098 / 44932 units
0.926 % cces all strata missing from sample,  and 34322 / 44932 units
0.926 % cces all strata missing from sample,  and 34269 / 44932 units
0.927 % cces all strata missing from sample,  and 34266 / 44932 units
0.925 % cces all strata missing from sample,  and 34116 / 44932 units
0.928 % cces all strata missing from sample,  and 34536 / 44932 units
0.927 % cces all strata missing from sample,  and 33852 / 44932 units
0.93 % cces all strata missing from sample,  and 34672 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
Joining with `by = join_by(strata_all)`
nsim: 3 DEFAULT 
nsim: 4 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 1 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 7 DEFAULT 
nsim: 6 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 5 DEFAULT 
nsim: 2 DEFAULT 
Searching for b value which maximizes the variance in K: Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: 5.532 selected 
Building kernel matrix
Running full SVD on kernel matrix 
5.546 selected 
Building kernel matrix
5.539 selected 
Building kernel matrix
5.522 selected 
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Building kernel matrix
Running full SVD on kernel matrix 
5.578 selected 
Building kernel matrix
5.56 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
5.571 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02759  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01719  
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01323  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01085  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02762  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01397  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01129  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02829  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02685  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02582  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01555  
Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01233  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00651  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02979  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00948  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01548  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02823  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01861  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01263  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01454  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01049  
36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00635  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00612  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00607  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01663  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 31 dimensions of K, ebalance convergence isWith  11 TRUEdimensions of K, ebalance convergence is  TRUE yielding biasbound (norm=1) of 0.00763 yielding biasbound (norm=1) of 0.01366  
 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01244  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00664  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01453  
WithWith  With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00586  
16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01314  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01106  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01042  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00581  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00665  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00575  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
WithWith  61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00615  
66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00574  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00652  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00566  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00777  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00652  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00648  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00565  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00629  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00561  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00623  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00606  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00581  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.006  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00561  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00603  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00628  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00589  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0056  
WithWith  76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00582  
66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00623  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00558  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00579  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00577  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00574  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00548  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0057  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00581  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0064  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0054  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00573  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00616  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00515  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00512  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00576  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00573  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00513  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0061  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00575  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00565  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00512  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00564  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00559  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00595  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00515  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00545  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00566  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00596  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00515  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00627  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00565  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00594  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00519  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00545  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00562  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00635  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00518  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00553  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00542  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00595  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00625  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00666  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00554  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00519  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00539  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00578  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00656  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00593  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0052  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00539  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00606  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00665  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00581  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00678  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00593  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00523  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00539  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00573  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00608  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00592  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00672  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00519  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00564  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0054  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00588  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00601  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00511  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00558  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0075  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00543  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0051  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00575  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00756  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00575  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00539  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00607  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00578  
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00791  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00508  
With 151 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00696  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00569  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00528  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.006  
With 141 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00822  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00506  
With 156 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00716  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00567  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 146 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00897  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 121 
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00519  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00499  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00585  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00564  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 161 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00829  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 126 
nsim: 1 CONV 
With 176 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00562  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00528  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00508  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
nsim: 6 CONV 
With 201 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00582  
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 181 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00577  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00513  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0053  
With 186 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00747  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 141 
With user-specified 111 dimensions, biasbound (norm=1) of  0.00684  
nsim: 1 MEANFIRST 
With 186 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00742  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 176 
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00524  
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00507  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
With 206 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00593  
nsim: 5 CONV 
nsim: 3 CONV 
With user-specified 126 dimensions, biasbound (norm=1) of  0.00656  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00515  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00527  
nsim: 6 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 211 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00607  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00527  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00526  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With user-specified 141 dimensions, biasbound (norm=1) of  0.00598  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00515  
nsim: 5 MEANFIRST 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 216 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00685  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00533  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00759  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00863  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With user-specified 171 dimensions, biasbound (norm=1) of  0.00564  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
Selected 32 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00847  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
nsim: 3 MEANFIRST 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00827  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00522  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00819  
Selected 31 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 226 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00516  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00829  
With 221 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00725  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 146 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00885  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00851  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0053  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00875  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00818  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
nsim: 7 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00909  
Re-running at optimal choice of numdims, 1 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 231 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00547  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
nsim: 1 CONSTR 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00729  
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 211 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00527  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00708  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01905  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00862  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With user-specified 146 dimensions, biasbound (norm=1) of  0.00553  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
nsim: 7 MEANFIRST 
WithWith  66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00708  
236 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00637  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00694  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 191 
With 216 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00575  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00884  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00889  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00984  
Re-running at optimal choice of numdims, 121 
nsim: 1 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00694  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00742  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
nsim: 2 CONV 
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01671  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00848  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00755  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 221 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00636  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00722  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00846  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00917  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00919  
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00929  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00931  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01031  
Re-running at optimal choice of numdims, 121 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00796  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
nsim: 1 CONSTR 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 226 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00647  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 196 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00691  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00737  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0074  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00837  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
Without balancing, biasbound (norm=1) is 0.02736 and the L1 discrepancy is 0.027 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00858  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00897  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00908  
Re-running at optimal choice of numdims, 1 
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
With user-specified 191 dimensions, biasbound (norm=1) of  0.00499  
=====================  SIM: 8 ===================== 
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
0.543 % cces original strata missing from sample,   and 6078 / 44932 units
0.764 % cces reduc strata missing from sample,   and 20517 / 44932 units
0.927 % cces all strata missing from sample,  and 34516 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_all)`
nsim: 8 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: nsim: 2 MEANFIRST 
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00872  
5.543 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
nsim: 4 CONV 
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0092  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.007  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00736  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00947  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02632  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01903  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0142  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01253  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00911  
Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00722  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
Used 31 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00682  
nsim: 6 CONSTR 
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00723  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00666  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00647  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00641  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00654  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01728  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00656  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00765  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With user-specified 196 dimensions, biasbound (norm=1) of  0.00515  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00759  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00751  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00738  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
nsim: 4 MEANFIRST 
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00635  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00738  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0074  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00613  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
Re-running at optimal choice of numdims, 81 
Selected 31 dimensions of "allx" to use as mean balance constraints. 
nsim: 6 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00831  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00763  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00611  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00643  
Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01482  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00912  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00783  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00852  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00754  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00668  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00666  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00681  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00647  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00923  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
Re-running at optimal choice of numdims, 91 
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00693  
nsim: 6 CONSTR 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00854  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00896  
With 141 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00708  
Re-running at optimal choice of numdims, 11 
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
Used 32 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
nsim: 5 CONSTR 
With 146 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00729  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00635  
Without balancing, biasbound (norm=1) is 0.02627 and the L1 discrepancy is 0.025 
WithWith  131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00956  
136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00751  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00834  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00818  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
nsim: 7 CONSTR 
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00828  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00824  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0085  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00854  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00861  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00867  
With 151 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00853  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 126 
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00881  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00895  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00922  
With 141 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00855  
Re-running at optimal choice of numdims, 121 
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00985  
Re-running at optimal choice of numdims, 1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01978  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01794  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With With16 dimensions of K, ebalance convergence is  TRUE 91 yielding biasbound (norm=1) of 0.0081  
dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00694  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00654  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00646  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00651  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.007  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00656  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00662  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00652  
nsim: 8 CONV 
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00729  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
Used 31 dimensions of "allx" for mean balancing, and an additional 121 dimensions of "K" from kernel balancing.
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00656  
101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00736  
101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0064  
nsim: 3 CONSTR 
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
WithWith  196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00833  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00851  
Re-running at optimal choice of numdims, 31 
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00677  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00677  
nsim: 5 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00695  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.007  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 246 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 251 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 256 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00856  
Re-running at optimal choice of numdims, 71 
nsim: 7 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02044  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01029  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0096  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0074  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00694  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00665  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00666  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00653  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00651  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01813  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00649  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00641  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00648  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00635  
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00682  
With user-specified 111 dimensions, biasbound (norm=1) of  0.00656  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01642  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00852  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00695  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00627  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
nsim: 8 MEANFIRST 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00715  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00694  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0064  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00651  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00721  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00653  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00653  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00661  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00815  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00661  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
Re-running at optimal choice of numdims, 31 
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00646  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00716  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00646  
nsim: 5 CONSTR 
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00721  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00649  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00654  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00651  
With 246 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 251 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 256 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00627  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00677  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00677  
With 261 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
Re-running at optimal choice of numdims, 151 
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
nsim: 3 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00738  
With 246 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 251 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
Without balancing, biasbound (norm=1) is 0.02952 and the L1 discrepancy is 0.033 
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00708  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00708  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00721  
With 256 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02692  
Re-running at optimal choice of numdims, 151 
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00738  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00737  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00659  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
nsim: 7 CONSTR 
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01934  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00934  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00865  
Re-running at optimal choice of numdims,With  11 
21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00755  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00722  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00691  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00656  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00654  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00661  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00653  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00648  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00647  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0064  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
Without balancing, biasbound (norm=1) is 0.02692 and the L1 discrepancy is 0.028 
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00628  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
WithWith  16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00619  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00646  
WithWith  21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 21 0.00709dimensions of K, ebalance convergence is  
 TRUE yielding biasbound (norm=1) of 0.00776  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00737  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0074  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 131 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00689  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00715  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00765  
With 246 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 251 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 256 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
Re-running at optimal choice of numdims, 151 
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00748  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
nsim: 3 CONSTR 
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00806  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00878  
Re-running at optimal choice of numdims, 1 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00809  
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00705  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 151 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00605  
Without balancing, biasbound (norm=1) is 0.02748 and the L1 discrepancy is 0.03 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00739  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00751  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00751  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 141 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00809  
Re-running at optimal choice of numdims, 1 
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
nsim: 2 CONSTR 
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 156 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00641  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00695  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00716  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0072  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00726  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 76 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00743  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 81 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00757  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00857  
Re-running at optimal choice of numdims, 121 
With 161 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00758  
Re-running at optimal choice of numdims, 141 
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00855  
=====================  SIM: 10 ===================== 
0.569 % cces original strata missing from sample,   and 6761 / 44932 units
0.784 % cces reduc strata missing from sample,   and 22064 / 44932 units
0.932 % cces all strata missing from sample,  and 34672 / 44932 units
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_all)`
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01589  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
nsim: 10 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00903  
Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00617  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00609  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00629  
5.526 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00625  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00619  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00609  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00617  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00616  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00619  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00629  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00628  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00617  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00599  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00898  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00595  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00591  
Used 31 dimensions of "allx" for mean balancing, and an additional 141 dimensions of "K" from kernel balancing.
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00587  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00594  
nsim: 4 CONSTR 
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00586  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0061  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00604  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00618  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00612  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00607  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00917  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00595  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00597  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00608  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00618  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00615  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0095  
Re-running at optimal choice of numdims, 1 
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00628  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
Used 30 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
nsim: 8 CONSTR 
With 246 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 251 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00715  
With 256 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
Re-running at optimal choice of numdims, 136 
nsim: 2 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02099  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01087  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00681  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00653  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00623  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00623  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00601  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00618  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00597  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00599  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00597  
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02796  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00599  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01686  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01334  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00599  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0059  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01207  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00582  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00914  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00578  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00579  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00573  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00721  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00572  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00578  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00576  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00586  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00581  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01742  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01145  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00588  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00599  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00732  
WithWith  56 dimensions of K, ebalance convergence is 181 TRUE dimensions of K, ebalance convergence isyielding biasbound (norm=1) of  TRUE 0.00667 yielding biasbound (norm=1) of  
0.00597  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00729  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00691  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01443  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00605  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00618  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00604  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0061  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00652  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00687  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
WithWith  116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00625  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00628  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0064  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00724  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00648  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00724  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00697  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00612  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00608  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00605  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.006  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00608  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00679  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.006  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0069  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00691  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
Re-running at optimal choice of numdims, 141 
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00637  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0072  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00629  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00627  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00721  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
nsim: 4 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00655  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00691  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00679  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
Re-running at optimal choice of numdims, 151 
With 241 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
Re-running at optimal choice of numdims, 136 
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
nsim: 8 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

nsim: 2 CONSTR 
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.007  
Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02007  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00724  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00669  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00631  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00614  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00605  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00601  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00601  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00604  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00598  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00606  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00601  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00594  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00588  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00586  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0058  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00578  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00588  
Without balancing, biasbound (norm=1) is 0.02582 and the L1 discrepancy is 0.025 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01494  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00587  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00968  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00956  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0062  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00584  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00693  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00647  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00594  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00641  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00654  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00729  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00605  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00688  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00684  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00602  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00671  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00604  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00672  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00686  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00613  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00699  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00703  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0061  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00696  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00689  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00629  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 136 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00673  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00677  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00716  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00675  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0068  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00663  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00705  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00676  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00712  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00678  
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00716  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
WithWith  156156  dimensions of K, ebalance convergence is TRUE dimensions of K, ebalance convergence is yielding biasbound (norm=1) of TRUE 0.00709  
yielding biasbound (norm=1) of 0.00689  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00692  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00714  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00731  
With 236 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
Re-running at optimal choice of numdims, 141 
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00701  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00719  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 141 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00673  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00737  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
nsim: 4 CONSTR 
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00734  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
Re-running at optimal choice of numdims, 1 
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 221 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
With 226 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 146 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00836  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 71 
With 231 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00871  
Re-running at optimal choice of numdims, 111 
=====================  SIM: 9 ===================== 
0.544 % cces original strata missing from sample,   and 6282 / 44932 units
0.769 % cces reduc strata missing from sample,   and 20604 / 44932 units
0.926 % cces all strata missing from sample,  and 33911 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata_reduc)`
Joining with `by = join_by(strata_all)`
nsim: 9 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 8 CONSTR 
Searching for b value which maximizes the variance in K: nsim: 10 CONV 
5.552 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.02798 and the L1 discrepancy is 0.031 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00706  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00698  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00702  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00683  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00718  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00725  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00682  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00667  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00665  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00647  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00644  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00633  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00641  
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00657  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00639  
Without balancing, biasbound (norm=1) is 0.02563 and the L1 discrepancy is 0.025 
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0065  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00643  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00638  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With user-specified 71 dimensions, biasbound (norm=1) of  0.00648  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00797  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00798  
nsim: 10 MEANFIRST 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00809  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00826  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00626  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00618  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00622  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00636  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00765  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0066  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00662  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0067  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00682  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00673  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00685  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
Selected 30 dimensions of "allx" to use as mean balance constraints. 
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00715  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00807  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
Re-running at optimal choice of numdims, 141 
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00855  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00886  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0087  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00942  
Re-running at optimal choice of numdims, 1 
Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02902  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01733  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01252  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01186  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00713  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0071  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00674  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00649  
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00658  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00645  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00809  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00649  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00814  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00642  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00634  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00632  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00624  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00619  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00615  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 86 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00856  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00625  
With 91 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00826  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00621  
With 96 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00822  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0063  
With 101 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00843  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00648  
With 106 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00885  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00661  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00896  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00913  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00659  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01053  
Re-running at optimal choice of numdims, 6 
Used 30 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 10 CONSTR 
With 161 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00649  
With 166 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00672  
With 171 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00697  
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 176 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00796  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 121 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01813  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01012  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0093  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0086  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00717  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00708  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00711  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00707  
nsim: 9 CONV 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00704  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00709  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00752  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00763  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00781  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00818  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00832  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00789  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00801  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00805  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0081  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00877  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
Re-running at optimal choice of numdims, 81 
nsim: 10 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With user-specified 121 dimensions, biasbound (norm=1) of  0.00615  
nsim: 9 MEANFIRST 
Selected 32 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0163  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0091  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00762  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0072  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00736  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00735  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00742  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00742  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00772  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00775  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00774  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00804  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0083  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00836  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00837  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00803  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00812  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0082  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0084  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00813  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00822  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0088  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0089  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00904  
Re-running at optimal choice of numdims, 56 
nsim: 10 CONSTR 
Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00727  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00755  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00755  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
Without balancing, biasbound (norm=1) is 0.02727 and the L1 discrepancy is 0.028 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00809  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00814  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00829  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00821  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00816  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00823  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00845  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00839  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00841  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00875  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00859  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00869  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00888  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00883  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00891  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00918  
Re-running at optimal choice of numdims, 6 
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 111 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00748  
With 116 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00756  
With 121 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00865  
With 126 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00949  
Re-running at optimal choice of numdims, 6 
Used 32 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
nsim: 9 CONSTR 
Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01904  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01066  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00906  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00799  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00765  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00746  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0075  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00715  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00754  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00741  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0074  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00761  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00802  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00793  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00796  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00827  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00868  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00901  
Re-running at optimal choice of numdims, 66 
nsim: 9 CONSTR 
Warning in kbal(K.svd = svdK, allx = kbal_data, cat_data = TRUE, sampled = kbal_data_sampled,  :
  "cat_data" TRUE only used in the construction of the kernel matrix "K" and is not used when "K" or "K.svd" is already user-supplied.

Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01676  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00811  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00757  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00749  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00758  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0076  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00742  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00743  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00742  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00745  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00747  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00733  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0073  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00738  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00744  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00748  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00753  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00768  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00776  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0079  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00794  
With 186 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00795  
With 191 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00792  
With 196 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00831  
With 201 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00849  
With 206 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00866  
With 211 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00893  
With 216 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
Re-running at optimal choice of numdims, 121 
nsim: 9 CONSTR 
Without balancing, biasbound (norm=1) is 0.02884 and the L1 discrepancy is 0.032 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00759  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00728  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00756  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00766  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00786  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00783  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0078  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00788  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 61 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 66 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00769  
With 71 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00777  
With 76 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 81 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00791  
With 86 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.008  
With 91 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00787  
With 96 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 101 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 106 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00785  
With 111 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00779  
With 116 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
With 121 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0077  
With 126 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00764  
With 131 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00778  
With 136 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00771  
With 141 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00759  
With 146 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
With 151 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00814  
With 156 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00818  
With 161 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00842  
With 166 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00853  
With 171 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 176 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00909  
With 181 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00916  
Re-running at optimal choice of numdims, 6 
    user   system  elapsed 
6083.946 1765.222 5251.024 
> 
> outcome = cces$outcome
> good = which(lapply(sims, function (x) return(class(x))) == "list")
> length(good)
[1] 10
> 
> #sims[-good]
> if(SAVE) {
+     save(sims, outcome, tolerance, maxit, increment, min_num_dims, noise, R2_outcome, eval_kpop,
+          coefs, coefs_outcome, selection_model, p_include, pS_denom, manual_lambda, lambda_min,
+          file = paste0("./res_kpop", eval_kpop, "lambdamin", lambda_min, "man", manual_lambda,
+                        "_noise", noise, "_on",
+                        Sys.Date(),
+                        "_nsims", length(good),
+                        ".RData"))
+ }
> sims = sims[good]
> 
> #combines all weights across rows but can group by b to get them per iteration
> if(eval_kpop) { 
+     weights <- lapply(sims, `[[`, 3) %>% bind_rows() 
+     residuals <- lapply(sims, `[[`, 4) %>% bind_rows()
+     
+     margins <- lapply(sims, `[[`, 8) 
+ } else { 
+     weights = NULL
+     if(!coverage_eval) {margins = lapply(sims, `[[`, 6) }
+ }
> 
> ##################### eval coverage ####################
> 
> 
> coverage <- function(SE, x_bar, truth =NULL, crit_val= qnorm(0.975)) {
+     if(is.null(truth)) {
+         truth = svymean(~outcome, cces_svy)[1]
+     }
+     x_upper = x_bar +  (SE*crit_val)
+     x_lower = x_bar - (SE*crit_val)
+     contains_truth = matrix(NA, ncol = ncol(SE), nrow = 1)
+     for(i in 1:ncol(x_upper)) {
+         contains_truth[,i] = sum((truth <= x_upper[,i] & truth >= x_lower[,i]))/nrow(SE)
+     }
+     colnames(contains_truth) = colnames(x_bar)
+     return(contains_truth)
+ }
> 
> # eval coverage of diff SEs
> all_SE_coverage <- function(sims, drop_NA = F, truth = NULL, methods = c("rake|kpop")) {
+     est <- lapply(sims, `[[`, 1) %>% bind_rows()
+     SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
+     est_c = est[grepl(methods, colnames(est))]
+     SEs = SEs[grepl(methods, colnames(SEs))]
+     # a bit of a pain to drop NAs colwise and get coverage rather than
+     # dropping all NA rows and then getting coverage 
+     #(unfairly drops rows in for methods that don't have NAs) 
+     if(drop_NA) {
+         n_drop = NULL
+         coverage_out = NULL
+         for(i in 1:ncol(est_c)) {
+             # drop = which(is.na(est_c[,i]))
+             # est_temp = est_c[-drop, ]
+             est_temp = na.omit(est_c[,i])
+             n_drop = c(n_drop, nrow(est) - length(est_temp))
+             if(i ==1) {
+                 names(n_drop) = colnames(est_c)[i]
+             } else {
+                 names(n_drop)[i] = colnames(est_c)[i]
+             }
+             
+             
+             SEs_temp = na.omit(SEs[grepl(colnames(est_c)[i], colnames(SEs))])
+             
+             SE_fixed = SEs_temp[grepl("SE_fixed$", colnames(SEs_temp))]
+             SE_linear = SEs_temp[grepl("SE_linear$", colnames(SEs_temp))]
+             SE_quasi = SEs_temp[grepl("SE_quasi$", colnames(SEs_temp))]
+             SE_chad= SEs_temp[grepl("SE_chad$", colnames(SEs_temp))]
+             # SE_svy= SEs_temp[grepl("SVY", colnames(SEs_temp))]
+             # search = gsub("_se_SVY","", colnames(SE_svy))
+             
+             coverage_out = cbind(coverage_out, rbind(coverage(SE_fixed, est_temp, truth = truth),
+                                      coverage(SE_linear, est_temp, truth = truth),
+                                      coverage(SE_quasi, est_temp,truth = truth),
+                                      coverage(SE_chad,est_temp,truth = truth)))
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
+             colnames(coverage_out)[i] = colnames(est_c)[i]
+         }
+     } else {
+         est_c = est[grepl(methods, colnames(est))]
+         SEs = SEs[grepl(methods, colnames(SEs))]
+         SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))]
+         SE_linear = SEs[grepl("SE_linear$", colnames(SEs))]
+         SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]
+         SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]
+         
+         SE_svy= SEs[grepl("SVY", colnames(SEs))]
+         if(ncol(SE_svy) != 0){
+             #just making sure we're getting the estimates for the same SEs that we output from svy obj which currently is demos_noedu
+             search = gsub("_se_SVY","", colnames(SE_svy))
+             grepl(search, colnames(est_c))
+             s = coverage(SE_svy, est_c[,grepl(search, colnames(est_c))])
+             s1 = rep(NA, ncol(SE_fixed))
+             s1[grepl(search, colnames(est_c))] = s
+             #colnames(s1) = colnames(SE_fixed)
+             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
+                                  coverage(SE_linear, est_c, truth = truth),
+                                  coverage(SE_quasi, est_c,truth = truth),
+                                  coverage(SE_chad,est_c,truth = truth), 
+                                  s1)
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_svy")
+         } else {
+             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
+                                  coverage(SE_linear, est_c, truth = truth),
+                                  coverage(SE_quasi, est_c,truth = truth),
+                                  coverage(SE_chad,est_c,truth = truth))
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
+         }
+         
+     }
+     
+     if(drop_NA) {
+         out = list()
+         out$n_drop = n_drop
+         out$coverage = coverage_out
+     } else {
+         out = coverage_out
+     }
+     return(out)
+ }
> 
> 
> #res: 
> good = which(lapply(sims, function (x) return(class(x))) == "list")
> length(good)
[1] 10
> 
> 
> ######## Bias
> est <- lapply(sims, `[[`, 1) %>% bind_rows()
> est = est[grepl(c("rake|kpop|post|unweighted|h"), colnames(est))]
> bias = colMeans(est, na.rm = T)
> bias = bias - mean(cces$outcome)
> bias = data.frame(bias = t(t(bias))*100)
> bias = bias %>% arrange(desc(abs(bias)))
> round(bias,3)
                      bias
unweighted          -3.582
post_strat_all      -3.206
rake_demos_noeduc   -1.675
rake_demos_weduc    -1.359
post_strat_reduc    -1.358
ht_truth            -0.651
post_stratification -0.643
kpop                -0.195
kpop_conv           -0.119
kpop_demos_wedu     -0.073
rake_truth           0.043
hayek_truth         -0.034
kpop_mf              0.024
kpop_all             0.022
rake_all            -0.009
kpop_demos           0.006
> # kable(round(bias, 3), format = "latex", booktabs = T, 
> #       caption = paste0("Bias \\textbf{in Percent} across ", length(good), " sims: All Methods (Target = ", round(mean(outcome),3)*100, ")"))
> 
> 
> ########## SEs
> SE_coverage = all_SE_coverage(sims, truth = mean(outcome), drop_NA = T)
> SE_coverage
$n_drop
rake_demos_noeduc  rake_demos_weduc          rake_all        rake_truth 
                0                 0                 0                 0 
             kpop           kpop_mf         kpop_conv        kpop_demos 
                0                 0                 0                 0 
  kpop_demos_wedu          kpop_all 
                0                 0 

$coverage
          rake_demos_noeduc rake_demos_weduc rake_all rake_truth kpop kpop_mf
SE_fixed                  0                0      1.0        1.0  0.9     0.9
SE_linear                 0                0      0.9        0.9  1.0     1.0
SE_quasi                  0                0      0.9        0.9  1.0     1.0
SE_chad                   0                0      0.9        0.9  1.0     1.0
          kpop_conv kpop_demos kpop_demos_wedu kpop_all              
SE_fixed        0.9        0.9             0.9      0.9 1 1 1 1 0.8 1
SE_linear       0.9        1.0             1.0      0.9 1 1 1 1 0.8 1
SE_quasi        0.9        1.0             1.0      0.9 1 1 1 1 0.8 1
SE_chad         0.9        1.0             1.0      0.9 1 1 1 1 0.8 1

> 
> SEs = lapply(sims, `[[`, 2) %>% bind_rows()
> est <- lapply(sims, `[[`, 1) %>% bind_rows()
> cols = if(eval_kpop) { c(3:7,10:12,14:20)} else {c(3:7,10:12)}
> est =est[, cols]
> 
> empirical_SEs <- function(sims, eval_kpop = T, na_rm = F) {
+     SEs = lapply(sims, `[[`, 2) %>% bind_rows()
+     est <- lapply(sims, `[[`, 1) %>% bind_rows()
+     cols = if(eval_kpop) { c(3:7,10:12,14:20)} else {c(3:7,10:12)}
+     est =est[, cols]
+     
+     #avg SEs
+     avg_SE = colMeans(SEs, na.rm = na_rm)
+     avg_SE_out = rbind(avg_SE[grepl("SE_fixed$", names(avg_SE))],
+                    avg_SE[grepl("SE_linear$", names(avg_SE))],
+                    avg_SE[grepl("SE_quasi$", names(avg_SE))],
+                    avg_SE[grepl("SE_chad", names(avg_SE))],
+                    avg_SE[grepl("SVY", names(avg_SE))])
+     rownames(avg_SE_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_SVY") 
+     colnames(avg_SE_out) = gsub("_SE_fixed", "", colnames(avg_SE_out))
+     avg_SE_out
+     
+     
+     #avg_SE_out = cbind(unweighted = NA, avg_SE_out)
+     
+     #bootstrapped SEs
+     boot_SE = t(as.matrix(apply(est, 2, sd)))
+     SE_boot = boot_SE[, colnames(boot_SE) %in% colnames(avg_SE_out)]
+     emp_SEs = rbind(avg_SE_out, SE_boot)
+     
+     return(list(emp_SEs =emp_SEs, 
+                 boot_SE = boot_SE,
+                 avg_SE = avg_SE_out) )    
+ }
> 
> emp_SE = empirical_SEs(sims = sims, eval_kpop = eval_kpop, na_rm = T)
> emp_SE
$emp_SEs
          rake_demos_noeduc rake_demos_weduc    rake_all  rake_truth
SE_fixed        0.003791524      0.003800702 0.003701691 0.003707043
SE_linear       0.003655306      0.003666121 0.003215293 0.003151817
SE_quasi        0.003618361      0.003630398 0.003186218 0.003125164
SE_chad         0.003653521      0.003664336 0.003213786 0.003152322
SE_SVY          0.003655306      0.003655306 0.003655306 0.003655306
SE_boot         0.002832996      0.003042919 0.003934981 0.003420779
                 kpop   kpop_conv     kpop_mf  kpop_demos kpop_demos_wedu
SE_fixed  0.003854999 0.003852034 0.003755061 0.003846430     0.003861582
SE_linear 0.003793462 0.003788478 0.003454616 0.003868856     0.003992004
SE_quasi  0.003786534 0.003780604 0.003427228 0.003850649     0.003972978
SE_chad   0.003809767 0.003803561 0.003453059 0.003873802     0.003995357
SE_SVY    0.003655306 0.003655306 0.003655306 0.003655306     0.003655306
SE_boot   0.004119782 0.003995182 0.003635107 0.003513445     0.004757972
             kpop_all
SE_fixed  0.003753751
SE_linear 0.003452663
SE_quasi  0.003425273
SE_chad   0.003451107
SE_SVY    0.003655306
SE_boot   0.004014002

$boot_SE
      unweighted rake_demos_noeduc rake_demos_weduc    rake_all
[1,] 0.003208482       0.002832996      0.003042919 0.003934981
     post_stratification  rake_truth   ht_truth hayek_truth        kpop
[1,]         0.005084994 0.003420779 0.02401681 0.007616639 0.004119782
         kpop_mf   kpop_conv  kpop_demos kpop_demos_wedu    kpop_all
[1,] 0.003995182 0.003635107 0.003513445     0.004757972 0.004014002
               bb
[1,] 0.0005929119

$avg_SE
          rake_demos_noeduc rake_demos_weduc    rake_all  rake_truth
SE_fixed        0.003791524      0.003800702 0.003701691 0.003707043
SE_linear       0.003655306      0.003666121 0.003215293 0.003151817
SE_quasi        0.003618361      0.003630398 0.003186218 0.003125164
SE_chad         0.003653521      0.003664336 0.003213786 0.003152322
SE_SVY          0.003655306      0.003655306 0.003655306 0.003655306
                 kpop   kpop_conv     kpop_mf  kpop_demos kpop_demos_wedu
SE_fixed  0.003854999 0.003852034 0.003755061 0.003846430     0.003861582
SE_linear 0.003793462 0.003788478 0.003454616 0.003868856     0.003992004
SE_quasi  0.003786534 0.003780604 0.003427228 0.003850649     0.003972978
SE_chad   0.003809767 0.003803561 0.003453059 0.003873802     0.003995357
SE_SVY    0.003655306 0.003655306 0.003655306 0.003655306     0.003655306
             kpop_all
SE_fixed  0.003753751
SE_linear 0.003452663
SE_quasi  0.003425273
SE_chad   0.003451107
SE_SVY    0.003655306

> 
> ########## Dropped Cells
> if(eval_kpop) {
+     ps_dropped <- lapply(sims, `[[`, 5) %>% bind_rows()
+     samp_check <- lapply(sims, `[[`, 6) %>% bind_rows()
+     counts <- lapply(sims, `[[`, 7) %>% bind_rows()
+ } else {
+     #may nto be accruate after adding in residuals and count, mess w the index to find the correct one
+     ps_dropped <- lapply(sims, `[[`, 3)
+     samp_check <- lapply(sims, `[[`, 4)
+ }
> colMeans(ps_dropped)
      dropped_cells dropped_cells_reduc   dropped_cells_all 
             6177.7             20917.0             34313.2 
> 
> 
> ############# check sample
> 
> #samp_check
> sum(samp_check$bad_sample)
[1] 0
> sum(samp_check$check.fail)
[1] 0
> #on average sample has 8/24 cats important in the selection model that have <5% units in it
> mean(samp_check$check.leq_5pp)
[1] 3
> #on average sample has 3/24 cats important in the selection model that have <1% units in it
> mean(samp_check$check.leq_1pp)
[1] 0
> counts %>% filter(leq_5pp ==1) %>% group_by(var) %>% summarise(n = n(),
+                                                                avg_prop = mean(prop))
# A tibble: 6 × 3
  var              n avg_prop
  <fct>        <int>    <dbl>
1 Dem 18 to 35     8   0.0415
2 Dem 36 to 50     5   0.0446
3 Dem 65+          6   0.0446
4 Hispanic         4   0.0468
5 Rep 18 to 35     6   0.0436
6 Yes 18 to 35     1   0.0455
> ########## RES ################
> est <- lapply(sims, `[[`, 1) %>% bind_rows()
> plot = est
> 
> 
> ################## results plots
> #### ported over from tex
> if(eval_kpop) {
+     plot_lasso_margin <- plot %>% 
+         dplyr::select(unweighted, 
+                       rake_demos_noeduc,
+                       rake_demos_weduc,
+                       rake_all,
+                       post_stratification,
+                       #post_strat_reduc,
+                       #post_strat_all,
+                       rake_truth,
+                       kpop, 
+                      #kpop_reduc, 
+                       #kpop_conv,
+                       #kpop_mf, 
+                       kpop_demos,
+                       kpop_demos_wedu,
+                       kpop_all, 
+                       ht_truth,
+                       hayek_truth) %>% 
+         pivot_longer(everything(),
+                      names_to = "estimator", 
+                      values_to = "margin") %>%
+         mutate(margin = margin * 100,
+                estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
+                                                  #estimator == "kpop_reduc" ~ "kpop\n REDUC",
+                                                  estimator == "kpop_mf" ~ "kpop aMF (All)",
+                                                  # estimator == "kpop_conv" ~ "kpop Converged",
+                                                  estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
+                                                  estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
+                                                  estimator == "kpop_all" ~ "kpop+MF:\n (All)",
+                                                  estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
+                                                  estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
+                                                  estimator == "rake_all" ~ "Mean Calibration:\n (All)",
+                                                  estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
+                                                  estimator == "post_stratification" ~ "Post-Strat Truth",
+                                                  estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
+                                                  estimator == "post_strat_all" ~ "Post-Strat All",
+                                                  estimator == "unweighted" ~ "Unweighted",
+                                                  estimator == "ht_truth" ~ "Horvitz-Thompson",
+                                                  estimator == "hayek_truth" ~ "Hayek"),
+                                        levels = c("Unweighted", 
+                                                   "Mean Calibration:\n (Demos)",
+                                                   "Mean Calibration:\n (Demos+Edu)",
+                                                   "Mean Calibration:\n (All)",
+                                                   "Post-Strat Truth", 
+                                                   #"Post-Stratification:\n (Reduc)", 
+                                                   #"Post-Strat All",
+                                                   "kpop",
+                                                   "kpop\n REDUC",
+                                                   # "kpop Converged",
+                                                   #"kpop aMF (All)",
+                                                   "kpop+MF:\n (Demos)",
+                                                   "kpop+MF:\n (Demos+Edu)",
+                                                   "kpop+MF:\n (All)",
+                                                   "Mean Calibration:\n True Selection\nModel",
+                                                   "Horvitz-Thompson",
+                                                   "Hayek"
+                                        ) ) )
+     
+ } else {
+     plot_lasso_margin <- plot %>% 
+         dplyr::select(unweighted, 
+                       rake_demos_noeduc,
+                       rake_demos_weduc,
+                       rake_all,
+                       post_stratification,
+                       post_strat_reduc,
+                       post_strat_all,
+                       rake_truth, 
+                       ht_truth, 
+                       hayek_truth) %>% 
+         pivot_longer(everything(),
+                      names_to = "estimator", 
+                      values_to = "margin") %>%
+         mutate(margin = margin * 100,
+                estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
+                                                  estimator == "kpop_mf" ~ "kpop aMF (All)",
+                                                  # estimator == "kpop_conv" ~ "kpop Converged",
+                                                  estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
+                                                  estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
+                                                  estimator == "kpop_all" ~ "kpop+MF:\n (All)",
+                                                  estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
+                                                  estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
+                                                  estimator == "rake_all" ~ "Mean Calibration:\n (All)",
+                                                  estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
+                                                  estimator == "post_stratification" ~ "Post-Strat",
+                                                  estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
+                                                  estimator == "post_strat_all" ~ "Post-Strat All",
+                                                  estimator == "unweighted" ~ "Unweighted",
+                                                  
+                                                  estimator == "ht_truth" ~ "Horvitz-Thompson",
+                                                  estimator == "hayek_truth" ~ "Hayek"),
+                                        levels = c("Unweighted", 
+                                                   "Mean Calibration:\n (Demos)",
+                                                   "Mean Calibration:\n (Demos+Edu)",
+                                                   "Mean Calibration:\n (All)",
+                                                   "Post-Strat", 
+                                                   "Post-Stratification:\n (Reduc)", 
+                                                   "Post-Strat All",
+                                                   "kpop",
+                                                   #"kpop Converged",
+                                                   #"kpop aMF (All)",
+                                                   "kpop+MF:\n (Demos)",
+                                                   "kpop+MF:\n (Demos+Edu)",
+                                                   "kpop+MF:\n (All)",
+                                                   "Mean Calibration:\n True Selection\nModel",
+                                                   "Horvitz-Thompson",
+                                                   "Hayek"
+                                        )))
+     
+ }
> 
> #target:
> #margin_sim = svymean(~outcome, cces_svy)[1]* 100
> #### Box Plot
> #options(dplyr.print_max = 1e9)
> gg_out = ggplot(data = plot_lasso_margin,
+                 aes(x = estimator_name, y = margin)) +
+     geom_boxplot(alpha = 0.2) +
+     geom_hline(yintercept = margin_sim) +
+     theme_bw() +
+     xlab("") +
+     ylab("Modeled Vote Margin") +
+     annotate(geom = "text", x = 0.85, y = margin_sim+0.25, size = 2.7, angle = 90,
+              label = "True Target\nPopulation\nMargin", hjust = 0) +
+     ggtitle(paste0(nrow(est)," sims w/avg n_samp =", round(mean(est$n)), " simple DGP ", simple_selection_model)) +
+     theme(panel.grid.major.x = element_blank(),
+           axis.text.x = element_text(angle = 45, hjust = 1))
> 
> gg_out
> ### table
> table = plot_lasso_margin %>% 
+     mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
+     group_by(estimator_name) %>%
+     summarize(
+         Bias = mean(margin - margin_sim),
+         SE_boot= sd(margin),
+         MSE = mean((margin - margin_sim)^2)
+     ) %>%
+     mutate(
+         Bias_Reduc = 1- Bias / Bias[estimator_name == "Unweighted"]
+     ) %>%
+     arrange(MSE)
> table
# A tibble: 12 × 5
   estimator_name                              Bias SE_boot    MSE Bias_Reduc
   <chr>                                      <dbl>   <dbl>  <dbl>      <dbl>
 1 Mean Calibration:  True Selection Model  0.0427    0.342  0.107      1.01 
 2 kpop+MF:  (Demos)                        0.00649   0.351  0.111      1.00 
 3 Mean Calibration:  (All)                -0.00861   0.393  0.139      0.998
 4 kpop+MF:  (All)                          0.0221    0.401  0.145      1.01 
 5 kpop                                    -0.195     0.412  0.191      0.946
 6 kpop+MF:  (Demos+Edu)                   -0.0732    0.476  0.209      0.980
 7 Hayek                                   -0.0336    0.762  0.523      0.991
 8 Post-Strat Truth                        -0.643     0.508  0.646      0.820
 9 Mean Calibration:  (Demos+Edu)          -1.36      0.304  1.93       0.621
10 Mean Calibration:  (Demos)              -1.67      0.283  2.88       0.532
11 Horvitz-Thompson                        -0.651     2.40   5.62       0.818
12 Unweighted                              -3.58      0.321 12.9        0    
> 
> if(SAVE) {
+     save(sims, outcome, SE_coverage, bias, table, plot_lasso_margin, noise,eval_kpop,emp_SE,
+          tolerance, maxit, increment, min_num_dims,
+          coefs, coefs_outcome, selection_model, p_include, pS_denom, manual_lambda, lambda_min,
+          file = paste0("./res_kpop", eval_kpop, "lambdamin", lambda_min, "man", manual_lambda, 
+                        "_noise", noise, "_on",
+                        Sys.Date(), 
+                        "_nsims", length(good),
+                        ".RData"))
+ }
> 
> proc.time()
    user   system  elapsed 
7738.709 2113.571 5261.408 
