---
title: "Kpop Simulation Results"
output: pdf_document
header-includes:
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage{makecell}
  - \usepackage{xcolor}
---

```{r libs, include =F, message=FALSE, warning= F}
suppressMessages(library(tidyverse))
library(parallel)
suppressMessages(library(knitr))
library(kableExtra)
library(survey)
```

```{r filenames, include = F}
#linear
r33_file_lin = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise1.414_on2022-11-16_nsims494.RData"
r66_file_lin= "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise0.707_on2022-11-16_nsims494.RData"
r66_bern_file_lin  = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise0.707_on_bernTRUE_2022-11-16_nsims494.RData"

#nonlin
r66_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise0.707106781186548_on2023-01-18_nsims481.RData"

r33_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise1.4142135623731_on2023-01-19_nsims494.RData"

r5_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise1_on2023-01-26_nsims481.RData"

```


```{r functions, include = F}
coverage <- function(SE, x_bar, truth, crit_val= qnorm(0.975)) {
    x_upper = x_bar +  (SE*crit_val)
    x_lower = x_bar - (SE*crit_val)
    contains_truth = matrix(NA, ncol = ncol(SE), nrow = 1)
    for(i in 1:ncol(x_upper)) {
        contains_truth[,i] = sum((truth <= x_upper[,i] & truth >= x_lower[,i]))/nrow(SE)
    }
    colnames(contains_truth) = colnames(x_bar)
    return(contains_truth)
}

# eval coverage of diff SEs
all_SE_coverage <- function(sims, drop_NA = F, truth = NULL, methods = c("rake|kpop")) {
    est <- lapply(sims, `[[`, 1) %>% bind_rows()
    SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
    est_c = est[grepl(methods, colnames(est))]
    SEs = SEs[grepl(methods, colnames(SEs))]
    # a bit of a pain to drop NAs colwise and get coverage rather than
    # dropping all NA rows and then getting coverage 
    #(unfairly drops rows in for methods that don't have NAs) 
    
    if(drop_NA) {
        n_drop = NULL
        coverage_out = NULL
        for(i in 1:ncol(est_c)) {
            # drop = which(is.na(est_c[,i]))
            # est_temp = est_c[-drop, ]
            est_temp = na.omit(est_c[,i])
            n_drop = c(n_drop, nrow(est) - length(est_temp))
            if(i ==1) {
                names(n_drop) = colnames(est_c)[i]
            } else {
                names(n_drop)[i] = colnames(est_c)[i]
            }
            SEs_temp = na.omit(SEs[grepl(paste0(colnames(est_c)[i],"_SE"), colnames(SEs))])
            
            SE_fixed = SEs_temp[grepl("SE_fixed$", colnames(SEs_temp))]
            SE_linear = SEs_temp[grepl("SE_linear$", colnames(SEs_temp))]
            SE_quasi = SEs_temp[grepl("SE_quasi$", colnames(SEs_temp))]
            SE_chad= SEs_temp[grepl("SE_chad$", colnames(SEs_temp))]
            # SE_svy= SEs_temp[grepl("SVY", colnames(SEs_temp))]
            # search = gsub("_se_SVY","", colnames(SE_svy))
            
            coverage_out = cbind(coverage_out, rbind(coverage(SE_fixed, est_temp, truth = truth),
                                     coverage(SE_linear, est_temp, truth = truth),
                                     coverage(SE_quasi, est_temp,truth = truth),
                                     coverage(SE_chad,est_temp,truth = truth)))
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
            colnames(coverage_out)[i] = colnames(est_c)[i]
            
        }
    } else {
        est_c = est[grepl(methods, colnames(est))]
        SEs = SEs[grepl(methods, colnames(SEs))]
        SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))]
        SE_linear = SEs[grepl("SE_linear$", colnames(SEs))]
        SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]
        SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]
        
        SE_svy= SEs[grepl("SVY", colnames(SEs))]
        if(ncol(SE_svy) != 0){
            #just making sure we're getting the estimates for the same SEs that we output from svy obj which currently is demos_noedu
            search = gsub("_se_SVY","", colnames(SE_svy))
            grepl(search, colnames(est_c))
            s = coverage(SE_svy, truth = truth, est_c[,grepl(search, colnames(est_c))])
            s1 = rep(NA, ncol(SE_fixed))
            s1[grepl(search, colnames(est_c))] = s
            #colnames(s1) = colnames(SE_fixed)
            coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
                                 coverage(SE_linear, est_c, truth = truth),
                                 coverage(SE_quasi, est_c,truth = truth),
                                 coverage(SE_chad,est_c,truth = truth), 
                                 s1)
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_svy")
        } else {
            coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
                                 coverage(SE_linear, est_c, truth = truth),
                                 coverage(SE_quasi, est_c,truth = truth),
                                 coverage(SE_chad,est_c,truth = truth))
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
        }
        
    }
    
    if(drop_NA) {
        out = list()
        out$n_drop = n_drop
        out$coverage = coverage_out
    } else {
        out = coverage_out
    }
    return(out)
}


manual_rescale <- function(dat, a =0, b= 1) {
    rescaled = (b-a)*( dat - min(dat))/(max(dat) - min(dat)) + a
    return(rescaled)
}
```



```{r outcome, include = F}

make_outcome <- function(noise, bern = F, p_include = NULL, outcome = NULL,
                         selection_coefs = NULL, outcome_coefs = NULL, 
                         selection_model = NULL, p_include_denom = 1,
                         coverage_eval = TRUE, linear_model = FALSE) {
    #have to copy paste everything from the sims bc didn't set seed right before noise added UGHUGHGU
    set.seed(9345876)
    
    if(detectCores() > 10) {
      path_data = "/home/csterbenz/Data/"
      cores_saved = 10
    } else if(detectCores() != 4) {
      path_data= "/Users/Ciara_1/Dropbox/kpop/Updated/application/data/"
      cores_saved = 6
    } else {
        path_data= "/Users/Ciara/Dropbox/kpop/Updated/application/data/"
        cores_saved = 2
    }
    
    POPW = FALSE
    bern_unused = FALSE
    coverage_eval_unused = TRUE
    linear_model_unused = FALSE # jsut to not write over the input but keep seed the same
    noise_unused = (1/2)*sqrt(2)
    TEST = FALSE # to run with a linear kernel so it's way faster; UPDATE: errors catch this as mistake and prevent
    tolerance = 1e-4
    maxit = 500
    #both for runtime
    increment = 5
    min_num_dims = NULL
    max_num_dims = NULL
    SAVE = TRUE
    ##### Central Params to adjust
    n_sample = 500
    simple_selection_model = TRUE
    nsims = (detectCores()-cores_saved)*13
    nsims
    ###################### Formulas ################
    formula_rake_demos_noeduc <- ~recode_age_bucket + recode_female + recode_race +
      recode_region + recode_pid_3way
    
    #updated to include 6 way edu
    formula_rake_demos_weduc <- ~recode_age_bucket + recode_female +
      recode_race + recode_region + recode_educ + recode_pid_3way
    
    formula_rake_all_vars <- ~recode_age_bucket + recode_female +
      recode_race + recode_region + recode_pid_3way + recode_educ +
      recode_income_5way + recode_relig_6way + recode_born + recode_attndch_4way
    
    formula_ps <- ~recode_age_3way + recode_female + recode_race +
      recode_region + recode_educ_wh_3way + recode_pid_3way
    
    formula_ps_reduc <- ~recode_age_3way + recode_female +
      recode_race + recode_region + recode_pid_3way  +
      recode_income_3way + recode_born + recode_educ_wh_3way
    
    # #let's coarsen income and religion and attend church for all:
    formula_ps_all <- ~recode_age_3way + recode_female +
      recode_race + recode_region + recode_pid_3way + recode_educ_3way +
      recode_income_3way + recode_relig_6way + recode_born + recode_attndch_bin
    
    create_targets <- function (target_design, target_formula) {
      target_mf <- model.frame(target_formula, model.frame(target_design))
      target_mm <- model.matrix(target_formula, target_mf)
      wts <- weights(target_design)
    
      return(colSums(target_mm * wts) / sum(wts))
    }
    
    manual_rescale <- function(dat, a =0, b= 1) {
        rescaled = (b-a)*( dat - min(dat))/(max(dat) - min(dat)) + a
        return(rescaled)
    }
    
    ### Post-stratification function
    ## For now assumes that strata variable is already created and in
    ## the data set and called "strata"
    postStrat <- function(survey, pop_counts, pop_w_col, strata_pass, warn = T) {
      survey_counts <- survey %>%
        group_by(!!as.symbol(strata_pass)) %>%
        summarize(n = n()) %>%
        ungroup() %>%
        mutate(w_survey = n / sum(n))
    
      pop_counts <- pop_counts %>%
        rename(w_pop = matches(pop_w_col))
      
      if(warn == T & nrow(survey_counts) !=  nrow(pop_counts)) {
          missing_strat = pop_counts[! (( pop_counts[, strata_pass]%>% pull()) %in% (survey_counts[, strata_pass]%>% pull() )), strata_pass]
          warning(paste("Strata in Pop not found in Sample. Dropping", 
                        sum(pop_counts[(pop_counts[, strata_pass] %>% pull()) %in% 
                                           (missing_strat %>% pull()),"n" ]), 
                        "empty cells\n"), immediate.  =T )
      } 
    
      post_strat <- pop_counts %>%
        left_join(survey_counts, by = strata_pass) %>%
        filter(!is.na(w_survey)) %>%
        ## Normalizes back to 1 after dropping
        ## empty cells
        mutate(w_pop = w_pop * 1/sum(w_pop),
               w = w_pop / w_survey) %>%
        dplyr::select(!!as.symbol(strata_pass), w)
    
      survey <- survey %>%
        left_join(post_strat)
    
      return(survey)
    }
    check_sample <- function(sample, selection_model) {
        check = model.matrix(selection_model, data = sample)
        check = colSums(check)
        fail = check[which(check ==0)]
        if(length(fail) ==0) {
            return(NULL)
        } else {
            return(fail)
        }
    }
    check_outcome <- function(outcome) {
        beyond_support = (min(outcome) <0 | max(outcome) > 1)
        return(beyond_support)
    }
    
    bound_outcome <- function(outcome, coefs, increment = 1, increment_intercept = .01, noise, cces_expanded, silent = T) {
        denom = 10
        fail = check_outcome(outcome)
        while(fail) {
            denom = denom + increment
            if(!silent) { cat(denom, ": ") }
            coefs_use = coefs/denom
            outcome = cces_expanded %*% coefs_use
            
            outcome = outcome + rnorm(nrow(cces_expanded), mean = 0, sd = sd(outcome)*noise)
            summary(outcome)
            if(max(outcome) <=1 & min(outcome) <0 ) {
                coefs[1] = coefs[1] + increment_intercept
                if(!silent) { cat("\nmoving intercept up", coefs[1],  "\n") }
                denom = denom - increment
            }
            if(max(outcome) >1 & min(outcome) >=0 ) {
                coefs[1] = coefs[1] - increment_intercept
                if(!silent) { cat("\nmoving intercept down", coefs[1],  "\n") }
                denom = denom - increment
            }
            fail = check_outcome(outcome)
            if(!silent) { cat(round(min(outcome),2), round(max(outcome),2),  "\n") }
        }
        if(!silent) { cat(paste("Min denom:", denom)) }
        return(list(outcome = outcome, coefs = coefs_use, denom = denom))
        
    }
    
    
    ############# Load Data #####################
    #these data have been cleaned already see app_modeled for how it was done
    ## Load Pew
    pew <- readRDS(paste0(path_data, "pew_lasso_061021.rds"))
    
    ### Load Target Data
    cces <- readRDS(paste0(path_data, "cces_lasso_061021.rds"))
    
    ################# Selection Model and Outcome Model ###########
    
    if(is.null(p_include) & is.null(outcome)) {
        if(!coverage_eval) {

            if(simple_selection_model) {
                #first attempt to make this worse
                selection_model = as.formula(~recode_pid_3way:poly(recode_age, 2) +
                                                 recode_female:recode_pid_3way)
                #first attempt to make this worse
                #center age then square
                pew = pew %>% mutate(centered_age = scale(recode_age, scale = F))
                cces = cces %>% mutate(centered_age = scale(recode_age, scale = F))
        
                selection_model = as.formula(~recode_pid_3way*poly(centered_age, 2) +
                                                 #recode_age:recode_pid_3way +
                                                 recode_female*recode_pid_3way)
        
                #selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
        
            } else {
                selection_model = as.formula(~recode_female:recode_pid_3way +
                                                 recode_age:recode_pid_3way +
                                                 #adds a bit mroe bias to edu+d
                                                 recode_pid_race +
                                                 recode_race_reg_wh_educ +
                                                 recode_educ_wh_3way +
                                                 poly(recode_age, 3))
            }
            # Stack data with S = 1 indicating Pew
            stack_data <- data.frame(bind_rows(pew, cces),
                                     S = c(rep(1, nrow(pew)), rep(0, nrow(cces))))
        
        
            mod <- model.matrix(selection_model, data = stack_data)
            nrow(mod)
            ## Remove columns where Pew missing strata
            ncol(mod)
            mod <- mod[, apply(mod[stack_data$S == 1, ], 2, sum) != 0]
            ## Remove columns where CCES missing Strata
            mod <- mod[, apply(mod[stack_data$S == 0, ], 2, sum) != 0]
            ncol(mod)
            lasso_lambda <- cv.glmnet(x= mod[,-1],
                                      y = as.matrix(stack_data$S),
                                      alpha = 1,
                                      family = "binomial",
                                      intercept = TRUE)
            lasso_lambda$lambda.min
            lasso_include <- glmnet(x= mod[,-1],
                                    y = as.matrix(stack_data$S),
                                    alpha = 1,
                                    lambda = lasso_lambda$lambda.min,
                                    weights = if(POPW){ c(rep(1, nrow(pew)),
                                                          cces$commonweight_vv_post)} else {NULL},
                                    family = "binomial",
                                    intercept = TRUE)
        
            lasso_include_coefs <- coef(lasso_include)
            res <- as.matrix(lasso_include_coefs)
            #View(res)
            ncol(mod)
            sum(lasso_include_coefs == 0)
        
            lasso_include_coefs
            lasso_pinclude = predict(lasso_include,
                                     s= lasso_lambda$lambda.min,
                                     type = "response",
                                     newx = mod[stack_data$S == 0,-1])
        
            p_include <- lasso_pinclude
            sum(p_include)
            cor(p_include, cces$outcome)
            cor(p_include, cces$mod_cces_on_cces_pD)
            cor(p_include, cces$mod_cces_on_cces_pR)
        
            ########### Sampling Inclusion Results
            summary(p_include)
            sum(p_include)
            mean(p_include)
            p_include_raw = p_include
        
        
            intercept_shift = 0
            p_include = p_include*(n_sample/sum(p_include)) + intercept_shift
            mean(p_include)
            summary(p_include)
            sum(p_include)
        
            #################### Define Outcome #########
            cces$outcome = cces$diff_cces_on_cces
        } else {
        if(linear_model) {
            ########## DESIGN SELECTION MODEL: Specifying Coefs Directly
            #coefs: pid, age, gender
            selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
            cces_expanded = model.matrix(selection_model, data = cces)
            #needs to be n x p X p x 1 -> coef matrix is p x 1
            coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            rownames(coefs) = colnames(cces_expanded)
            coefs[,1] = c(-5.2, #intercept
                          .1, #selection of indep pos
                          .4, #selection of R pos
                          .1, #36-50,
                          .3, #51-64,
                          .8, #65+,
                          .4 #male pos
            )
            
            xbeta = cces_expanded %*% coefs
            p_include = plogis(xbeta)
            sum(p_include)
            summary(p_include)
            
            #################### DESIGN OUTCOME MODEL ##################
            coefs_outcome = coefs
            
            coefs_outcome[,1] = c(6.1, #intercept
                                  -.3, #  indep pos #decreasing lowers mean, increases corr #try 1
                                  -1.1, #  R pos #empirically
                                  -.2 ,#36-50, #empirically in cces lean dem 50% #.55
                                  -.3, #51-64, #empirically lean rep slightly 50%
                                  -.7, #65+, #empirically lean rep 51%
                                  #base cat: 18-35 lean strongly dem 58%
                                  -.7 #male #empirically women lean dem 53%
            )
            coefs_outcome = coefs_outcome/10
            
            xbeta_outcome = cces_expanded %*% coefs_outcome
            
            if(!bern) {
                cat(paste("(linear) Adding sd(outcome)*",round(noise, 3), "\n"))   
                xbeta_outcome = xbeta_outcome + rnorm(nrow(cces), mean = 0, sd = sd(xbeta_outcome)*noise)
            }
            if(bern) {
                cat(paste("(linear) Adding bernoulli ### UPDATE ###",round(noise, 3), "\n"))  
                xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
            }
            cat(paste("Range of outcome w/noise is\n"))
            cat(paste(summary(xbeta_outcome), "\n"))
            if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {warning("outcome beyond prob support for some units when noise is added", immediate. = T)}
            s = summary(lm(xbeta_outcome ~ recode_pid_3way + recode_age_bucket + recode_female,data = cces))
            R2_outcome = s$adj.r.squared
            cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
            cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
            cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))
            #bernoulli draw
            coefs_outcome_pass = coefs_outcome
            cces$outcome = xbeta_outcome
            outcome = xbeta_outcome
    } else {
  
        if(is.null(selection_model)) {
            selection_model = as.formula(~recode_pid_3way + recode_female + recode_age_bucket 
                                          + recode_educ_3way 
                                          + recode_pid_3way:recode_age_bucket)
            
            cces_expanded = model.matrix(selection_model, data = cces)
            coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            rownames(coefs) = colnames(cces_expanded)
                
            #(p(S)) for negative bias select non dem voters
            coefs[,1] = c(-7.7, #intercept -5 w race
                          2, #selection of indep pos
                          3, #selection of R pos
                          .5, #male
                          .15, #36-50,
                          .1, #51-64,
                          .2, #65+,
                          .7, #college
                          -0.6 , #post-grad
                          .2,#ind x 36-50
                          1, #rep x 36-50,
                          .5, #ind x 51-64,
                          .5, #rep x 51-64,
                          -.2, #ind x 65+
                          1 #rep x 65+
            )
            #################### DESIGN OUTCOME MODEL ##################
            coefs_outcome = coefs
            #p(D)
             coefs_outcome[,1] = c(8, #intercept #5 w race
                              -3,#-.5, #selection of indep pos
                              -5,# -.8, #selection of R pos
                              -.3, #male
                              -.5, #36-50,
                              -.1, #51-64,
                              -.2, #65+,
                              .8, #college
                              .9,  #post-grad
                              -2.8,#ind x 36-50
                              -2.8, #rep x 36-50,
                              -.5, #ind x 51-64,
                              -.5, #rep x 51-64,
                              .8, #ind x 65+
                              -1 #rep x 65+
                              )
             cces = cces %>% mutate(strata = paste(recode_pid_3way,
                                      recode_female, 
                                      recode_age_bucket,
                                      recode_educ_3way,
                                      sep = "_"))
    
            pew = pew %>% mutate(strata = paste(recode_pid_3way,
                                                  recode_female, 
                                                  recode_age_bucket,
                                                  recode_educ_3way,
                                                  sep = "_"))
        } else {
            
            #### NB: does not currently make strata consistent w selection model if passed in
            
            cces_expanded = model.matrix(selection_model, data = cces)
            if(is.null(selection_coefs) | is.null(outcome_coefs) |
               nrow(selection_coefs) != ncol(cces_expanded) |
               nrow(outcome_coefs) != ncol(cces_expanded)) {
                stop("when selection model inputted must also input selection and outcome coefsand
                     rows must match levels of selection model.")
            }
            #coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            #rownames(coefs) = colnames(cces_expanded)
            coefs = selection_coefs
            coefs_outcome = outcome_coefs
        }
            
            xbeta = cces_expanded %*% coefs
            p_include = plogis(xbeta)
            p_include = p_include/p_include_denom
            sum(p_include)
        
            if(!bern) {
                cat(paste("Adding sd(outcome)*",round(noise, 3), "\n")) 
                #toggle off for earlier results
                if(!is.null(selection_model)) {set.seed(1383904)}
                bound = bound_outcome(outcome = cces_expanded %*% coefs_outcome,
                                      coefs = coefs_outcome,
                                      cces_expanded = cces_expanded,
                                      noise = noise)
                xbeta_outcome = bound$outcome
                coefs_outcome_pass = bound$coefs
                beyond_support = check_outcome(xbeta_outcome)
                cces$outcome = xbeta_outcome
            }
            if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {
                warning("Outcome beyond prob support for some units when noise is added, but should be bounded\n",
                                                                        immediate. = T)
            }
            if(bern) {
                cat(paste("Adding bernoulli noise and making outcome binary\n"))  
                xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
                cat(paste("Corr of S (one sample draw) and Y", round(cor(sample, xbeta_outcome),3)))
            }
            
            cat(paste("Range of outcome w/noise is\n"))
            cat(paste("xbeta is", is.null(xbeta_outcome), "null and", length(xbeta_outcome), "long 1\n"))
            cat(paste(summary(xbeta_outcome), "\n"))
            cat(paste("xbeta is", is.null(xbeta_outcome), "null and", length(xbeta_outcome), "long 2\n"))
            s = summary(lm(update(selection_model, outcome ~ .), data = cces))
            R2_outcome = s$adj.r.squared
            cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
            cat(paste("OK2\n"))
            cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3), "\n"))
            cat(paste("Corr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3),"\n"))
            cces$outcome = xbeta_outcome
            outcome = xbeta_outcome
            }
        
        }
    } else { 
        coefs_outcome_pass = outcome_coefs
        xbeta_outcome = outcome
        if(is.null(selection_model)) {
            stop("pass in selection model to calculate R2")
        }
        s = summary(lm(update(selection_model, outcome ~ .), data = cces))
        R2_outcome = s$adj.r.squared
        cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
    }
    
    if(coverage_eval) {
        formula_ps <- selection_model
    } else {
        formula_ps <- ~recode_age_3way + recode_pid_3way + recode_female
    }
   
    ######P-include plot
    gg_dat = data.frame(Selection_Probability = p_include,
                        Pid = cces$recode_pid_3way,
                        Outcome_pD = outcome)
    
    gg_p_include = ggplot(gg_dat) +
        geom_density(aes(x= Selection_Probability)) + 
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of Selection Probabilities")) +
        theme_bw()
    
    la = data.frame(pS = p_include, pid = cces$recode_pid_3way) %>%
        group_by(pid) %>% summarise(max = round(max(pS)*100,2 ))
    
    gg_p_include_pid = ggplot(gg_dat) +
        geom_density(aes(x= Selection_Probability, color = Pid)) + 
        annotate(geom = "label", x=quantile(p_include,.5),
                 y=Inf, vjust = 1,
                 color = "red",
                 label =  paste0("Dem Max P(S)= ", la[la$pid =="Dem", "max"], "%")) + 
        annotate(geom = "label",x=quantile(p_include,.5), 
                 y=Inf, vjust = 3,
                 label= paste0("Ind Max P(S)= ", la[la$pid =="Ind", "max"], "%" ), 
                  color = "green") +
        annotate(geom = "label",x=quantile(p_include,.5), 
                 y=Inf, vjust = 5,
                 label= paste0("Rep Max P(S)= ", la[la$pid =="Rep", "max"], "%" ),
                  color = "blue") +
        # geom_text(x=.10, y=250, label= paste0("Dem Max P(S)=", la[la$pid =="Dem", "max"] ), 
        #           color = "red") +
        #  geom_text(x=.10, y=200, label= paste0("Ind Max P(S)=", la[la$pid =="Ind", "max"] ), 
        #           color = "green") +
        #  geom_text(x=.10, y=150, label= paste0("Rep Max P(S)=", la[la$pid =="Rep", "max"] ), 
        #           color = "Blue") +
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of Seleciton Probabilities by Party")) +
        theme_bw()
    #gg_p_include_pid
    
    gg_p_include_outcome = ggplot(gg_dat) +
        geom_point(aes(x= Selection_Probability, y= Outcome_pD, color = Pid)) + 
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of p(Y=1) and p(S=1)")) +
        theme_bw()
        
    
    ############### kable_res
    selection_coefs_kable = data.frame(coefs)
    colnames(selection_coefs_kable) = "Coefficient Value"
    rownames(selection_coefs_kable) = gsub("^recode_", "", rownames(selection_coefs_kable))
    selection_coefs_kable = kable(selection_coefs_kable,
                                      format = "latex", booktabs = T,
                       caption = paste("Linear=",linear_model, "Selection Model" ))
    
    cat(paste("Sum of Selection Probs is:", round(sum(p_include), 3), "\n"))
    out = c(min(p_include), 
            quantile(p_include, c(0.25, 0.5, .75)), 
            max(p_include), 
            sum(p_include))
    out = as.matrix(out)
    rownames(out) =  c("Min", "25%", "Mean", "75%", "Max", "Sum")
    colnames(out) = c("Selection Probability")
    
    prob_kable = kable(round(out, 4), format = "latex", 
                       booktabs = T,
                       caption = paste("Linear=",linear_model,"Sample Inclusion Probabilities"))
    
    coefs_outcome_kable = data.frame(coefs_outcome_pass)
    colnames(coefs_outcome_kable) = "Coefficient Value"
    rownames(coefs_outcome_kable) = gsub("^recode_", "", rownames(coefs_outcome_kable))
    coefs_outcome_kable = kable(coefs_outcome_kable,
                                      format = "latex", booktabs = T,
                       caption = paste("Linear=",linear_model," R2=",
                      round(R2_outcome,2), "Outcome Model") )
    

    return(list(R2 = R2_outcome,
                p_include = p_include,
                plot_pinclude = gg_p_include,
                plot_pinclude_pid = gg_p_include_pid,
                plot_pinclude_outcome = gg_p_include_outcome,
                linear = linear_model,
                target = mean(outcome),
                corr = cor(xbeta_outcome, p_include), 
                selection_prob_kable = prob_kable,
                coefs_outcome_kable= coefs_outcome_kable, 
                selection_coefs_kable= selection_coefs_kable))
}

```

```{r emp_SEs, include = F}
empirical_SEs <- function(sims, eval_kpop = T, return_svy_package = F, na_rm = F) {
    SEs = lapply(sims, `[[`, 2) %>% bind_rows()
    est <- lapply(sims, `[[`, 1) %>% bind_rows()
    cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
    #cols = if(eval_kpop) { c(3:7,10:12,14:19)} else {c(3:7,10:12)}
    est =est[, cols]

    #avg SEs
    avg_SE = colMeans(SEs, na.rm = na_rm)
    avg_SE_out = rbind(avg_SE[grepl("SE_fixed$", names(avg_SE))],
                   avg_SE[grepl("SE_linear$", names(avg_SE))],
                   avg_SE[grepl("SE_quasi$", names(avg_SE))],
                   avg_SE[grepl("SE_chad", names(avg_SE))],
                   avg_SE[grepl("SVY", names(avg_SE))])
    rownames(avg_SE_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_SVY") 
    colnames(avg_SE_out) = gsub("_SE_fixed", "", colnames(avg_SE_out))
    
    #bootstrapped SEs
    boot_SE = t(as.matrix(apply(est, 2, sd)))
    SE_boot = boot_SE[, colnames(boot_SE) %in% colnames(avg_SE_out)]
    #stupid fix for non names kpop mf Ses in some sims
    good_cnames = grepl("kpop|rake|post_stratification|unweighted", colnames(avg_SE_out))
    good_cnames = colnames(avg_SE_out)[good_cnames]
    emp_SEs = rbind(avg_SE_out, SE_boot[good_cnames])
    if(rownames(emp_SEs)[nrow(emp_SEs)] == "") { rownames(emp_SEs)[nrow(emp_SEs)] = "SE_boot" }
    
    if(!return_svy_package) {
        avg_SE_out = avg_SE_out[-which(rownames(avg_SE_out) == "SE_SVY"), ]
        emp_SEs = emp_SEs[-which(rownames(emp_SEs) == "SE_SVY"), ]
    }
    
    return(list(emp_SEs =emp_SEs, 
                boot_SE = boot_SE,
                avg_SE = avg_SE_out) )    
}

```


```{r run_outcome, include = F, cache =TRUE}
r33_lin = make_outcome(noise = sqrt(2), linear_model = T)
r66_lin = make_outcome(noise = sqrt(2)*(1/2), linear_model = T)
load(r66_nonlin_file)
r66_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, selection_model, pS_denom)

load(r33_nonlin_file)
r33_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
#dumb change to update cache
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, selection_model, pS_denom)

load(r5_nonlin_file)
r5_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
rm(sims, coefs, coefs_outcome, p_include, outcome, SE_coverage, emp_SE, selection_model, pS_denom)

```




# Simulation Set Up: Linear Model

## Linear Selection Model
$p(S=1) = logit^{-1}\Big( PID(3 way) + Age(4way)+ Gender  \Big)$

More specifically:
$$p(S=1) = logit^{-1}\Big( \alpha + \beta_1 Indep + \beta_2 Rep + \beta_3 Age_{36-50} + \beta_4  Age_{51-64} + \beta_5 Age_{65+} + \beta_6 Male \Big)$$

Where coefs are chosen roughly similar to a fitted model to pew that yields a sample size around 500. Namely:
```{r, echo = F}
r33_lin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling  probabilities

```{r, echo = F}
r33_lin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

## Linear Outcome Model

Trying to keep things straight forward, I use the identical outcome model with coefficients chosen to roughly match the observed vote choice Democratic margin. In other words, again we have:
$$p(Vote=D) = \alpha + \beta_1 Indep + \beta_2 Rep + \beta_3 Age_{36-50} + \beta_4  Age_{51-64} + \beta_5 Age_{65+} + \beta_6 Male$$

Coefficients were chose to be realistic and to yield $\hat{y}$'s that lie within a probability range:

```{r, echo = F}
r33_lin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target  in percentage points of:
```{r, echo = F}
cat(paste0("Linear=", r33_lin$linear, " Target= ",round(r33_lin$target,4)*100, "%"))
```


I then add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. 

I run simulations where this noise is $\sqrt(2)$, yielding an $R^2$ of .33, and of $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66. Finally, I run one last batch of simulations additionally adding noise by defining the outcome as bernoulli draws using the $\hat{y}$'s with $R^2$ of .66. This yields a $R^2$ of 0.017.

```{r, echo = F}
cat(paste0("For Linear Model w/ R2=",round(r33_lin$R2,3), " the correlation of Y with p(S=1) is ", round(r33_lin$corr,2), "\n"))
cat(paste0("For Linear Model w/ R2=",round(r66_lin$R2,3), " the correlation of Y with p(S=1) is ", round(r66_lin$corr,2), "\n"))
# cat(paste0("For R2=",round(r66_bern$R2,3), " the correlation with p(S=1) is ", 
#            round(r66_bern$corr,2), "\n"))
```

\clearpage

# Results: Linear Model

Bias and Standard Errors for the linear model with two levels of noise added  ($R^2=0.33$ & $R^2=0.66$)). 

## Linear + $\mathbf{R^2=.33}$

```{r r33_lin_res, echo = F}
load(r33_file_lin)
outcome_obj= r33_lin
good = which(lapply(sims, function (x) return(class(x))) == "list")

r33_emp_SEs = empirical_SEs(sims = sims)

dplyr_is_dumb = bias["unweighted","bias"]
bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )

kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear, " Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r33_emp_SEs$emp_SEs[,c(5:ncol(r33_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear, "Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(r33_lin$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear, "SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

rm(sims, gg_out, table, outcome, bias, SE_coverage)

```


## Linear + $\mathbf{R^2=.66}$

```{r r66_lin_res, echo=F}
load(r66_file_lin)
outcome_obj = r66_lin
good = which(lapply(sims, function (x) return(class(x))) == "list")

r66_emp_SEs = empirical_SEs(sims = sims)


dplyr_is_dumb = bias["unweighted","bias"]
bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear, " Bias \\textbf{in Percent} across ", length(good),
      " sims: All Methods (Target = ", round(mean(outcome),3)*100, "), $R^2$ on Outcome = ",
      round(r66_lin$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(r66_emp_SEs$emp_SEs[,c(5:ncol(r66_emp_SEs$emp_SEs))]*100,3), format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear, "Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear, "SE Coverage Results: Kpop",
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3)))%>%
  kable_styling(latex_options = "hold_position")
rm(sims, gg_out, table, outcome, bias, SE_coverage)
```


```{r, echo = F, eval = F, include = F}
load(r66_bern_file_lin)

good = which(lapply(sims, function (x) return(class(x))) == "list")

r66_bern_emp_SEs = empirical_SEs(sims = sims)

kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Bias \\textbf{in Percent} across ", length(good),
      " sims: All Methods (Target = ", round(mean(outcome),3)*100, "), $R^2$ on Outcome = ",
      round(r66_bern$R2, 3)))

kable(round(r66_bern_emp_SEs$emp_SEs[,c(5:ncol(r66_bern_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(r66_bern_lin$R2, 3)))

kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("SE Coverage Results: Kpop",
                          length(good), "sims; $R^2$ on Outcome = ", round(r66_bern_lin$R2, 3)))


```

\clearpage

# Simulation Set Up: Non-linear Models

## Non-Linear Selection Model 1:

This is the same model as above simply adding race to increase the complexity of the cross-sectional strata and hopefully increase the bias in ps due to dropped cells.

$$p(S=1) = logit^{-1}\Big( PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) + PID(3way)*Age(4way)\Big)$$


```{r, echo = F}
r66_nonlin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling probabilities:
```{r, echo = F}
r66_nonlin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

\clearpage

Looking visually at the selection probabilities:

```{r, echo = F, fig.width=14, fig.height=16}
gridExtra::grid.arrange(r66_nonlin$plot_pinclude, r66_nonlin$plot_pinclude_pid, nrow = 2)
```

## Non-Linear Outcome Model 1: 

As always, the outcome models using the same variables as the selection model, here adding race.

$$p(Vote=D) =  PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) + PID(3way)*Age(4way)$$

I again add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. Again this noise is $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66 and $\sqrt(2)$ yielding an $R^2$ of .33 . 

```{r, echo = F}
cat(paste0("For Non-Linear Model w/ RACE included and R2=",round(r33_nonlin$R2,3), "\n the correlation of Y with p(S=1) is ", round(r33_nonlin$corr,2), "\n"))

cat(paste0("For Non-Linear Model w/ RACE included and R2=",round(r66_nonlin$R2,3), "\n the correlation of Y with p(S=1) is ", round(r66_nonlin$corr,2), "\n"))

```


Coefficients are chosen through an automated procedure to be vaguely realistic and to yield $\hat{y}$'s that lie within a probability range. As a result, different noise-levels lead to the selection of slightly different outcome coefficients and population targets.

```{r, echo = F}
r33_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
r66_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target in percentage points of:
```{r, echo = F}
cat(paste0("Linear=", r33_nonlin$linear, " Target= ",round(r33_nonlin$target,4)*100, "%"))
cat(paste0("Linear=", r66_nonlin$linear, " Target= ",round(r66_nonlin$target,4)*100, "%"))
```


### Post-Stratification Dropped Cells 

I set up this non-linear model with variables that have many levels so that the intersection of $X^P$ in post-stratification is so complex that we're forced to drop units in the CCES population that have empty cells in the sample. I add race to this model to try and increase the complexity of the strata intersection and increase the bias of ps. Doing this, we drop the following:

```{r ps_drop_model2, echo = F}
load(r33_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model w/RACE included and R2=",round(r33_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE,  selection_model, pS_denom)


load(r66_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model w/RACE included and R2=",round(r66_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE,  selection_model, pS_denom)

```

\clearpage

#### Visual Correlation of Selection and Non-linear Outcome Model with Race

```{r, echo = F, fig.align = 'center', fig.width = 10, fig.height = 7}
 r66_nonlin$plot_pinclude_outcome
```

\clearpage


## Non-Linear Selection Model 2:

Here I add born again status and an interaction between born again status and age buckets to our non-linear model.

$$p(S=1) = logit^{-1}\Big( PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\ + BornAgain + PID(3way)*Age(4way) + BornAgain*Age(4way)\Big)$$


```{r, echo = F}
r5_nonlin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling probabilities:
```{r, echo = F}
r5_nonlin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

\clearpage

Looking visually at the selection probabilities:

```{r, echo = F, fig.width=14, fig.height=16}
gridExtra::grid.arrange(r5_nonlin$plot_pinclude, r5_nonlin$plot_pinclude_pid, nrow = 2)
```

\clearpage

## Non-Linear Outcome Model 2: 

As always, the outcome models using the same variables as the selection model, here adding race.

$$p(Vote=D) =  PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\ + BornAgain + PID(3way)*Age(4way) + BornAgain*Age(4way)$$

I again add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. Again this noise is $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66 and $\sqrt(2)$ yielding an $R^2$ of .33 . 

```{r, echo = F}
cat(paste0("For Non-Linear Model w/ RACE and BORNAGAIN included\n R2=",round(r5_nonlin$R2,3), "the correlation of Y with p(S=1) is ", round(r5_nonlin$corr,2), "\n"))
```

Coefficients are still chosen through an automated procedure to be vaguely realistic and to yield $\hat{y}$'s that lie within a probability range. 

```{r, echo = F}
r5_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target in percentage points of:
```{r, echo = F}
cat(paste0("Linear=", r5_nonlin$linear, " Target= ",round(r5_nonlin$target,4)*100, "%"))
```


### Post-Stratification Dropped Cells Model 2 (Race + Born)

```{r ps_drop_model, echo = F}
load(r5_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model w/RACE and BORNAGAIN included \n R2=",round(r5_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```

\clearpage


## Non-Linear Results:

Recall the true model includes: pid, gender, age buckets, 3way education and race. So it looks like our automated MF routine **"kpop_mf"** is potentially looking like the best recommendation?

- **"post_stratification"** applies ps on the true model (pid, education, gender, age, race)
- **"rake_truth"** calibrates on the true model (pid, education, gender, age, race)
- **"kpop REDUC"** is the kpop run without any constraints added, but with ONLY the variables in the selection model (pid, education, gender, age, race) given provided to kpop. 
- **"kpop Converged"** refers to default kpop with the full data, but forcing ebalance convergence. 

The other kpop methods have constraints appended to match the variables in each of the raking methods with names referring accordingly. **kpop+MF (XX)** appends "XX" variables directly while **kpop aMF** is the automated mean first approach.

- **"Demos"** appends age_buckets (4way), gender, race (4way), region (4way), and pid (3way) **"Demos+Edu"** adds education (6way) to these (NB: note that this "demos" model is almost the true model, but adds region and misses 3way education so it's interesting it seems to do so well)
- **"All"** appends all available variables including age buckets, gender, race, region, pid, education, income (5way), religion (5way), born-again (binary), and church attendance (4way).
- **"kpop+aMF"** refers to the automated mean first routine which appends an $m$ optimal number of constraints which are the $m$ left singular vectors of the svd of all the variables (same variables as **"all"**).


```{r r33_res, echo=F}
load(r33_nonlin_file)
outcome_obj = r33_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r33 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

g_out_r33 = ggplot(data = plot_lasso_margin_r33,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("Race Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))

table_r33 = plot_lasso_margin_r33 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))
#table_r33
dplyr_sux = table_r33 %>% filter(estimator_name == "Unweighted") %>% select(Bias) %>% pull()
table_r33 = table_r33 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r33 = as.data.frame(table_r33)
rownames(table_r33) = table_r33$estimator_name
table_r33 = table_r33[,-1]

rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```


```{r r66_res, echo=F}
load(r66_nonlin_file)
outcome_obj = r66_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r66 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

g_out_r66 = ggplot(data = plot_lasso_margin_r66,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("Race Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))


table_r66 = plot_lasso_margin_r66 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))

dplyr_sux = table_r66 %>% filter(estimator_name == "Unweighted") %>% select(Bias) %>% pull()
table_r66 = table_r66 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r66 = as.data.frame(table_r66)
rownames(table_r66) = table_r66$estimator_name
table_r66 = table_r66[,-1]

rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```

```{r r5_res, echo=F}
load(r5_nonlin_file)
outcome_obj = r5_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r5 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

g_out_r5 = ggplot(data = plot_lasso_margin_r5,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("Race Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))


table_r5 = plot_lasso_margin_r5 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))

dplyr_sux = table_r5 %>% filter(estimator_name == "Unweighted") %>% select(Bias) %>% pull()
table_r5 = table_r5 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r5 = as.data.frame(table_r5)
rownames(table_r5) = table_r5$estimator_name
table_r5 = table_r5[,-1]

rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```


### Non-linear + $\mathbf{R^2=.33}$ (Race only)

Note there is less bias here that the $R^2=.66$ results. This is because the automated procedure which selected the outcome coefficients resulted in an outcome model that was less strongly correlated with the selection probability. We still get around the same proportional decrease in bias however. Note that, unfortunately, we have very slight undercoverage for several of the kpop methods (most importantly kpop_mf and kpop_all)



```{r non_lin_res_race_r33, echo = F, warnings = F}
load(r33_nonlin_file)
outcome_obj = r33_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)
r33_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

# est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias))) 
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )

SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage

kable(round(table_r33, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(w Race) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r33_emp_SEs$emp_SEs[,grepl("kpop", colnames(r33_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(w Race) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(w Race) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```


\clearpage

### Non-linear + $\mathbf{R^2=.66}$ (Race only)

```{r non_lin_res_race_r66, echo = F, warnings = F}
load(r66_nonlin_file)
outcome_obj = r66_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)

r66_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

#est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias)))
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage

kable(round(table_r66, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(w Race) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(r66_emp_SEs$emp_SEs[,grepl("kpop", colnames(r66_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(w Race) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(w Race) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)

rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```

\clearpage

### Non-linear + $\mathbf{R^2=.5}$ (Race + Born Again)

```{r non_lin_res_race_r55, echo = F, warnings = F}
load(r5_nonlin_file)
rm(table)
outcome_obj = r5_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)

# est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias)))
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
# bias
SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage
r5_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))



kable(round(table_r5, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(w Race + Born) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r5_emp_SEs$emp_SEs[,grepl("kpop", colnames(r5_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(w Race + Born) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(w Race + Born) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```

\clearpage
### Box Plots 

```{r echo=F,fig.width=8, fig.height=6}
g_out_r33
g_out_r66
g_out_r5
```

















```{r la, echo = F, include = F}
load(r66_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
ps_dropped <- lapply(sims[good], `[[`, 4)
samp_check <- lapply(sims[good], `[[`, 5)

dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

mean(dropped_cells$sum)
samp_check = samp_check %>% bind_rows()
sum(samp_check$bad_sample)
mean(samp_check$drop_ps)
table(samp_check$check.leq_1pp) == length(good)
unique(samp_check$check.fail)
#what about less than 5pp? hmm yes more
table(samp_check$check.leq_5pp)
```




