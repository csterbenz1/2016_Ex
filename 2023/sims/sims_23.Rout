
R version 3.6.0 (2019-04-26) -- "Planting of a Tree"
Copyright (C) 2019 The R Foundation for Statistical Computing
Platform: x86_64-apple-darwin15.6.0 (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> ### Packages
> library(MASS)
Warning message:
package ‘MASS’ was built under R version 3.6.2 
> library(tidyverse)
── Attaching packages ─────────────────────────────────────── tidyverse 1.3.1 ──
✔ ggplot2 3.3.3     ✔ purrr   0.3.4
✔ tibble  3.2.1     ✔ dplyr   1.1.1
✔ tidyr   1.1.3     ✔ stringr 1.4.0
✔ readr   1.4.0     ✔ forcats 0.5.1
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
✖ dplyr::select() masks MASS::select()
Warning messages:
1: package ‘tidyverse’ was built under R version 3.6.2 
2: package ‘ggplot2’ was built under R version 3.6.2 
3: package ‘tidyr’ was built under R version 3.6.2 
4: package ‘readr’ was built under R version 3.6.2 
5: package ‘purrr’ was built under R version 3.6.2 
6: package ‘forcats’ was built under R version 3.6.2 
> library(survey)
Loading required package: grid
Loading required package: Matrix

Attaching package: ‘Matrix’

The following objects are masked from ‘package:tidyr’:

    expand, pack, unpack

Loading required package: survival

Attaching package: ‘survey’

The following object is masked from ‘package:graphics’:

    dotchart

Warning message:
package ‘Matrix’ was built under R version 3.6.2 
> #devtools::install_github("csterbenz1/KBAL", ref = "cat_kernel")
> library(kbal)
> library(parallel)
> library(knitr)
Warning message:
package ‘knitr’ was built under R version 3.6.2 
> library(glmnet)
Loaded glmnet 4.1-1
Warning message:
package ‘glmnet’ was built under R version 3.6.2 
> library(tictoc)
Warning message:
package ‘tictoc’ was built under R version 3.6.2 
> 
> ###### SET PARAMS  ###############
> set.seed(9345876)
> 
> if(detectCores() > 10) {
+   path_data = "/home/csterbenz/Data/"
+   cores_saved = 10
+   eval_kpop = T
+ } else if(detectCores() != 4) {
+   path_data= "/Users/Ciara_1/Dropbox/kpop/Updated/application/data/"
+   cores_saved = 3
+ } else {
+     path_data= "/Users/Ciara/Dropbox/kpop/Updated/application/data/"
+     cores_saved = 2
+ }
> options(dplyr.print_max = 1e9)
> #fit kpop and others to cces with populations weights?
> POPW = FALSE
> # to run with a linear kernel so it's way faster; UPDATE: errors catch this as mistake and prevent
> TEST = FALSE 
> #ebal tolerance and max iterations for kpop
> tolerance = 1e-4
> maxit = 500
> #adjust these both for runtime
> increment = 5
> min_num_dims = NULL
> max_num_dims = NULL
> eval_kpop = FALSE
> kpop_constraints = T
> SAVE = TRUE #save .Rdata results?
> #Need to adjust accordingly to machine for adequate number of sims
> #for cluster
> #nsims = (detectCores()-cores_saved)*14*2
> #First test with 10 sims
> #test runtime w/  1 iteration per core
> #nsims = (detectCores()-cores_saved)*1
> #full run nsims approx 1000
> nsims = (detectCores()-cores_saved)*60
> nsims
[1] 420
> 
> ##### Central Params to adjust
> 
> #add bernoulli noise by drawing binary from p(S=1)?
> bern = FALSE 
> #sd(y)*noise; 1-> r^2 = .5; sqrt(2) -> r^2 = .33; 1/2*sqrt(2) -> r^2 = .66;
> noise = 1 
> #adjusts sample size by dividing p(S) by scalar pS_denom (i.e. pS = plogis(XBeta)/pS_denom)
> pS_denom = 30
> #use the manually specified range of lambdas in the ridge residualization or allow glmnet to choose internally?
> manual_lambda = FALSE 
> #T=lambda as that which minimizes cverror in residualization; F= 1 sd from min choice
> lambda_min = FALSE 
> 
> 
> ###################### Formulas ################
> formula_rake_demos_noeduc <- ~recode_age_bucket + recode_female + recode_race +
+   recode_region + recode_pid_3way
> 
> #updated to include 6 way edu
> formula_rake_demos_weduc <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_educ + recode_pid_3way
> 
> formula_rake_all_vars <- ~recode_age_bucket + recode_female +
+   recode_race + recode_region + recode_pid_3way + recode_educ +
+   recode_income_5way + recode_relig_6way + recode_born + recode_attndch_4way
> 
> create_targets <- function (target_design, target_formula) {
+   target_mf <- model.frame(target_formula, model.frame(target_design))
+   target_mm <- model.matrix(target_formula, target_mf)
+   wts <- weights(target_design)
+ 
+   return(colSums(target_mm * wts) / sum(wts))
+ }
> 
> ### Post-stratification function
> ## For now assumes that strata variable is already created and in
> ## the data set and called "strata"
> postStrat <- function(survey, pop_counts, pop_w_col, strata_pass, warn = T) {
+   survey_counts <- survey %>%
+     group_by(!!as.symbol(strata_pass)) %>%
+     summarize(n = n()) %>%
+     ungroup() %>%
+     mutate(w_survey = n / sum(n))
+ 
+   pop_counts <- pop_counts %>%
+     rename(w_pop = matches(pop_w_col))
+   
+   if(warn == T & nrow(survey_counts) !=  nrow(pop_counts)) {
+       missing_strat = pop_counts[! (( pop_counts[, strata_pass]%>% pull()) %in% (survey_counts[, strata_pass]%>% pull() )), strata_pass]
+       warning(paste("Strata in Pop not found in Sample. Dropping", 
+                     sum(pop_counts[(pop_counts[, strata_pass] %>% pull()) %in% 
+                                        (missing_strat %>% pull()),"n" ]), 
+                     "empty cells\n"), immediate.  =T )
+   } 
+   post_strat <- pop_counts %>%
+     left_join(survey_counts, by = strata_pass) %>%
+     filter(!is.na(w_survey)) %>%
+     ## Normalizes back to 1 after dropping
+     ## empty cells
+     mutate(w_pop = w_pop * 1/sum(w_pop),
+            w = w_pop / w_survey) %>%
+     dplyr::select(!!as.symbol(strata_pass), w)
+ 
+   survey <- survey %>%
+     left_join(post_strat)
+ 
+   return(survey)
+ }
> 
> check_sample <- function(sample, selection_model) {
+     check = model.matrix(selection_model, data = sample)
+     check = colSums(check)
+     fail = check[which(check ==0)]
+     fail_bin = length(check[which(check ==0)]) > 0 
+     check_prop = check/nrow(sample)
+     
+     return(list(samp_prop = check_prop,
+                 fail  = fail, 
+                 fail_bin = fail_bin,
+                 counts = check))
+ }
> 
> check_sample_outcome <- function(sample, selection_model, interaction_cols, interaction_cols_2 = NULL) {
+     
+     vars = all.vars(selection_model)
+     var = NULL
+     counts = NULL
+     prop = NULL
+     u_outcome = NULL
+     #uninteracted variables
+     for(i in 1:length(vars)) {
+         t = sample %>% group_by_at(vars[i]) %>%
+             summarise(n = n(), 
+                       avg_outcome = mean(outcome)) %>%
+             mutate(prop = round(n/nrow(sample),4))
+         var = c(var, as.character(t[,1] %>% pull()))
+         counts = c(counts, t$n)
+         prop = c(prop,  t$prop)
+         u_outcome = c(u_outcome, t$avg_outcome)
+     }
+     #interactions
+     t = suppressMessages(sample %>% group_by_at(interaction_cols) %>% 
+         summarise(n = n(),
+                   avg_outcome = mean(outcome)) %>%
+         mutate(prop = round(n/nrow(sample), 4)))
+     interaction = apply(t, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+     counts = data.frame(var  = c(var, interaction),
+                n = c(counts, t$n), 
+                prop = c(prop, t$prop),
+                avg_outcome = c(u_outcome, t$avg_outcome))
+     
+     if(!is.null(interaction_cols_2)) {
+         t2 = suppressMessages(sample %>% group_by_at(interaction_cols_2) %>% 
+                                  summarise(n = n(),
+                                            avg_outcome = mean(outcome)) %>%
+                                  mutate(prop = round(n/nrow(sample), 4)))
+         interaction = apply(t2, 1,  function(r) paste(r[1],r[2], collapse = "_"))
+         append = cbind(data.frame(var = interaction), t2[, - c(1,2)])
+         counts = rbind(counts, append)
+     }
+     
+     fail = sum(counts$n == 0)
+     bad = sum(counts$prop <= 0.05)
+     bad_strata = data.frame(strata = as.character(counts$var[counts$prop <= 0.05]), prop = counts$prop[counts$prop <= 0.05])
+     v_bad = sum(counts$prop <= 0.01)
+     v_bad_strata = data.frame(strata = as.character(counts$var[counts$prop <= 0.01]), prop = counts$prop[counts$prop <= 0.01])
+     counts$var[counts$prop <= 0.01]
+     
+     counts = counts %>% mutate(leq_5pp = as.numeric(prop <= 0.05),
+                                leq_1pp = as.numeric(prop <= 0.01))
+     
+    
+     return(list(counts = counts,
+                 fail = fail, 
+                 bad = bad, 
+                 v_bad = v_bad, 
+                 bad_strata = bad_strata,
+                 v_bad_strata = v_bad_strata))
+ }
> 
> check_outcome <- function(outcome) {
+     beyond_support = (min(outcome) <0 | max(outcome) > 1)
+     return(beyond_support)
+ }
> 
> bound_outcome <- function(outcome, coefs, increment = 1, increment_intercept = .01, noise, cces_expanded, silent = T) {
+     denom = 10
+     fail = check_outcome(outcome)
+     while(fail) {
+         denom = denom + increment
+         if(!silent) { cat(denom, ": ") }
+         coefs_use = coefs/denom
+         outcome = cces_expanded %*% coefs_use
+         
+         outcome = outcome + rnorm(nrow(cces_expanded), mean = 0, sd = sd(outcome)*noise)
+         summary(outcome)
+         if(max(outcome) <=1 & min(outcome) <0 ) {
+             coefs[1] = coefs[1] + increment_intercept
+             if(!silent) { cat("\nmoving intercept up", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         if(max(outcome) >1 & min(outcome) >=0 ) {
+             coefs[1] = coefs[1] - increment_intercept
+             if(!silent) { cat("\nmoving intercept down", coefs[1],  "\n") }
+             denom = denom - increment
+         }
+         fail = check_outcome(outcome)
+         if(!silent) { cat(round(min(outcome),2), round(max(outcome),2),  "\n") }
+     }
+     if(!silent) { cat(paste("Min denom:", denom)) }
+     return(list(outcome = outcome, coefs = coefs_use, denom = denom))
+     
+ }
> 
> 
> ############# Load Data #####################
> #these data have been cleaned already see app_modeled for how it was done
> ## Load Pew
> pew <- readRDS(paste0(path_data, "pew_lasso_061021.rds"))
> pew$recode_age_bucket = as.character(pew$recode_age_bucket)
> pew$recode_age_3way= as.character(pew$recode_age_3way)
> ### Load Target Data
> cces <- readRDS(paste0(path_data, "cces_lasso_061021.rds"))
> cces$recode_age_bucket = as.character(cces$recode_age_bucket)
> cces$recode_age_3way= as.character(cces$recode_age_3way)
> 
> cces <- cces %>% mutate(recode_agesq = recode_age^2/ mean(recode_age^2),
+                         recode_agecubed = recode_age^3/ mean(recode_age^3))
> 
>  ##################### LASSO: Selection #############################
> 
> selection_model = as.formula(~recode_pid_3way + recode_female + recode_age_bucket 
+                                      + recode_educ_3way 
+                                      + recode_race
+                                      + recode_born 
+                                      + recode_born:recode_age_bucket
+                                      + recode_pid_3way:recode_age_bucket
+         )
> inter = c("recode_pid_3way", "recode_age_bucket")
> inter_2 = c("recode_born", "recode_age_bucket")
> cces_expanded = model.matrix(selection_model, data = cces)
> coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
> rownames(coefs) = colnames(cces_expanded)
> coefs
                                             [,1]
(Intercept)                                    NA
recode_pid_3wayInd                             NA
recode_pid_3wayRep                             NA
recode_femaleMale                              NA
recode_age_bucket36 to 50                      NA
recode_age_bucket51 to 64                      NA
recode_age_bucket65+                           NA
recode_educ_3wayCollege                        NA
recode_educ_3wayPost-grad                      NA
recode_raceHispanic                            NA
recode_raceOther                               NA
recode_raceWhite                               NA
recode_bornYes                                 NA
recode_age_bucket36 to 50:recode_bornYes       NA
recode_age_bucket51 to 64:recode_bornYes       NA
recode_age_bucket65+:recode_bornYes            NA
recode_pid_3wayInd:recode_age_bucket36 to 50   NA
recode_pid_3wayRep:recode_age_bucket36 to 50   NA
recode_pid_3wayInd:recode_age_bucket51 to 64   NA
recode_pid_3wayRep:recode_age_bucket51 to 64   NA
recode_pid_3wayInd:recode_age_bucket65+        NA
recode_pid_3wayRep:recode_age_bucket65+        NA
> #(p(S)) for negative bias select non dem voters
> # coefs[,1] = c(-2, #intercept -5 w race
> #               2, #selection of indep pos
> #               2, #selection of R pos
> #               .5, #male
> #               .15, #36-50,
> #               .2, #51-64,
> #               .2, #65+,
> #               .7, #college
> #               -1 , #post-grad
> #               .5,#hispanic
> #               .3,#other
> #               .7,#white
> #                2, #bornagain
> #               1,#bornagain x 36-50
> #               1.5, #bornagain x 51-64
> #               2, #bornagain x 65+
> #               .3,#ind x 36-50
> #               .5, #rep x 36-50,
> #               1, #ind x 51-64,
> #               1, #rep x 51-64,
> #               -.2, #ind x 65+
> #               2 #rep x 65+ )
> coefs[,1] = c(-4, #intercept -5 w race
+               .5, #selection of indep pos
+               1.5, #selection of R pos
+               .5, #male
+               2, #36-50,
+               1.6, #51-64,
+               1, #65+,
+               .7, #college
+               -.5, #post-grad
+               .5,#hispanic
+               .3,#other
+               .7,#white
+               2, #bornagain
+               1,#bornagain x 36-50
+               1.5, #bornagain x 51-64
+               2, #bornagain x 65+
+               -3.4,#ind x 36-50
+               -5.8, #rep x 36-50,
+               .7, #ind x 51-64,
+               -2, #rep x 51-64,
+               -1.5, #ind x 65+
+               1.1 #rep x 65+
+ )
> 
> xbeta = cces_expanded %*% coefs
> p_include = plogis(xbeta)
> p_include = p_include/pS_denom
> sum(p_include)
[1] 499.4255
> 
> # gg_dat = data.frame(Selection_Probability = p_include,
> #                    Pid = cces$recode_pid_3way,
> #                    age = cces$recode_age_bucket,
> #                    born = cces$recode_born,
> #                    Outcome_pD = NA)
> # ggplot(gg_dat) +
> #     geom_density(aes(x= Selection_Probability, color = age, linetype = Pid)) +
> #     ggtitle("Distribution of Selection Probabilities by Party") +
> #     theme_bw()
> 
> 
> 
> # LA:tinkering
> 
> # formula_ps <- selection_model
> # cces <- bind_cols(cces, cces %>%
> #                       unite("strata", all.vars(formula_ps), remove = FALSE) %>%
> #                       unite("strata_reduc", all.vars(formula_ps_reduc),
> #                             remove = FALSE) %>%
> #                       unite("strata_all", all.vars(formula_ps_all)) %>%
> #                       dplyr::select(strata, strata_reduc, strata_all))
> # sample <- rbinom(nrow(cces), 1, p_include)
> # sum(sample)
> # survey_sim <- cces[sample == 1, ]
> # 
> # check_sample_outcome(survey_sim, selection_model, inter)
> # #check_sample_outcome(survey_sim, selection_model, inter_2)
> # n_distinct(cces$strata) - n_distinct(survey_sim$strata)
> # dropped_strata = unique(cces$strata)[which(!(unique(cces$strata) %in%
> #                                                  unique(survey_sim$strata)))]
> # cces %>% filter(strata %in% dropped_strata) %>% count()
> # 
> # la = data.frame(pS = p_include, pid = cces$recode_pid_3way) %>%
> #     group_by(pid) %>% summarise(max = round(max(pS)*100,2 ))
> # # la
> # #
> # # check_sample_outcome(survey_sim,selection_model, inter)
> # gg_dat = data.frame(Selection_Probability = p_include,
> #                     Pid = cces$recode_pid_3way,
> #                     age = cces$recode_age_bucket,
> #                     born = cces$recode_born,
> #                     Outcome_pD = NA)
> # gg_p_include_pid = ggplot(gg_dat) +
> #     geom_density(aes(x= Selection_Probability, color = Pid)) +
> #     annotate(geom = "label", x=quantile(p_include,.25),y=Inf, vjust = 1,
> #              color = "red",
> #              label =  paste0("Dem Max P(S)= ", la[la$pid =="Dem", "max"], "%")) +
> #     annotate(geom = "label",x=quantile(p_include,.25), y=Inf, vjust =3,
> #              label= paste0("Ind Max P(S)= ", la[la$pid =="Ind", "max"], "%" ),
> #              color = "green") +
> #     annotate(geom = "label",x=quantile(p_include,.25), y=Inf, vjust = 5,
> #              label= paste0("Rep Max P(S)= ", la[la$pid =="Rep", "max"], "%" ),
> #              color = "blue") +
> #     # geom_text(x=.10, y=250, label= paste0("Dem Max P(S)=", la[la$pid =="Dem", "max"] ),
> #     #           color = "red") +
> #     #  geom_text(x=.10, y=200, label= paste0("Ind Max P(S)=", la[la$pid =="Ind", "max"] ),
> #     #           color = "green") +
> #     #  geom_text(x=.10, y=150, label= paste0("Rep Max P(S)=", la[la$pid =="Rep", "max"] ),
> #     #           color = "Blue") +
> #     ggtitle("Distribution of Selection Probabilities by Party") +
> #     theme_bw()
> # gg_p_include_pid
> # 
> # ggplot(gg_dat) +
> #     geom_density(aes(x= Selection_Probability, color = age, linetype = Pid)) +
> #     ggtitle("Distribution of Selection Probabilities by Party") +
> #     theme_bw()
> 
> # 
> #most likely to select: rep, 65+, male, protestants
> # cces[which(p_include == max(p_include)),
> #      c("recode_pid_3way", "recode_age_bucket", "recode_female", "recode_relig_6way")]
> # #summary(p_include)
> 
> #################### DESIGN OUTCOME MODEL ##################
> coefs_outcome = coefs
> #p(D)
> #this -coefs method gets good bias on ps and other raking but rake all is p good :/
> coefs_outcome = -coefs
> #cor(p_include, cces_expanded %*% coefs_outcome)
> #coefs_outcome = coefs_outcome*1.5
> coefs_outcome[1] = 15
> cor(p_include, cces_expanded %*% coefs_outcome)
           [,1]
[1,] -0.9567332
> if(!bern) {
+     cat(paste("Adding sd(outcome)*",round(noise, 3), "\n")) 
+     set.seed(1383904)
+     bound = bound_outcome(outcome = cces_expanded %*% coefs_outcome,
+                           coefs = coefs_outcome,
+                           cces_expanded = cces_expanded,
+                           noise = noise, silent = F)
+     coefs_outcome = bound$coefs
+     xbeta_outcome = bound$outcome
+     beyond_support = check_outcome(xbeta_outcome)
+ 
+     if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {
+         warning("Outcome beyond prob support for some units when noise is added",
+                                                                 immediate. = T)
+     }
+ } else {
+     cat(paste("Adding bernoulli noise",noise, "\n"))  
+     set.seed(1383904)
+     xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
+     cat(paste("Corr of S (one sample draw) and Y", round(cor(sample, xbeta_outcome),3)))
+ }
Adding sd(outcome)* 1 
11 : -0.07 2.11 
12 : -0.01 2.06 
13 : -0.18 1.79 
14 : -0.09 1.68 
15 : -0.04 1.57 
16 : -0.06 1.49 
17 : -0.1 1.34 
18 : -0.17 1.38 
19 : -0.04 1.29 
20 : -0.02 1.14 
21 : -0.01 1.12 
22 : -0.04 1.18 
23 : -0.02 1.05 
24 : 
moving intercept up 15.01 
-0.02 0.97 
24 : 
moving intercept up 15.02 
-0.08 0.99 
24 : -0.06 1.01 
25 : 
moving intercept up 15.03 
-0.02 0.96 
25 : 0 0.95 
Min denom: 25> summary(xbeta_outcome)
       V1          
 Min.   :0.003209  
 1st Qu.:0.403368  
 Median :0.489107  
 Mean   :0.483646  
 3rd Qu.:0.568801  
 Max.   :0.950449  
> # cor(xbeta_outcome, p_include)
> # cces[which(xbeta_outcome == min(xbeta_outcome)),
> #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
> # cces[which(xbeta_outcome == max(xbeta_outcome)),
> #      c("recode_pid_3way", "recode_age_bucket", "recode_female")]
> # 
> plot(density(xbeta_outcome))
> cat(paste("Mean outcome w/noise is", round(mean(xbeta_outcome)*100,3), "\n"))
Mean outcome w/noise is 48.365 
> 
> cat(paste("Range of outcome w/noise is\n"))
Range of outcome w/noise is
> cat(paste(summary(xbeta_outcome), "\n"))
Min.   :0.003209   
 1st Qu.:0.403368   
 Median :0.489107   
 Mean   :0.483646   
 3rd Qu.:0.568801   
 Max.   :0.950449   
> s = summary(lm(update(selection_model, xbeta_outcome ~ .),data = cces))
> R2_outcome = s$adj.r.squared
> cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
R^2 outcome is 0.496 
> cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
Mean scaled outcome (target) is 48.365> cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))

Corr of sampling prob and outcome  -0.675> cces$outcome = xbeta_outcome
> 
> # #LA: 
> # gg_dat = data.frame(Selection_Probability = p_include,
> #                     Pid = cces$recode_pid_3way,
> #                     age = cces$recode_age_bucket,
> #                     Outcome_pD = cces$outcome)
> # gg_p_include_outcome = ggplot(gg_dat) +
> #     geom_point(aes(x= Selection_Probability, y= Outcome_pD, color = age, shape = Pid)) +
> #     theme_bw()
> # gg_p_include_outcome
> 
> 
> 
> ######### Make STRATA variable in CCES and Pew ############
> formula_ps <- selection_model
> cces <- bind_cols(cces, cces %>%
+                       unite("strata", all.vars(formula_ps), remove = FALSE) %>%
+                       dplyr::select(strata))
> 
> 
> #################### Targets ###################
> if(POPW) {
+   cces_svy <- svydesign(ids = ~1, weights = ~commonweight_vv_post, data = cces)
+ } else {
+   cces_svy <- suppressWarnings(svydesign(ids = ~1, data = cces))
+ }
> margin_sim = svymean(~outcome, cces_svy)[1]* 100
> margin_sim
 outcome 
48.36457 
> targets_rake_demos_noeduc <- create_targets(cces_svy,
+                                             formula_rake_demos_noeduc)
> targets_rake_demos_weduc <- create_targets(cces_svy, formula_rake_demos_weduc)
> targets_rake_all_vars <- create_targets(cces_svy,
+                                         formula_rake_all_vars)
> targets_demo_truth <- create_targets(cces_svy, selection_model)
> 
> 
> ## Make table of Population Counts for post-stratification for manual ps function
> cces_counts <- cces %>%
+   group_by(strata) %>%
+   summarize(n = if(!POPW) {n()} else {sum(commonweight_vv_post, na.rm = TRUE)}) %>%
+   ungroup() %>%
+   mutate(w = n / sum(n, na.rm = TRUE))
> 
> 
> 
> ########################### PREP SIMS ##################################
> #save mem
> rm(pew)
> #rm(pew, lasso_pinclude, lasso_include,lasso_lambda,
> #   stack_data, mod)
> 
> 
> margins_formula <- ~recode_vote_2016 + 
+   mod_cces_on_cces_pR + mod_cces_on_cces_pD + mod_cces_on_cces_pO+
+   diff_cces_on_cces + margin_cces_on_cces + 
+   recode_pid_3way + 
+   recode_female + recode_race +recode_region + recode_educ + recode_relig_6way + 
+   recode_born + recode_attndch_4way + recode_income_5way +
+   recode_age_bucket + recode_age + recode_agesq + recode_agecubed + recode_age_factor + 
+   recode_race_educ_reg + recode_educ_wh_3way + 
+   recode_educ_pid_race +
+   recode_pid_race + 
+   recode_educ_pid +
+   recode_race_reg_wh_educ + 
+   recode_midwest_edu_race +
+   recode_midwest_wh_edu
> 
> est_mean <- function(outcome, design) {
+   svymean(as.formula(paste0("~", outcome)), design, na.rm = TRUE)[1]
+ }
> 
> #########################################
> ############## variance calc ###########
> ## Variance functions
> var_fixed <- function(Y, weights, pop_size) {
+     ## note: needs weights that sum to population total
+     if(round(sum(weights)) != pop_size) { weights = weights*pop_size/sum(weights)}
+     return(Hmisc::wtd.var(Y, weights))
+ }
> 
> ## kott (14) (under poisson)
> var_quasi <- function(weights, residuals, pop_size) {
+     #moving from kott 14 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     #sum^n (w_i^2 - 1/N_pop w_i)e_i^2 
+     return(sum((weights^2 - (weights / pop_size))*residuals^2))
+ }
> 
> ## kott (15) linearization
> var_linear <- function(weights, residuals, sample_size) {
+     #moving from kott 14 w sum w =N to weights that sum to 1 + var of total to var of mean:
+     # n/(n-1) sum^n (w_i*e_i)^2 - (1/n-1) [sum^n] *using this for now
+     # approx = sum^n (w_i*e_i)^2 - (1/n) [sum^n]
+     n = sample_size
+     return((n/(n-1))*sum((weights * residuals)^2) - 1/n * sum(weights * residuals)^2)
+ }
> 
> ## chad
> var_chad <- function(weights, residuals) {
+     return(sum(weights^2 * residuals^2))
+ }
> 
> ## calculate all variances
> calc_SEs <- function(Y, residuals, pop_size, weights, sample_size) {
+     if(round(sum(weights)) != 1 ) {
+         weights = weights/sum(weights)
+     }
+     return(data.frame(SE_fixed = sqrt(var_fixed(Y, weights, pop_size) / length(Y)),
+                       SE_quasi = sqrt(var_quasi(weights, residuals, pop_size)),
+                       SE_linear = sqrt(var_linear(weights, residuals, sample_size)),
+                       SE_chad = sqrt(var_chad(weights, residuals))))
+ }
> 
> ########################### RUN SIMS ##################################
> if(detectCores() < 20) {
+     nsims = 50
+     eval_kpop = TRUE
+     SAVE = T
+     kpop_constraints = F
+ }
> #double check params are as expected:
> nsims
[1] 50
> sum(p_include)
[1] 499.4255
> SAVE
[1] TRUE
> eval_kpop
[1] TRUE
> manual_lambda
[1] FALSE
> pryr::mem_used()
185 MB
> rm(bound, cces_expanded, s, xbeta, xbeta_outcome)
> pryr::mem_used()
176 MB
> # lambda_min
> system.time({
+   sims <- mclapply(1:nsims, function(nsim) {
+     #
+     system.time({
+     cat(paste("=====================  SIM:",nsim, 
+               "===================== \n"))
+ 
+     sample <- rbinom(nrow(cces), 1, p_include)
+     survey_sim <- cces[sample == 1, ]
+     
+     survey_design <- suppressWarnings(svydesign(ids = ~1, data = survey_sim))
+     
+     ########################### check sample ##########################
+     check_s = check_sample(survey_sim, selection_model)
+     bad_sample = check_s$fail_bin
+     check_2 = check_sample_outcome(survey_sim, selection_model, interaction_cols = inter, interaction_cols_2 = inter_2)
+     
+     check_nums = c(leq_5pp = check_2$bad,
+                    leq_1pp = check_2$v_bad, 
+                    fail = check_2$fail)
+     s = survey_sim %>% group_by(recode_pid_3way,recode_female, recode_age_bucket,
+                                 recode_educ_3way) %>% count() %>%
+         mutate(n_s = round(n/nrow(survey_sim), 3))
+     c = cces %>% group_by(recode_pid_3way, recode_female, recode_age_bucket,
+                           recode_educ_3way) %>% count() %>%
+         mutate(n_c = round(n/nrow(cces), 3))
+     count = nrow(c) - nrow(s)
+     
+     ############################################
+     ## Unweighted estimate
+     ############################################
+     
+     unweighted <- est_mean("outcome", survey_design)
+     ############################################
+     ## Sample size
+     ############################################\
+     n <- sum(sample) 
+     
+     ############################################
+     ## Raking on demographics (no education)
+     ############################################
+     rake_demos_noeduc_svyd <- try(calibrate(design = survey_design,
+                                         formula = formula_rake_demos_noeduc,
+                                         population = targets_rake_demos_noeduc,
+                                         calfun = "raking"), silent = T)
+     
+     rake_demos_noeduc <- tryCatch(est_mean("outcome", rake_demos_noeduc_svyd), error = function(e) NA)
+     
+     #SEs
+     if(class(rake_demos_noeduc_svyd)[1] == "try-error") {
+         rake_demos_noeduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY  = NA)
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_noeduc, outcome ~ .), 
+                                  data = rake_demos_noeduc_svyd$variables))
+ 
+         res_rake_demos_noeduc = data.frame(min = min(residuals), 
+                                            perc_25 = quantile(residuals, .25), 
+                                            mean = mean(residuals),
+                                            perc_75 = quantile(residuals, .75),
+                                            var = var(residuals))
+ 
+         rake_demos_noeduc_se <- calc_SEs(Y = rake_demos_noeduc_svyd$variables$outcome, 
+                                          residuals = residuals, 
+                                          pop_size = nrow(cces), 
+                                          sample_size = sum(sample),
+                                          weights = weights(rake_demos_noeduc_svyd))
+         names(rake_demos_noeduc_se) = paste0("rake_demos_noeduc_", names(rake_demos_noeduc_se))
+         
+         
+         rake_demos_noeduc_se_SVY = data.frame(rake_demos_noeduc_se_SVY = data.frame(svymean(~outcome, 
+                                                                                             rake_demos_noeduc_svyd, 
+                                                                                             na.rm = TRUE))[1,2])
+         
+     }
+     ############################################
+     #### Raking on demographics (with education)
+     ############################################
+     
+     rake_demos_weduc_svyd <- try(calibrate(design = survey_design,
+                                        formula = formula_rake_demos_weduc,
+                                        population = targets_rake_demos_weduc,
+                                        calfun = "raking"), silent = T)
+ 
+     rake_demos_weduc <- tryCatch(est_mean("outcome", rake_demos_weduc_svyd), 
+                                 error = function(e) NA)
+     
+     
+     #SEs
+     if(class(rake_demos_weduc_svyd)[1] == "try-error") {
+         rake_demos_weduc_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_demos_weduc, outcome ~ .), 
+                                  data = rake_demos_weduc_svyd$variables))
+         res_rake_demos_wedu = data.frame(min = min(residuals), 
+                                          perc_25 = quantile(residuals, .25), 
+                                          mean = mean(residuals),
+                                          perc_75 = quantile(residuals, .75),
+                                          var = var(residuals))
+         rake_demos_weduc_se <- calc_SEs(Y = rake_demos_weduc_svyd$variables$outcome, 
+                                         residuals = residuals, 
+                                         pop_size = nrow(cces),
+                                         sample_size = sum(sample),
+                                         weights = weights(rake_demos_weduc_svyd))
+         names(rake_demos_weduc_se) = paste0("rake_demos_weduc_", names(rake_demos_weduc_se))
+         
+     }
+     
+     ############################################
+     #### Raking on everything
+     ############################################
+     rake_all_svyd <- try(calibrate(design = survey_design,
+                                formula = formula_rake_all_vars,
+                                population = targets_rake_all_vars,
+                                calfun = "raking"))
+     
+     rake_all <- tryCatch(est_mean("outcome", rake_all_svyd), 
+                          error = function(e) NA)
+     #SEs
+     if(class(rake_all_svyd)[1] == "try-error") {
+         rake_all_se <- data.frame(SE_fixed = NA,
+                                           SE_quasi = NA, 
+                                           SE_linear = NA, 
+                                           SE_chad = NA) 
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     } else {
+         residuals = residuals(lm(update(formula_rake_all_vars, outcome ~ .), 
+                                  data = rake_all_svyd$variables))
+         res_rake_all = data.frame(min = min(residuals), 
+                                   perc_25 = quantile(residuals, .25), 
+                                   mean = mean(residuals),
+                                   perc_75 = quantile(residuals, .75),
+                                   var = var(residuals))
+         rake_all_se <- calc_SEs(Y = rake_all_svyd$variables$outcome, 
+                                 residuals = residuals, 
+                                 pop_size = nrow(cces), 
+                                 sample_size = sum(sample),
+                                 weights = weights(rake_all_svyd))
+         names(rake_all_se) = paste0("rake_all_", names(rake_all_se))
+         
+     }
+     
+     ############################################
+     ## Post-stratification: Truth
+     ############################################
+     
+     #track empty cells:
+     #this subsets cces strata to only those in survey_sim
+     missing_strata <- unique(cces$strata)[!(unique(cces$strata) %in%
+                                                 unique(survey_sim$strata))]
+     cat(round(length(missing_strata)/ length(unique(cces$strata)),3),
+         "% cces original strata missing from sample, ",
+         " and", cces %>% filter(strata %in% missing_strata) %>% summarise(n()) %>% pull(), "/", nrow(cces), "units\n" )
+     dropped_cells = cces %>% filter(strata %in% missing_strata) %>% group_by(strata) %>% count()
+     #dropped_cells = data.frame(sum = sum(dropped_cells$n), strata = paste(dropped_cells$strata, collapse = " | "))
+     dropped_cells = sum(dropped_cells$n)
+    
+     #dropped_cells_all = data.frame(sum = sum(dropped_cells_all$n), strata = paste(dropped_cells_all$strata, collapse = " | "))
+ 
+     #note that we no longer have the issue of pew having strata that cces doesn bc
+     #we use survey_sim a sample of cces so it will never have diff strata
+     
+     post_stratification_svyd = svydesign(~1, data = postStrat(survey_sim, 
+                                                               cces_counts, "w", 
+                                                               strata_pass = "strata", 
+                                                               warn = F),
+                                          weights = ~w)
+     
+     post_stratification <- est_mean("outcome", post_stratification_svyd)
+     
+     #SEs
+     post_stratification_se <- data.frame(post_strat_SE_svy = data.frame(svymean(~outcome, post_stratification_svyd, na.rm = TRUE))[1,2])
+     
+ 
+     
+     ############################################
+     #### Raking on true model
+     ############################################
+     #very messy error catching for the moment just a stop gap to see how things look
+     rake_truth_svyd <- try(calibrate(design = survey_design,
+                                      formula = selection_model,
+                                      population = targets_demo_truth,
+                                      maxit = 100,
+                                      calfun = "raking"), silent = T)
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+         rake_truth_svyd <- try(calibrate(design = survey_design,
+                                          formula = selection_model,
+                                          population = targets_demo_truth,
+                                          calfun = "raking",
+                                          maxit = 100,
+                                          epsilon = .009), silent = T)
+     }
+     
+     if(class(rake_truth_svyd)[1] == "try-error") {
+     
+         rake_truth_se <- data.frame(SE_fixed = NA,
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA) 
+         names(rake_truth_se) = paste0("rake_all_", names(rake_all_se))
+         
+     } else {
+         lambdas <- 10^seq(3, -2, by = -.1)
+         x <- model.matrix(update(selection_model, outcome ~ .),
+                           data = rake_truth_svyd$variables)[, -1]
+         fit <- glmnet(x, 
+                       rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         cv_fit <- cv.glmnet(x, rake_truth_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         opt_lambda <- cv_fit$lambda.min
+         fit <- cv_fit$glmnet.fit
+         
+         residuals = rake_truth_svyd$variables$outcome - predict(fit, s = opt_lambda, newx = x)
+         res_rake_truth = data.frame(min = min(residuals), 
+                                     perc_25 = quantile(residuals, .25), 
+                                     mean = mean(residuals),
+                                     perc_75 = quantile(residuals, .75),
+                                     var = var(residuals))
+         rake_truth_se <- tryCatch(calc_SEs(Y = rake_truth_svyd$variables$outcome,
+                                            residuals = residuals,
+                                            pop_size = nrow(cces),
+                                            sample_size = sum(sample),
+                                            weights = weights(rake_truth_svyd)), error = function(e) NA)
+     }
+     
+     rake_truth <- tryCatch(est_mean("outcome", rake_truth_svyd), 
+                            error = function(e) NA)
+     truth_margins <- tryCatch(svymean(margins_formula, rake_truth_svyd), 
+                            error = function(e) NA)
+ 
+     #HT and Hayek
+     p_sample <- as.matrix((p_include[sample==1]))
+     
+     #HT
+     ht_truth = sum((cces[sample ==1, "outcome"]/p_sample))/nrow(cces) 
+     #Hayek
+     hayek_truth = sum((cces[sample ==1, "outcome"]/p_sample))/sum(1/p_sample)
+     
+     ############################################
+     ## Kpop: Categorical Data + b = argmax V(K)
+     ############################################
+     if(eval_kpop) {
+ 
+       # Select the covariates for use in Kbal: updated cat data no cont age
+       #one-hot coded for cat kernel
+       kbal_data <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                           recode_female,
+                                                           recode_race,
+                                                           recode_region,
+                                                           recode_pid_3way,
+                                                           recode_educ,
+ 
+                                                           recode_income_5way,
+                                                           recode_relig_6way,
+                                                           recode_born,
+                                                           recode_attndch_4way),
+                              cces %>% dplyr::select(recode_age_bucket,
+                                                     recode_female,
+                                                     recode_race,
+                                                     recode_region,
+                                                     recode_pid_3way,
+                                                     recode_educ,
+ 
+                                                     recode_income_5way,
+                                                     recode_relig_6way,
+                                                     recode_born,
+                                                     recode_attndch_4way))
+       
+       kbal_data <- bind_rows(survey_sim %>% dplyr::select(recode_pid_3way, 
+                                                           recode_female, 
+                                                           recode_age_bucket,
+                                                           recode_educ_3way,
+                                                           recode_race,
+                                                           recode_born),
+                              cces %>% dplyr::select(recode_pid_3way, 
+                                                     recode_female, 
+                                                     recode_age_bucket,
+                                                     recode_educ_3way,
+                                                     recode_race,
+                                                     recode_born))
+       
+       
+       kbal_data_reduc <- bind_rows(survey_sim %>% dplyr::select(all.vars(selection_model)),
+                              cces %>% dplyr::select(all.vars(selection_model)))
+       
+       kbal_data_sampled <- c(rep(1, nrow(survey_sim)), rep(0, nrow(cces)))
+       ##### Demos Constraint
+       rake_demos_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                       recode_female,
+                                                                       recode_race,
+                                                                       recode_region,
+                                                                       recode_pid_3way),
+                                          cces %>% dplyr::select(recode_age_bucket,
+                                                                 recode_female,
+                                                                 recode_race,
+                                                                 recode_region,
+                                                                 recode_pid_3way))%>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_demos_constraint <- rake_demos_constraint[,-1]
+       rake_demos_constraint <- scale(rake_demos_constraint)
+ 
+ 
+       rake_demos_wedu_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                            recode_female,
+                                                                            recode_race,
+                                                                            recode_region,
+                                                                            recode_pid_3way,
+                                                                            recode_educ),
+                                               cces %>% dplyr::select(recode_age_bucket,
+                                                                      recode_female,
+                                                                      recode_race,
+                                                                      recode_region,
+                                                                      recode_pid_3way,
+                                                                      recode_educ))%>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_demos_wedu_constraint <- rake_demos_wedu_constraint[,-1]
+       rake_demos_wedu_constraint <- scale(rake_demos_wedu_constraint)
+ 
+ 
+       rake_all_constraint <- bind_rows(survey_sim %>% dplyr::select(recode_age_bucket,
+                                                                     recode_female,
+                                                                     recode_race,
+                                                                     recode_region,
+                                                                     recode_pid_3way,
+                                                                     recode_educ,
+ 
+                                                                     recode_income_5way,
+                                                                     recode_relig_6way,
+                                                                     recode_born,
+                                                                     recode_attndch_4way),
+                                        cces %>% dplyr::select(recode_age_bucket,
+                                                               recode_female,
+                                                               recode_race,
+                                                               recode_region,
+                                                               recode_pid_3way,
+                                                               recode_educ,
+                                                               recode_income_5way,
+                                                               recode_relig_6way,
+                                                               recode_born,
+                                                               recode_attndch_4way)) %>%
+         model.matrix(as.formula("~."), .)
+ 
+       rake_all_constraint <- rake_all_constraint[,-1]
+       rake_all_constraint <- scale(rake_all_constraint)
+       
+       
+         #cat("===================== Running Kbal ==================\n")
+         
+         #### DEFAULT ######
+         cat(paste("nsim:", nsim, "DEFAULT", "\n"))
+         kbal_est <- kbal(allx=kbal_data,
+                          sampled = kbal_data_sampled,
+                          #b = b_manual[i],
+                          cat_data = TRUE,
+                          incrementby = increment,
+                          meanfirst = FALSE,
+                          ebal.tol = tolerance,
+                          ebal.maxit = maxit,
+                          minnumdims = min_num_dims,
+                          maxnumdims = max_num_dims,
+                          linkernel = if(TEST){TRUE} else{ FALSE},
+                          sampledinpop = FALSE,
+                          fullSVD = TRUE)
+         
+         kpop_svyd <- svydesign(~1, data = survey_sim,
+                                weights = kbal_est$w[kbal_data_sampled ==1])
+         
+         kpop <- est_mean("outcome", kpop_svyd)
+         b_kpop = kbal_est$b
+         #save memory by saving only the svd to re use
+         svdK = kbal_est$svdK 
+         numdims = kbal_est$numdims
+         biasbound_r = kbal_est$biasbound_ratio
+         biasbound = kbal_est$biasbound_opt
+         
+         ##### Kpop SEs
+         kpop <- tryCatch(est_mean("outcome", kpop_svyd), error = function(e) NA)
+         
+         lambdas <- if(manual_lambda) { 10^seq(3, -2, by = -.1) } else {NULL}
+         
+         x <- as.matrix(data.frame(kbal_dims = kbal_est$svdK$v[, 1:kbal_est$numdims]))
+         cv_fit <- cv.glmnet(x, kpop_svyd$variables$outcome, alpha = 0, lambda = lambdas)
+         lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+         
+         residuals = kpop_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                           s = lambda_pass, newx = x)
+         res_kpop = data.frame(min = min(residuals), 
+                               perc_25 = quantile(residuals, .25), 
+                               mean = mean(residuals),
+                               perc_75 = quantile(residuals, .75),
+                               var = var(residuals))
+         kpop_se <- tryCatch(calc_SEs(Y = kpop_svyd$variables$outcome,
+                                      residuals = residuals,
+                                      pop_size = nrow(cces),
+                                      sample_size = sum(sample),
+                                      weights = weights(kpop_svyd)), error = function(e) NA)
+         
+         if(length(kpop_se) == 1) {
+             kpop_se <- data.frame(SE_fixed = NA, 
+                                   SE_quasi = NA, 
+                                   SE_linear = NA, 
+                                   SE_chad = NA)
+         }
+         names(kpop_se) = tryCatch(paste0("kpop_", names(kpop_se)), error = function(e) NA)
+         
+ 
+     
+         #CONVERGED
+         dist_record = data.frame(t(kbal_est$dist_record))
+         min_converged = dist_record[which.min(dist_record[dist_record$Ebal.Convergence ==1,"BiasBound"]), "Dims"]
+         
+         rm(kbal_est, residuals, x, cv_fit)
+         
+         #CONVERGED
+         #### CONVG ####
+         cat(paste("nsim:", nsim, "CONV", "\n"))
+         if(is.null(min_converged) | length(min_converged) ==0) {
+           kpop_svyd_conv <- "dn converge"
+           kpop_conv <- "dn converge"
+           
+           numdims_conv = "dn converge"
+           biasbound_r_conv = "dn converge"
+           biasbound_conv = "dn converge"
+           kpop_conv_se = data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           
+         } else {
+           kbal_est_conv <- kbal(allx=kbal_data,
+                                 K.svd = svdK,
+                                 sampled = kbal_data_sampled,
+                                 numdims = min_converged,
+                                 ebal.tol = tolerance,
+                                 ebal.maxit = maxit,
+                                 minnumdims = min_num_dims,
+                                 maxnumdims = max_num_dims,
+                                 scale_data = FALSE,
+                                 drop_MC = FALSE,
+                                 incrementby = increment,
+                                 meanfirst = FALSE,
+                                 sampledinpop = FALSE,
+                                 ebal.convergence = TRUE)
+           kpop_svyd_conv <- svydesign(~1, data = survey_sim,
+                                       weights = kbal_est_conv$w[kbal_data_sampled ==1])
+           kpop_conv <- est_mean("outcome", kpop_svyd_conv)
+           
+           numdims_conv = kbal_est_conv$numdims
+           biasbound_r_conv = kbal_est_conv$biasbound_ratio
+           biasbound_conv = kbal_est_conv$biasbound_opt
+           
+           #SEs
+           x <- as.matrix(data.frame(kbal_dims = kbal_est_conv$svdK$v[, 1:kbal_est_conv$numdims]))
+           cv_fit <- cv.glmnet(x, kpop_svyd_conv$variables$outcome, alpha = 0, 
+                               lambda = lambdas)
+           fit <- cv_fit$glmnet.fit
+           lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+           residuals = kpop_svyd_conv$variables$outcome - predict(cv_fit$glmnet.fit, 
+                                                                  s = lambda_pass, 
+                                                                            newx = x)
+           res_kpop_conv = data.frame(min = min(residuals), 
+                                      perc_25 = quantile(residuals, .25), 
+                                      mean = mean(residuals),
+                                      perc_75 = quantile(residuals, .75),
+                                      var = var(residuals))
+           kpop_conv_se <- tryCatch(calc_SEs(Y = kpop_svyd_conv$variables$outcome,
+                                        residuals = residuals,
+                                        pop_size = nrow(cces),
+                                        sample_size = sum(sample),
+                                        weights = weights(kpop_svyd_conv)), error = function(e) NA)
+           if(length(kpop_conv_se) == 1) {
+               kpop_conv_se <- data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+           }
+           names(kpop_conv_se) = tryCatch(paste0("kpop_conv_", names(kpop_conv_se)), error = function(e) NA)
+          #KRLS SEs are exactly the same for coverged
+           rm(kbal_est_conv, residuals, x, cv_fit) 
+         }
+         
+         
+         ####### MF #######
+         cat(paste("nsim:", nsim, "MEANFIRST", "\n"))
+         #### BROKEN HERE
+         kbal_mf_est <- kbal(K.svd = svdK,
+                             cat_data = T,
+                             allx=kbal_data,
+                             sampled = kbal_data_sampled,
+                             ebal.tol = tolerance,
+                             ebal.maxit = maxit,
+                             minnumdims = min_num_dims,
+                             maxnumdims = max_num_dims,
+                             incrementby = increment,
+                             meanfirst = TRUE,
+                             sampledinpop = FALSE)
+         
+         kpop_mf_svyd <- svydesign(~1, data = survey_sim, 
+                                   weights = kbal_mf_est$w[kbal_data_sampled ==1])
+         
+         kpop_mf <- est_mean("outcome", kpop_mf_svyd)
+         
+         mfnumdims = kbal_mf_est$numdims
+         mf_appended_dims = kbal_mf_est$meanfirst_dims
+         if(is.null(mf_appended_dims)) {mf_appended_dims = c(NA)}
+         biasbound_r_mf = kbal_mf_est$biasbound_ratio
+         biasbound_mf = kbal_mf_est$biasbound_opt
+         
+         if(is.null(mfnumdims)) {
+             mfnumdims = c(NA) 
+             kpop_mf_se = data.frame(SE_fixed = NA, 
+                                     SE_quasi = NA, 
+                                     SE_linear = NA, 
+                                     SE_chad = NA)
+         } else {
+ 
+             V <-  data.frame(kbal_dims = kbal_mf_est$svdK$v[, c(1:kbal_mf_est$numdims)])
+             #binding mf cols for sample units to V
+             X <- as.matrix(cbind(kbal_mf_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+ 
+             cv_fit <- cv.glmnet(X, kpop_mf_svyd$variables$outcome, alpha = 0,
+                                 lambda = lambdas, 
+                                 penalty.factor = c(rep(0, kbal_mf_est$meanfirst_dims), 
+                                                    rep(1, kbal_mf_est$numdims)))
+             lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+             residuals = kpop_mf_svyd$variables$outcome - predict(cv_fit$glmnet.fit, 
+                                                                  s = lambda_pass, 
+                                                                  newx = X)
+             res_kpop_mf = data.frame(min = min(residuals), 
+                                      perc_25 = quantile(residuals, .25), 
+                                      mean = mean(residuals),
+                                      perc_75 = quantile(residuals, .75),
+                                      var = var(residuals))
+             
+             kpop_mf_se <- tryCatch(calc_SEs(Y = kpop_mf_svyd$variables$outcome,
+                                               residuals = residuals,
+                                               pop_size = nrow(cces),
+                                               sample_size = sum(sample),
+                                               weights = weights(kpop_mf_svyd)), 
+                                    error = function(e) NA)
+             if(length(kpop_mf_se) == 1) {
+                 kpop_mf_se <- data.frame(SE_fixed = NA, 
+                                       SE_quasi = NA, 
+                                       SE_linear = NA, 
+                                       SE_chad = NA)
+             }
+             names(kpop_mf_se) = tryCatch(paste0("kpop_mf_", names(kpop_mf_se)),
+                                          error = function(e) NA)
+             
+         }
+ 
+         rm(kbal_mf_est, residuals,X, V, cv_fit)
+         
+         if(kpop_constraints) {
+             #########demos constraint method:
+             cat(paste("nsim:", nsim, "CONSTR", "\n"))
+             kbal_demos_est <- kbal(K.svd = svdK,
+                                    allx=kbal_data,
+                                    #cat_data = TRUE,
+                                    sampled = kbal_data_sampled,
+                                    ebal.tol = tolerance,
+                                    ebal.maxit = maxit,
+                                    minnumdims = min_num_dims,
+                                    maxnumdims = max_num_dims,
+                                    scale_data = FALSE,
+                                    drop_MC = FALSE,
+                                    incrementby = increment,
+                                    #scaling these
+                                    constraint = rake_demos_constraint,
+                                    meanfirst = FALSE,
+                                    sampledinpop = FALSE)
+             kpop_demos_svyd <- svydesign(~1, data = survey_sim, 
+                                          weights = kbal_demos_est$w[kbal_data_sampled ==1])
+             
+             kpop_demos <- est_mean("outcome", kpop_demos_svyd)
+             
+             numdims_demos = kbal_demos_est$numdims
+             if(is.null(numdims_demos)) {
+                 numdims_demos = c(NA) 
+                 kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                             SE_quasi = NA, 
+                                             SE_linear = NA, 
+                                             SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_demos_est$svdK$v[, c(1:kbal_demos_est$numdims)])
+                 X <- as.matrix(cbind(kbal_demos_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_demos_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                     penalty.factor = c(rep(0, ncol(kbal_demos_est$appended_constraint_cols)), rep(1, kbal_demos_est$numdims)))
+                 
+                 lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+                 residuals =  kpop_demos_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                          s = lambda_pass, 
+                                                                          newx = X)
+                 res_kpop_demos = data.frame(min = min(residuals), 
+                                             perc_25 = quantile(residuals, .25), 
+                                             mean = mean(residuals),
+                                             perc_75 = quantile(residuals, .75),
+                                             var = var(residuals))
+                 
+                 kpop_demos_se <- tryCatch(calc_SEs(Y = kpop_demos_svyd$variables$outcome,
+                                                    residuals = residuals,
+                                                    pop_size = nrow(cces),
+                                                    sample_size = sum(sample),
+                                                    weights = weights(kpop_demos_svyd)), 
+                                           error = function(e) NA)
+                 if(length(kpop_demos_se) == 1) {
+                     kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                                 SE_quasi = NA, 
+                                                 SE_linear = NA, 
+                                                 SE_chad = NA)
+                 }
+                 names(kpop_demos_se) = tryCatch(paste0("kpop_demos_", names(kpop_demos_se)),
+                                                 error = function(e) NA)
+             }
+             biasbound_r_demos = kbal_demos_est$biasbound_ratio
+             biasbound_demos = kbal_demos_est$biasbound_opt
+             
+             rm(kbal_demos_est, residuals, X, V, cv_fit)
+             
+             
+             #########demos + educ constraint method:
+             cat(paste("nsim:", nsim, "CONSTR", "\n"))
+             kbal_demos_wedu_est <- kbal(K.svd = svdK,
+                                         allx=kbal_data,
+                                         cat_data = TRUE,
+                                         sampled = kbal_data_sampled,
+                                         ebal.tol = tolerance,
+                                         ebal.maxit = maxit,
+                                         minnumdims = min_num_dims,
+                                         maxnumdims = max_num_dims,
+                                         scale_data = FALSE,
+                                         drop_MC = FALSE,
+                                         incrementby = increment,
+                                         #scaling these
+                                         constraint = rake_demos_wedu_constraint,
+                                         meanfirst = FALSE,
+                                         sampledinpop = FALSE)
+             kpop_demos_wedu_svyd <- svydesign(~1, data = survey_sim, 
+                                               weights = kbal_demos_wedu_est$w[kbal_data_sampled ==1])
+             
+             kpop_demos_wedu <- est_mean("outcome", kpop_demos_wedu_svyd)
+             
+             numdims_demos_wedu = kbal_demos_wedu_est$numdims
+             if(is.null(numdims_demos_wedu)) {
+                 numdims_demos_wedu = c(NA)
+                 kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                  SE_quasi = NA, 
+                                                  SE_linear = NA, 
+                                                  SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_demos_wedu_est$svdK$v[, c(1:kbal_demos_wedu_est$numdims)])
+                 X <- as.matrix(cbind(kbal_demos_wedu_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_demos_wedu_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                     penalty.factor = c(rep(0, ncol(kbal_demos_wedu_est$appended_constraint_cols)),
+                                                        rep(1, kbal_demos_wedu_est$numdims)))
+                 
+                 lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+                 residuals =  kpop_demos_wedu_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                               s = lambda_pass, 
+                                                                               newx = X)
+                 res_kpop_demos_wedu = data.frame(min = min(residuals), 
+                                                  perc_25 = quantile(residuals, .25), 
+                                                  mean = mean(residuals),
+                                                  perc_75 = quantile(residuals, .75),
+                                                  var = var(residuals))
+                 kpop_demos_wedu_se <- tryCatch(calc_SEs(Y = kpop_demos_wedu_svyd$variables$outcome,
+                                                         residuals = residuals,
+                                                         pop_size = nrow(cces),
+                                                         sample_size = sum(sample),
+                                                         weights = weights(kpop_demos_wedu_svyd)), 
+                                                error = function(e) NA)
+                 if(length(kpop_demos_wedu_se) == 1) {
+                     kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                      SE_quasi = NA, 
+                                                      SE_linear = NA, 
+                                                      SE_chad = NA)
+                 }
+                 names(kpop_demos_wedu_se) = tryCatch(paste0("kpop_demos_wedu_", names(kpop_demos_wedu_se)),
+                                                      error = function(e) NA)
+                 
+             }
+             biasbound_r_demos_wedu = kbal_demos_wedu_est$biasbound_ratio
+             biasbound_demos_wedu = kbal_demos_wedu_est$biasbound_opt
+             
+             rm(kbal_demos_wedu_est, residuals, X, V, cv_fit)
+             
+             
+             #########all constraint method:
+             cat(paste("nsim:", nsim, "CONSTR", "\n"))
+             kbal_all_est <- kbal(K.svd = svdK,
+                                  allx=kbal_data,
+                                  #cat_data = TRUE,
+                                  sampled = kbal_data_sampled,
+                                  ebal.tol = tolerance,
+                                  ebal.maxit = maxit,
+                                  minnumdims = min_num_dims,
+                                  maxnumdims = max_num_dims,
+                                  scale_data = FALSE,
+                                  drop_MC = FALSE,
+                                  incrementby = increment,
+                                  #scaling these
+                                  constraint = rake_all_constraint,
+                                  meanfirst = FALSE,
+                                  sampledinpop = FALSE)
+             kpop_all_svyd <- svydesign(~1, data = survey_sim, 
+                                        weights = kbal_all_est$w[kbal_data_sampled ==1])
+             
+             kpop_all <- est_mean("outcome", kpop_all_svyd)
+             
+             numdims_all = kbal_all_est$numdims
+             if(is.null(numdims_all)) {
+                 numdims_all = c(NA)
+                 numdims_all_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             } else {
+                 V <-  data.frame(kbal_dims = kbal_all_est$svdK$v[, c(1:kbal_all_est$numdims)])
+                 X <- as.matrix(cbind(kbal_all_est$appended_constraint_cols[kbal_data_sampled==1, ], V))
+                 
+                 cv_fit <- cv.glmnet(X, kpop_all_svyd$variables$outcome, alpha = 0, lambda = lambdas,
+                                     penalty.factor = c(rep(0, ncol(kbal_all_est$appended_constraint_cols)),
+                                                        rep(1, kbal_all_est$numdims)))
+                 
+                 lambda_pass = if(lambda_min) { cv_fit$lambda.min} else {cv_fit$lambda.1se}
+                 residuals =  kpop_all_svyd$variables$outcome - predict(cv_fit$glmnet.fit,
+                                                                        s = lambda_pass, 
+                                                                        newx = X)
+                 res_kpop_all = data.frame(min = min(residuals), 
+                                           perc_25 = quantile(residuals, .25), 
+                                           mean = mean(residuals),
+                                           perc_75 = quantile(residuals, .75),
+                                           var = var(residuals))
+                 kpop_all_se <- tryCatch(calc_SEs(Y = kpop_all_svyd$variables$outcome,
+                                                  residuals = residuals,
+                                                  pop_size = nrow(cces),
+                                                  sample_size = sum(sample),
+                                                  weights = weights(kpop_all_svyd)), 
+                                         error = function(e) NA)
+                 if(length(kpop_demos_wedu_se) == 1) {
+                     kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                                      SE_quasi = NA, 
+                                                      SE_linear = NA, 
+                                                      SE_chad = NA)
+                 }
+                 names(kpop_all_se) = tryCatch(paste0("kpop_all_", names(kpop_all_se)),
+                                               error = function(e) NA)
+                 
+             }
+             
+             biasbound_r_all = kbal_all_est$biasbound_ratio
+             biasbound_all = kbal_all_est$biasbound_opt
+             
+             rm(kbal_all_est, residuals, X,V, cv_fit)
+         } else {
+             kpop_demos <- NA
+             numdims_demos = c(NA) 
+             kpop_demos_se <- data.frame(SE_fixed = NA, 
+                                         SE_quasi = NA, 
+                                         SE_linear = NA, 
+                                         SE_chad = NA)
+             res_kpop_demos = data.frame(min = NA, 
+                                       perc_25 = NA, 
+                                       mean = NA,
+                                       perc_75 = NA,
+                                       var = NA)
+             biasbound_r_demos = NA
+             biasbound_demos = NA
+         
+             kpop_demos_wedu <- NA
+             numdims_demos_wedu = c(NA) 
+             kpop_demos_wedu_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             res_kpop_demos_wedu =  data.frame(min = NA, 
+                                               perc_25 = NA, 
+                                               mean = NA,
+                                               perc_75 = NA,
+                                               var = NA)
+             biasbound_r_demos_wedu = NA
+             biasbound_demos_wedu = NA
+             
+             kpop_all <- NA
+             numdims_all = c(NA) 
+             kpop_all_se <- data.frame(SE_fixed = NA, 
+                                              SE_quasi = NA, 
+                                              SE_linear = NA, 
+                                              SE_chad = NA)
+             res_kpop_all =  data.frame(min = NA, 
+                                               perc_25 = NA, 
+                                               mean = NA,
+                                               perc_75 = NA,
+                                               var = NA)
+             biasbound_r_all = NA
+             biasbound_all = NA
+             
+             #for weights
+             kpop_demos_svyd = survey_design
+             kpop_demos_wedu_svyd = survey_design
+             kpop_all_svyd = survey_design
+         }
+         
+         
+         rm(svdK)
+   
+          ##### return
+         kpop_res = list()
+         b_out = b_kpop
+        # b_reduc = b_kpop_reduc
+         b = b_out
+         kpop_res$sims = data.frame(b_out,
+                               kpop = kpop,
+                               kpop_mf = kpop_mf,
+                               kpop_conv = kpop_conv,
+                               kpop_demos = kpop_demos,
+                               kpop_demos_wedu = kpop_demos_wedu,
+                               kpop_all = kpop_all,
+                               bb = biasbound,
+                               bbr = biasbound_r,
+                               bb_conv = biasbound_conv,
+                               bbr_conv = biasbound_r_conv,
+                               bb_mf = biasbound_mf,
+                               bbr_mf = biasbound_r_mf,
+                               bb_demos = biasbound_demos,
+                               bbr_demos = biasbound_r_demos,
+                               bb_demos_wedu = biasbound_demos_wedu,
+                               bbr_demos_wedu = biasbound_r_demos_wedu,
+                               bb_all = biasbound_all,
+                               bbr_all = biasbound_r_all,
+                               numdims,
+                               numdims_conv,
+                               mfnumdims, 
+                               mf_appended_dims, 
+                               numdims_demos,
+                               numdims_demos_wedu,
+                               numdims_all)
+         
+         #Starndard Errors:
+         kpop_res$SEs = data.frame(rake_demos_noeduc_se,
+                              rake_demos_noeduc_se_SVY,
+                              rake_demos_weduc_se,
+                              rake_all_se,
+                              post_stratification_se,
+                              rake_truth_se,
+                              kpop_se,
+                              kpop_conv_se,
+                              kpop_mf_se,
+                              kpop_demos_se,
+                              kpop_demos_wedu_se,
+                              kpop_all_se)
+         
+         
+         #weights
+         kpop_res$weights = list(b = b_out,
+                            kpop_w = weights(kpop_svyd),
+                            #kpop_w_reduc = weights(kpop_svyd_reduc),
+                            kpop_w_conv = weights(kpop_svyd_conv),
+                            kpop_mf_w = weights(kpop_mf_svyd), 
+                            kpop_demos_w = weights(kpop_demos_svyd),
+                            kpop_demos_wedu_w = weights(kpop_demos_wedu_svyd),
+                            kpop_all_w = weights(kpop_all_svyd))
+         
+         #residuals
+         kpop_res$residuals = rbind(b = b_out,
+                            kpop = res_kpop,
+                           # kpop_w_reduc = res_kpop_reduc,
+                            kpop_conv = res_kpop_conv,
+                            kpop_mf = res_kpop_mf,
+                            kpop_demos = res_kpop_demos,
+                            kpop_demos_wedu = res_kpop_demos_wedu,
+                            kpop_all = res_kpop_all,
+                            rake_truth = res_rake_truth,
+                            rake_demos = res_rake_demos_noeduc,
+                            rake_demos_wedu = res_rake_demos_wedu,
+                            rake_all = res_rake_all
+                           )
+         
+         ######## Kpop Margins ########
+         
+         kpop_res$km <- round(cbind(b = b_out/100,
+                               kpop = svymean(margins_formula, kpop_svyd),
+                              # kpop_reduc = svymean(margins_formula, kpop_svyd_reduc),
+                               kpop_conv = svymean(margins_formula, kpop_svyd_conv),
+                               kpop_mf = svymean(margins_formula, kpop_mf_svyd),
+                               kpop_demos = svymean(margins_formula, kpop_demos_svyd),
+                               kpop_demos_wedu = svymean(margins_formula, kpop_demos_wedu_svyd),
+                               kpop_all = svymean(margins_formula, kpop_all_svyd)) * 100,
+                         4)
+         
+         rm(kpop_svyd, 
+            #kpop_svyd_reduc,
+            kpop_mf_svyd, kpop_svyd_conv, kpop_demos_svyd,
+            kpop_demos_wedu_svyd, kpop_all_svyd)
+       
+     }
+     
+     ############################################ OUTPUT
+     out = list()
+     if(eval_kpop) {
+       out$sims = cbind(nsim,
+                        n,
+                        unweighted,
+                        rake_demos_noeduc,
+                        rake_demos_weduc,
+                        rake_all,
+                        post_stratification,
+                        rake_truth,
+                        ht_truth, 
+                        hayek_truth,
+                        kpop_res$sims)
+       
+       out$SEs = kpop_res$SEs
+       out$weights = kpop_res$weights
+       out$residuals = kpop_res$residuals
+       out$dropped_cells = c(dropped_cells = dropped_cells)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      #chaningn temporarily to the fuller view from check_nums
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+       
+       margin <- round(cbind(sample = svymean(margins_formula, survey_design),
+                             cces =  svymean(margins_formula, cces_svy),
+                             rake_demos_noeduc = svymean(margins_formula,
+                                                         rake_demos_noeduc_svyd),
+                             rake_demos_weduc = svymean(margins_formula,
+                                                        rake_demos_weduc_svyd),
+                             rake_all = svymean(margins_formula,
+                                                rake_all_svyd),
+                             post_stratification = svymean(margins_formula,
+                                                           post_stratification_svyd),
+                             rake_truth = truth_margins) * 100, 5)
+       
+       #these are just means so let's not multiply by 100
+       margin["recode_age",] <- margin["recode_age",]/100 
+       margin["recode_agesq",] <- margin["recode_agesq",]/100
+       margin["recode_agecubed",] <- margin["recode_agecubed",]/100
+       
+       margin = cbind(margin,
+                      kpop_res$km)
+       
+       margin <- margin[,grepl("km.kpop_res", colnames(margin))|
+                          !grepl("km.", colnames(margin)) ]
+       
+     } else {
+         margin <- round(cbind(sample = svymean(margins_formula, survey_design),
+                               cces =  svymean(margins_formula, cces_svy),
+                               rake_demos_noeduc = svymean(margins_formula,
+                                                           rake_demos_noeduc_svyd),
+                               rake_demos_weduc = svymean(margins_formula,
+                                                          rake_demos_weduc_svyd),
+                               rake_all = svymean(margins_formula,
+                                                  rake_all_svyd),
+                               post_stratification = svymean(margins_formula,
+                                                             post_stratification_svyd),
+                               rake_truth = truth_margins) * 100, 5)
+ 
+         #these are just means so let's not multiply by 100
+         margin["recode_age",] <- margin["recode_age",]/100
+         margin["recode_agesq",] <- margin["recode_agesq",]/100
+         margin["recode_agecubed",] <- margin["recode_agecubed",]/100
+     
+       out$sims = data.frame(nsim,
+                             n,
+                             unweighted,
+                             rake_demos_noeduc,
+                             rake_demos_weduc,
+                             rake_all,
+                             post_stratification,
+                             rake_truth,
+                             ht_truth, 
+                             hayek_truth)
+       
+       out$SEs = data.frame(rake_demos_noeduc_se,
+                            rake_demos_weduc_se,
+                            rake_all_se,
+                            post_stratification_se,
+                            rake_truth_se)
+       out$residuals = rbind(b = NULL,
+                             rake_truth = res_rake_truth,
+                             rake_demos = res_rake_demos_noeduc,
+                             rake_demos_wedu = res_rake_demos_wedu ,
+                             rake_all = res_rake_all
+       )
+       
+       out$dropped_cells = c(dropped_cells = dropped_cells)
+       
+       out$sample = c(drop_ps = count, 
+                      bad_sample = bad_sample, 
+                      check = check_nums)
+       out$samp_counts = check_2$counts
+       
+       out$SEs = data.frame(rake_demos_noeduc_se,
+                  rake_demos_weduc_se,
+                  rake_demos_noeduc_se_SVY,
+                  rake_all_se,
+                  post_stratification_se,
+                  rake_truth_se)
+     } 
+     
+     out$margins = margin
+     
+     })
+     return(out)
+     
+   }, mc.cores = detectCores() - cores_saved) 
+   
+ })
=====================  SIM: 3 ===================== 
=====================  SIM: 1 ===================== 
=====================  SIM: 2 ===================== 
=====================  SIM: 4 ===================== 
=====================  SIM: 6 ===================== 
=====================  SIM: 5 ===================== 
=====================  SIM: 7 ===================== 
0.709 % cces original strata missing from sample,   and 16288 / 44932 units
0.711 % cces original strata missing from sample,   and 13024 / 44932 units
0.738 % cces original strata missing from sample,   and 0.69718077 / 44932 units
 % cces original strata missing from sample,   and 13278 / 44932 units
0.736 % cces original strata missing from sample,   and 15952 / 44932 units
0.715 % cces original strata missing from sample,   and 14669 / 44932 units
0.72 % cces original strata missing from sample,   and 14682 / 44932 units
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
Joining with `by = join_by(strata)`
nsim: 2 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 7 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 4 DEFAULT 
nsim: 1 DEFAULT 
nsim: 6 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

nsim: 5 DEFAULT 
nsim: 3 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: Searching for b value which maximizes the variance in K: 2.557 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.538 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.527 selected 
Building kernel matrix
2.514 selected 
Building kernel matrix
2.547 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
Running full SVD on kernel matrix 
2.547 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.533 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05931 and the L1 discrepancy is 0.119 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05329  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02281  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01538  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01425  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01306  
Without balancing, biasbound (norm=1) is 0.05632 and the L1 discrepancy is 0.114 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05423  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0235  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01165  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01889  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01817  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00905  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01627  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01742  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00942  
Without balancing, biasbound (norm=1) is 0.05307 and the L1 discrepancy is 0.099 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05242  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0172  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02499  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01456  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0189  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01853  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01643  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01532  
Without balancing, biasbound (norm=1) is 0.05402 and the L1 discrepancy is 0.109 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01548  
nsim: 2 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05228  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0311  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
Without balancing, biasbound (norm=1) is 0.05935 and the L1 discrepancy is 0.112 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02379  
Without balancing, biasbound (norm=1) is 0.05653 and the L1 discrepancy is 0.112 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05725  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05519  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01634  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0134  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02668  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02111  
Without balancing, biasbound (norm=1) is 0.06156 and the L1 discrepancy is 0.116 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01997  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01573  
nsim: 7 CONV 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01514  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05245  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01253  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01841  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01628  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02242  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01499  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01561  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01261  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01549  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01577  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01503  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01165  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01084  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01301  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01562  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01411  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01067  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01249  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01482  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01347  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01127  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00873  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01238  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01307  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0128  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01232  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01302  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01057  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00817  
Without balancing, biasbound (norm=1) is 0.05931 and the L1 discrepancy is 0.119 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01175  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0273  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0172  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00844  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01302  
With user-specified 41 dimensions, biasbound (norm=1) of  0.00905  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0126  
nsim: 2 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.05632 and the L1 discrepancy is 0.114 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00982  
nsim: 6 CONV 
nsim: 3 CONV 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01981  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02114  
With user-specified 26 dimensions, biasbound (norm=1) of  0.01627  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 7 MEANFIRST 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
nsim: 4 CONV 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01554  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
nsim: 1 CONV 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
nsim: 5 CONV 
Without balancing, biasbound (norm=1) is 0.05307 and the L1 discrepancy is 0.099 
Without balancing, biasbound (norm=1) is 0.05935 and the L1 discrepancy is 0.112 
Without balancing, biasbound (norm=1) is 0.05931 and the L1 discrepancy is 0.119 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01529  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01262  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01165  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01301  
With user-specified 36 dimensions, biasbound (norm=1) of  0.01258  
nsim: 6 MEANFIRST 
nsim: 3 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.05402 and the L1 discrepancy is 0.109 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01305  
Without balancing, biasbound (norm=1) is 0.05632 and the L1 discrepancy is 0.114 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01047  
Without balancing, biasbound (norm=1) is 0.06156 and the L1 discrepancy is 0.116 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01756  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01238  
nsim: 4 MEANFIRST 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01697  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.05653 and the L1 discrepancy is 0.112 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01628  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01175  
nsim: 1 MEANFIRST 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01597  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00921  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With user-specified 46 dimensions, biasbound (norm=1) of  0.00817  
nsim: 5 MEANFIRST 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01568  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00915  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01422  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01123  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01923  
Re-running at optimal choice of numdims, 21 
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01785  
Re-running at optimal choice of numdims, 36 
Used 12 dimensions of "allx" for mean balancing, and an additional 36 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.05307 and the L1 discrepancy is 0.099 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01545  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0145  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01493  
Without balancing, biasbound (norm=1) is 0.05935 and the L1 discrepancy is 0.112 
=====================  SIM: 14 ===================== 
0.713 % cces original strata missing from sample,   and 15082 / 44932 units
Joining with `by = join_by(strata)`
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01367  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01681  
Without balancing, biasbound (norm=1) is 0.05402 and the L1 discrepancy is 0.109 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01157  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01546  
nsim: 14 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01402  
Searching for b value which maximizes the variance in K: With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01305  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01108  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01348  
=====================  SIM: 9 ===================== 
0.73 % cces original strata missing from sample,   and 17474 / 44932 units
Joining with `by = join_by(strata)`
Without balancing, biasbound (norm=1) is 0.06156 and the L1 discrepancy is 0.116 
Without balancing, biasbound (norm=1) is 0.05653 and the L1 discrepancy is 0.112 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01412  
2.558 selected 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01302  
Building kernel matrix
Running full SVD on kernel matrix 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01413  
nsim: 9 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01016  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01365  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01363  
Searching for b value which maximizes the variance in K: With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01351  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01186  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01187  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0132  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01045  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01396  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00956  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00964  
2.53 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01149  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0143  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00917  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00838  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01989  
Re-running at optimal choice of numdims, 31 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01906  
Re-running at optimal choice of numdims, 21 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01214  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00784  
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01493  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01317  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00723  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01837  
Re-running at optimal choice of numdims, 21 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02124  
Re-running at optimal choice of numdims, 26 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00907  
Re-running at optimal choice of numdims, 36 
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
Used 13 dimensions of "allx" for mean balancing, and an additional 36 dimensions of "K" from kernel balancing.
=====================  SIM: 10 ===================== 
=====================  SIM: 13 ===================== 
0.709 % cces original strata missing from sample,   and 14477 / 44932 units
Joining with `by = join_by(strata)`
0.729 % cces original strata missing from sample,   and 15577 / 44932 units
Joining with `by = join_by(strata)`
nsim: 10 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: nsim: 13 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.533 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.52 selected 
Building kernel matrix
=====================  SIM: 8 ===================== 
Running full SVD on kernel matrix 
0.693 % cces original strata missing from sample,   and 12835 / 44932 units
Joining with `by = join_by(strata)`
=====================  SIM: 11 ===================== 
0.699 % cces original strata missing from sample,   and 13899 / 44932 units
Joining with `by = join_by(strata)`
=====================  SIM: 12 ===================== 
0.72 % cces original strata missing from sample,   and 16514 / 44932 units
Joining with `by = join_by(strata)`
nsim: 8 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: nsim: 11 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: nsim: 12 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.541 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.527 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.53 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05739 and the L1 discrepancy is 0.111 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05531  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02368  
Without balancing, biasbound (norm=1) is 0.06104 and the L1 discrepancy is 0.112 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01694  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05653  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02653  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01545  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01642  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01332  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01546  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01142  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01566  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01406  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01063  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01422  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01057  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01362  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01132  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01352  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01078  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01449  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02209  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02131  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 14 CONV 
nsim: 9 CONV 
Without balancing, biasbound (norm=1) is 0.05495 and the L1 discrepancy is 0.104 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05389  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02412  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01495  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01622  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01743  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0164  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
Without balancing, biasbound (norm=1) is 0.05746 and the L1 discrepancy is 0.106 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05512  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02313  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01087  
Without balancing, biasbound (norm=1) is 0.06104 and the L1 discrepancy is 0.112 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01602  
Without balancing, biasbound (norm=1) is 0.057 and the L1 discrepancy is 0.11 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01506  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05125  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01049  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.023  
Without balancing, biasbound (norm=1) is 0.05739 and the L1 discrepancy is 0.111 
Without balancing, biasbound (norm=1) is 0.05454 and the L1 discrepancy is 0.106 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01715  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01352  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01372  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05389  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01613  
nsim: 9 MEANFIRST 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02142  
With user-specified 36 dimensions, biasbound (norm=1) of  0.01057  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01256  
Without balancing, biasbound (norm=1) is 0.05424 and the L1 discrepancy is 0.103 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01584  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01515  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01113  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05205  
nsim: 14 MEANFIRST 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01654  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.025  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01099  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01305  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01287  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01911  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01356  
With 16 dimensions of K, ebalance convergence is TRUE With yielding biasbound (norm=1) of 0.02258  
51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01671  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01086  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01223  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02127  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01184  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01321  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01094  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01956  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0103  
nsim: 10 CONV 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01211  
Selected 15 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00847  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01656  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01196  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01272  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01783  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00835  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01556  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02138  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 31 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01535  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00825  
nsim: 13 CONV 
nsim: 12 CONV 
nsim: 11 CONV 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01064  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 51 
Without balancing, biasbound (norm=1) is 0.06104 and the L1 discrepancy is 0.112 
nsim: 8 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01457  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01465  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01724  
Without balancing, biasbound (norm=1) is 0.05495 and the L1 discrepancy is 0.104 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01681  
Without balancing, biasbound (norm=1) is 0.05739 and the L1 discrepancy is 0.111 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01483  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01729  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01298  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01455  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01049  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01685  
nsim: 10 MEANFIRST 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01452  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01752  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01211  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0129  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02712  
Re-running at optimal choice of numdims, 1 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Used 13 dimensions of "allx" for mean balancing, and an additional 1 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01506  
Without balancing, biasbound (norm=1) is 0.057 and the L1 discrepancy is 0.11 
Without balancing, biasbound (norm=1) is 0.05424 and the L1 discrepancy is 0.103 
Without balancing, biasbound (norm=1) is 0.05746 and the L1 discrepancy is 0.106 
With user-specified 11 dimensions, biasbound (norm=1) of  0.01911  
nsim: 12 MEANFIRST 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01539  
Re-running at optimal choice of numdims, 21 
With user-specified 36 dimensions, biasbound (norm=1) of  0.01086  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01211  
nsim: 13 MEANFIRST 
nsim: 11 MEANFIRST 
Used 15 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
Timing stopped at: 68.81 8.778 90.17
Without balancing, biasbound (norm=1) is 0.05454 and the L1 discrepancy is 0.106 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With user-specified 51 dimensions, biasbound (norm=1) of  0.00825  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
nsim: 8 MEANFIRST 
=====================  SIM: 16 ===================== 
0.683 % cces original strata missing from sample,   and 12886 / 44932 units
Joining with `by = join_by(strata)`
nsim: 16 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Selected 13 dimensions of "allx" to use as mean balance constraints. 
2.53 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05495 and the L1 discrepancy is 0.104 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01308  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0127  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01231  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01235  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01315  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01247  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01177  
Without balancing, biasbound (norm=1) is 0.05424 and the L1 discrepancy is 0.103 
Without balancing, biasbound (norm=1) is 0.057 and the L1 discrepancy is 0.11 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01698  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01587  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02248  
Re-running at optimal choice of numdims, 11 
Without balancing, biasbound (norm=1) is 0.05746 and the L1 discrepancy is 0.106 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01492  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01509  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01663  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0145  
Used 12 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01409  
With 11 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01574  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01481  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01213  
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01373  
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01694  
Without balancing, biasbound (norm=1) is 0.05454 and the L1 discrepancy is 0.106 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01167  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01498  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01916  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01514  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0107  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01417  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02106  
Re-running at optimal choice of numdims, 11 
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02277  
Re-running at optimal choice of numdims, 6 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01075  
Used 13 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
Used 12 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00941  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01307  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0092  
=====================  SIM: 17 ===================== 
0.729 % cces original strata missing from sample,   and 16887 / 44932 units
Joining with `by = join_by(strata)`
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01974  
Re-running at optimal choice of numdims, 26 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00927  
nsim: 17 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00808  
2.544 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00782  
=====================  SIM: 18 ===================== 
0.695 % cces original strata missing from sample,   and 14023 / 44932 units
Joining with `by = join_by(strata)`
=====================  SIM: 19 ===================== 
0.704 % cces original strata missing from sample,   and 15012 / 44932 units
Joining with `by = join_by(strata)`
nsim: 18 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00941  
nsim: 19 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.556 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.564 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01486  
Re-running at optimal choice of numdims, 41 
=====================  SIM: 20 ===================== 
0.732 % cces original strata missing from sample,   and 16584 / 44932 units
Joining with `by = join_by(strata)`
nsim: 20 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Used 13 dimensions of "allx" for mean balancing, and an additional 41 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.05279 and the L1 discrepancy is 0.1 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05188  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01925  
2.525 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01618  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01458  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01303  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01218  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01234  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01146  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00937  
=====================  SIM: 15 ===================== 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.00884  
0.715 % cces original strata missing from sample,   and 16272 / 44932 units
Joining with `by = join_by(strata)`
nsim: 15 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01507  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
2.542 selected 
Building kernel matrix
Running full SVD on kernel matrix 
nsim: 16 CONV 
Without balancing, biasbound (norm=1) is 0.05643 and the L1 discrepancy is 0.106 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05688  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02166  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0157  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01638  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01507  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01317  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01284  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01243  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01506  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01775  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
nsim: 17 CONV 
Without balancing, biasbound (norm=1) is 0.05509 and the L1 discrepancy is 0.105 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05557  
Without balancing, biasbound (norm=1) is 0.05565 and the L1 discrepancy is 0.113 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01904  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0536  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01665  
Without balancing, biasbound (norm=1) is 0.05279 and the L1 discrepancy is 0.1 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02144  
Without balancing, biasbound (norm=1) is 0.05839 and the L1 discrepancy is 0.105 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01438  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01705  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05702  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0231  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0141  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01608  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01763  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0124  
With user-specified 41 dimensions, biasbound (norm=1) of  0.00937  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01833  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01556  
nsim: 16 MEANFIRST 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01183  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01372  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01352  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01327  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00982  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01294  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01272  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00996  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0114  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01232  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00874  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01056  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01136  
Without balancing, biasbound (norm=1) is 0.05643 and the L1 discrepancy is 0.106 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01082  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00988  
With user-specified 31 dimensions, biasbound (norm=1) of  0.01284  
nsim: 17 MEANFIRST 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01067  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01101  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01883  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
nsim: 19 CONV 
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02186  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 18 CONV 
Without balancing, biasbound (norm=1) is 0.05967 and the L1 discrepancy is 0.114 
nsim: 20 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05654  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02517  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0175  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01697  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01686  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01534  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01423  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01275  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01266  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01313  
Without balancing, biasbound (norm=1) is 0.05279 and the L1 discrepancy is 0.1 
Without balancing, biasbound (norm=1) is 0.05643 and the L1 discrepancy is 0.106 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01575  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01525  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01455  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01457  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01469  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01668  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01331  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01456  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02503  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
Without balancing, biasbound (norm=1) is 0.05509 and the L1 discrepancy is 0.105 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01251  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01323  
Without balancing, biasbound (norm=1) is 0.05839 and the L1 discrepancy is 0.105 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01158  
nsim: 15 CONV 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0129  
With user-specified 46 dimensions, biasbound (norm=1) of  0.00874  
Without balancing, biasbound (norm=1) is 0.05565 and the L1 discrepancy is 0.113 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01056  
nsim: 19 MEANFIRST 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0097  
nsim: 20 MEANFIRST 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01295  
With user-specified 46 dimensions, biasbound (norm=1) of  0.01039  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01007  
nsim: 18 MEANFIRST 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02612  
Re-running at optimal choice of numdims, 26 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0093  
Selected 14 dimensions of "allx" to use as mean balance constraints. 
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01394  
Re-running at optimal choice of numdims, 26 
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
=====================  SIM: 24 ===================== 
0.697 % cces original strata missing from sample,   and 13903 / 44932 units
Joining with `by = join_by(strata)`
nsim: 24 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.05967 and the L1 discrepancy is 0.114 
Searching for b value which maximizes the variance in K: 2.551 selected 
Building kernel matrix
=====================  SIM: 23 ===================== 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01266  
Running full SVD on kernel matrix 
0.711 % cces original strata missing from sample,   and 16009 / 44932 units
Joining with `by = join_by(strata)`
nsim: 15 MEANFIRST 
nsim: 23 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.05839 and the L1 discrepancy is 0.105 
Searching for b value which maximizes the variance in K: With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01448  
Without balancing, biasbound (norm=1) is 0.05509 and the L1 discrepancy is 0.105 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01339  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01446  
2.521 selected 
Building kernel matrix
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01436  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01412  
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05565 and the L1 discrepancy is 0.113 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01268  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01304  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01535  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01347  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01134  
Selected 15 dimensions of "allx" to use as mean balance constraints. 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01222  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01359  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01212  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01298  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01052  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01135  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01457  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01178  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0103  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01387  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01201  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00939  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02484  
Re-running at optimal choice of numdims, 26 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01231  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01651  
Re-running at optimal choice of numdims, 36 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02074  
Re-running at optimal choice of numdims, 21 
Used 14 dimensions of "allx" for mean balancing, and an additional 36 dimensions of "K" from kernel balancing.
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
=====================  SIM: 27 ===================== 
0.707 % cces original strata missing from sample,   and 16491 / 44932 units
Joining with `by = join_by(strata)`
Without balancing, biasbound (norm=1) is 0.05967 and the L1 discrepancy is 0.114 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01542  
nsim: 27 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: =====================  SIM: 26 ===================== 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01406  
0.715 % cces original strata missing from sample,   and 14899 / 44932 units
Joining with `by = join_by(strata)`
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0151  
=====================  SIM: 25 ===================== 
0.73 % cces original strata missing from sample,   and 17877 / 44932 units
Joining with `by = join_by(strata)`
nsim: 26 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

2.53 selected 
Building kernel matrix
Searching for b value which maximizes the variance in K: Running full SVD on kernel matrix 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01374  
nsim: 25 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01388  
2.531 selected 
Building kernel matrix
Running full SVD on kernel matrix 
2.527 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01398  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02007  
Re-running at optimal choice of numdims, 16 
Used 15 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.05323 and the L1 discrepancy is 0.108 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05196  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0214  
Without balancing, biasbound (norm=1) is 0.05695 and the L1 discrepancy is 0.106 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01722  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05449  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01762  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02517  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01764  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01617  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01445  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01436  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01234  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01273  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01201  
=====================  SIM: 22 ===================== 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01155  
0.718 % cces original strata missing from sample,   and 16575 / 44932 units
Joining with `by = join_by(strata)`
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01051  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0113  
nsim: 22 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01039  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01236  
2.525 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02122  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01238  
nsim: 23 CONV 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01979  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 24 CONV 
Without balancing, biasbound (norm=1) is 0.06056 and the L1 discrepancy is 0.119 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05351  
Without balancing, biasbound (norm=1) is 0.0625 and the L1 discrepancy is 0.11 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02482  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.06124  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01938  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0264  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02106  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01457  
Without balancing, biasbound (norm=1) is 0.05695 and the L1 discrepancy is 0.106 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01466  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02006  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01376  
With user-specified 36 dimensions, biasbound (norm=1) of  0.0113  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01908  
nsim: 23 MEANFIRST 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00994  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02049  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
Without balancing, biasbound (norm=1) is 0.05323 and the L1 discrepancy is 0.108 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02602  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
Without balancing, biasbound (norm=1) is 0.05719 and the L1 discrepancy is 0.113 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01063  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05314  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02437  
nsim: 25 CONV 
Selected 14 dimensions of "allx" to use as mean balance constraints. 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01608  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01039  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01027  
nsim: 24 MEANFIRST 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01715  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01606  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01005  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0146  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01266  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00967  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01219  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01522  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 51 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01017  
Without balancing, biasbound (norm=1) is 0.0582 and the L1 discrepancy is 0.113 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05088  
nsim: 27 CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02497  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01688  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01566  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01656  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01398  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01277  
Withnsim: 26 CONV 
 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0131  
Without balancing, biasbound (norm=1) is 0.06056 and the L1 discrepancy is 0.119 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01389  
With user-specified 11 dimensions, biasbound (norm=1) of  0.01938  
Without balancing, biasbound (norm=1) is 0.05695 and the L1 discrepancy is 0.106 
nsim: 25 MEANFIRST 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01455  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01179  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01263  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01288  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01276  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02213  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0121  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01281  
nsim: 22 CONV 
Without balancing, biasbound (norm=1) is 0.05323 and the L1 discrepancy is 0.108 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01533  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01466  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01232  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01358  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01624  
Re-running at optimal choice of numdims, 16 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01304  
Without balancing, biasbound (norm=1) is 0.0625 and the L1 discrepancy is 0.11 
Used 14 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01169  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0111  
With user-specified 51 dimensions, biasbound (norm=1) of  0.00967  
nsim: 27 MEANFIRST 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00993  
Without balancing, biasbound (norm=1) is 0.05719 and the L1 discrepancy is 0.113 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01573  
Re-running at optimal choice of numdims, 31 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With user-specified 46 dimensions, biasbound (norm=1) of  0.01017  
nsim: 26 MEANFIRST 
Used 13 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.06056 and the L1 discrepancy is 0.119 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01689  
=====================  SIM: 30 ===================== 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01661  
0.707 % cces original strata missing from sample,   and 15474 / 44932 units
Joining with `by = join_by(strata)`
With 11 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01853  
nsim: 30 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.0582 and the L1 discrepancy is 0.113 
Searching for b value which maximizes the variance in K: Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02394  
Re-running at optimal choice of numdims, 6 
Used 12 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
Timing stopped at: 55.77 5.353 65.5
With user-specified 41 dimensions, biasbound (norm=1) of  0.01179  
2.522 selected 
Building kernel matrix
nsim: 22 MEANFIRST 
Running full SVD on kernel matrix 
=====================  SIM: 31 ===================== 
0.704 % cces original strata missing from sample,   and 15645 / 44932 units
Joining with `by = join_by(strata)`
Selected 12 dimensions of "allx" to use as mean balance constraints. 
nsim: 31 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.554 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.0625 and the L1 discrepancy is 0.11 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01286  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01189  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01228  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01123  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01101  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00969  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01091  
Without balancing, biasbound (norm=1) is 0.05719 and the L1 discrepancy is 0.113 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01454  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01391  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01496  
Re-running at optimal choice of numdims, 26 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01337  
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01411  
Without balancing, biasbound (norm=1) is 0.0582 and the L1 discrepancy is 0.113 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01369  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01603  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01555  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01122  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01633  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01524  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01192  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0134  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01577  
Re-running at optimal choice of numdims, 26 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01345  
Used 13 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01379  
=====================  SIM: 34 ===================== 
0.711 % cces original strata missing from sample,   and 14650 / 44932 units
Joining with `by = join_by(strata)`
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02989  
Re-running at optimal choice of numdims, 21 
nsim: 34 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
2.535 selected 
Building kernel matrix
Running full SVD on kernel matrix 
=====================  SIM: 33 ===================== 
0.716 % cces original strata missing from sample,   and 14529 / 44932 units
Joining with `by = join_by(strata)`
nsim: 33 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Without balancing, biasbound (norm=1) is 0.05421 and the L1 discrepancy is 0.097 
Searching for b value which maximizes the variance in K: With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05284  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02421  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01621  
=====================  SIM: 29 ===================== 
2.532 selected 
Building kernel matrix
0.727 % cces original strata missing from sample,   and 15824 / 44932 units
Running full SVD on kernel matrix 
Joining with `by = join_by(strata)`
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0158  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01415  
nsim: 29 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01336  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01141  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00968  
2.538 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00953  
Without balancing, biasbound (norm=1) is 0.05916 and the L1 discrepancy is 0.11 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05559  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00949  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02352  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0161  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0148  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00887  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01428  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01355  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01258  
With 56 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0098  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01245  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01078  
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0257  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 51 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01118  
nsim: 30 CONV 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01739  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 31 CONV 
Without balancing, biasbound (norm=1) is 0.05833 and the L1 discrepancy is 0.107 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0555  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02533  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01789  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01721  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01557  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01466  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01341  
Without balancing, biasbound (norm=1) is 0.05421 and the L1 discrepancy is 0.097 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01159  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01157  
With user-specified 51 dimensions, biasbound (norm=1) of  0.00887  
nsim: 30 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.05916 and the L1 discrepancy is 0.11 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01504  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01078  
nsim: 31 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.06283 and the L1 discrepancy is 0.122 
nsim: 34 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05543  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02586  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01665  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01733  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01769  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01532  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01453  
Without balancing, biasbound (norm=1) is 0.05768 and the L1 discrepancy is 0.106 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05619  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0135  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02656  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01585  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01725  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01304  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01666  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01699  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01492  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0144  
nsim: 33 CONV 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01451  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01369  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01804  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
Without balancing, biasbound (norm=1) is 0.05833 and the L1 discrepancy is 0.107 
nsim: 29 CONV 
Without balancing, biasbound (norm=1) is 0.05421 and the L1 discrepancy is 0.097 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01157  
nsim: 34 MEANFIRST 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01495  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0149  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01188  
Without balancing, biasbound (norm=1) is 0.05916 and the L1 discrepancy is 0.11 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0121  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01507  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01115  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0148  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01438  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01216  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01046  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01147  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00958  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01183  
Without balancing, biasbound (norm=1) is 0.06283 and the L1 discrepancy is 0.122 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01203  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01738  
Re-running at optimal choice of numdims, 36 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01304  
nsim: 33 MEANFIRST 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01935  
Re-running at optimal choice of numdims, 21 
Used 12 dimensions of "allx" for mean balancing, and an additional 36 dimensions of "K" from kernel balancing.
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.05768 and the L1 discrepancy is 0.106 
With user-specified 31 dimensions, biasbound (norm=1) of  0.0144  
nsim: 29 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.05833 and the L1 discrepancy is 0.107 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01424  
=====================  SIM: 37 ===================== 
0.709 % cces original strata missing from sample,   and 13772 / 44932 units
Joining with `by = join_by(strata)`
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01318  
=====================  SIM: 38 ===================== 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01266  
0.713 % cces original strata missing from sample,   and 13851 / 44932 units
Joining with `by = join_by(strata)`
nsim: 37 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01217  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
nsim: 38 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01132  
2.527 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01228  
2.519 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01435  
Re-running at optimal choice of numdims, 21 
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.06283 and the L1 discrepancy is 0.122 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01543  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01534  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01481  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01359  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01286  
=====================  SIM: 41 ===================== 
0.72 % cces original strata missing from sample,   and 15432 / 44932 units
Joining with `by = join_by(strata)`
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01372  
nsim: 41 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01691  
Re-running at optimal choice of numdims, 21 
Used 12 dimensions of "allx" for mean balancing, and an additional 21 dimensions of "K" from kernel balancing.
2.541 selected 
Building kernel matrix
Without balancing, biasbound (norm=1) is 0.05768 and the L1 discrepancy is 0.106 
Running full SVD on kernel matrix 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01482  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01405  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01763  
Re-running at optimal choice of numdims, 6 
Used 13 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
=====================  SIM: 40 ===================== 
0.704 % cces original strata missing from sample,   and 15022 / 44932 units
Joining with `by = join_by(strata)`
nsim: 40 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.555 selected 
Building kernel matrix
Running full SVD on kernel matrix 
=====================  SIM: 36 ===================== 
0.72 % cces original strata missing from sample,   and 14712 / 44932 units
Joining with `by = join_by(strata)`
nsim: 36 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.529 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05902 and the L1 discrepancy is 0.114 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05282  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02061  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01492  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01429  
Without balancing, biasbound (norm=1) is 0.05195 and the L1 discrepancy is 0.098 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05459  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01289  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02361  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01891  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01167  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01619  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01135  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01525  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01133  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01406  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.011  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01365  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01175  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01143  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0102  
With 51 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01168  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01074  
With 56 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01111  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01295  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 61 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.012  
nsim: 38 CONV 
With 66 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01681  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
Without balancing, biasbound (norm=1) is 0.05521 and the L1 discrepancy is 0.108 
nsim: 37 CONV 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05602  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02029  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01709  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0163  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01453  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01204  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01044  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01021  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0095  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00992  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01256  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
Without balancing, biasbound (norm=1) is 0.05363 and the L1 discrepancy is 0.109 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05265  
nsim: 41 CONV 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02416  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01722  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01615  
Without balancing, biasbound (norm=1) is 0.0596 and the L1 discrepancy is 0.115 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05454  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01648  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02379  
Without balancing, biasbound (norm=1) is 0.05195 and the L1 discrepancy is 0.098 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01632  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01361  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01606  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01291  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01455  
With user-specified 41 dimensions, biasbound (norm=1) of  0.0102  
Without balancing, biasbound (norm=1) is 0.05902 and the L1 discrepancy is 0.114 
nsim: 38 MEANFIRST 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01213  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01358  
With user-specified 41 dimensions, biasbound (norm=1) of  0.011  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01444  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01187  
nsim: 37 MEANFIRST 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01283  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01037  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01318  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02363  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 46 
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01916  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
nsim: 36 CONV 
nsim: 40 CONV 
Without balancing, biasbound (norm=1) is 0.05521 and the L1 discrepancy is 0.108 
With user-specified 41 dimensions, biasbound (norm=1) of  0.0095  
nsim: 41 MEANFIRST 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.05195 and the L1 discrepancy is 0.098 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01523  
Without balancing, biasbound (norm=1) is 0.05902 and the L1 discrepancy is 0.114 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01461  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01335  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01418  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01282  
Without balancing, biasbound (norm=1) is 0.0596 and the L1 discrepancy is 0.115 
Without balancing, biasbound (norm=1) is 0.05363 and the L1 discrepancy is 0.109 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01336  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01241  
With user-specified 36 dimensions, biasbound (norm=1) of  0.01283  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0109  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01297  
nsim: 36 MEANFIRST 
With user-specified 46 dimensions, biasbound (norm=1) of  0.01037  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01045  
nsim: 40 MEANFIRST 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01223  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01004  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01157  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00945  
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01146  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.00973  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01537  
Re-running at optimal choice of numdims, 36 
Without balancing, biasbound (norm=1) is 0.05521 and the L1 discrepancy is 0.108 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01188  
Re-running at optimal choice of numdims, 31 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01478  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01384  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01317  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
Used 13 dimensions of "allx" for mean balancing, and an additional 36 dimensions of "K" from kernel balancing.
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01255  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01196  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01065  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01027  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01053  
=====================  SIM: 44 ===================== 
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02087  
Re-running at optimal choice of numdims, 31 
0.741 % cces original strata missing from sample,   and 17549 / 44932 units
Joining with `by = join_by(strata)`
=====================  SIM: 45 ===================== 
Without balancing, biasbound (norm=1) is 0.0596 and the L1 discrepancy is 0.115 
0.723 % cces original strata missing from sample,   and 15359 / 44932 units
Joining with `by = join_by(strata)`
nsim: 44 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01395  
Used 12 dimensions of "allx" for mean balancing, and an additional 31 dimensions of "K" from kernel balancing.
nsim: 45 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0133  
Without balancing, biasbound (norm=1) is 0.05363 and the L1 discrepancy is 0.109 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01516  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01591  
2.53 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01407  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01558  
2.53 selected 
Building kernel matrix
Running full SVD on kernel matrix 
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01347  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01457  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01269  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01453  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01208  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01102  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01335  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01007  
=====================  SIM: 48 ===================== 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02208  
Re-running at optimal choice of numdims, 6 
0.707 % cces original strata missing from sample,   and 13039 / 44932 units
Joining with `by = join_by(strata)`
Used 12 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01097  
nsim: 48 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02351  
Re-running at optimal choice of numdims, 26 
2.545 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Used 12 dimensions of "allx" for mean balancing, and an additional 26 dimensions of "K" from kernel balancing.
=====================  SIM: 43 ===================== 
0.718 % cces original strata missing from sample,   and 15585 / 44932 units
Joining with `by = join_by(strata)`
nsim: 43 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.558 selected 
Building kernel matrix
Running full SVD on kernel matrix 
=====================  SIM: 47 ===================== 
0.709 % cces original strata missing from sample,   and 15254 / 44932 units
Joining with `by = join_by(strata)`
nsim: 47 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.549 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05825 and the L1 discrepancy is 0.104 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05399  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02494  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01745  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01678  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01472  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01272  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01226  
Without balancing, biasbound (norm=1) is 0.05622 and the L1 discrepancy is 0.104 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05557  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02381  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0109  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01959  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01828  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01572  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 36 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01806  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01558  
nsim: 44 CONV 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0161  
Without balancing, biasbound (norm=1) is 0.05882 and the L1 discrepancy is 0.118 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01523  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05528  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02178  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01565  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01488  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01303  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01339  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01073  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0236  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01088  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01089  
nsim: 45 CONV 
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01053  
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01143  
Without balancing, biasbound (norm=1) is 0.05873 and the L1 discrepancy is 0.115 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05515  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02758  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01843  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02106  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01635  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01666  
nsim: 48 CONV 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01508  
Without balancing, biasbound (norm=1) is 0.05917 and the L1 discrepancy is 0.115 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0526  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.02293  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01488  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01423  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01588  
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01441  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01488  
Without balancing, biasbound (norm=1) is 0.05825 and the L1 discrepancy is 0.104 
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01271  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01252  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01222  
With user-specified 31 dimensions, biasbound (norm=1) of  0.01226  
nsim: 44 MEANFIRST 
With 36 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01221  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01268  
With 41 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01221  
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01856  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
With 46 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01288  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
nsim: 43 CONV 
With 51 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02919  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 41 
nsim: 47 CONV 
Without balancing, biasbound (norm=1) is 0.05622 and the L1 discrepancy is 0.104 
With user-specified 36 dimensions, biasbound (norm=1) of  0.01523  
nsim: 45 MEANFIRST 
Without balancing, biasbound (norm=1) is 0.05882 and the L1 discrepancy is 0.118 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01053  
nsim: 48 MEANFIRST 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
Selected 14 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.05825 and the L1 discrepancy is 0.104 
Without balancing, biasbound (norm=1) is 0.05873 and the L1 discrepancy is 0.115 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01576  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01397  
Without balancing, biasbound (norm=1) is 0.05917 and the L1 discrepancy is 0.115 
With user-specified 41 dimensions, biasbound (norm=1) of  0.01252  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01462  
nsim: 43 MEANFIRST 
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01277  
With user-specified 41 dimensions, biasbound (norm=1) of  0.01221  
nsim: 47 MEANFIRST 
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01333  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01264  
Selected 13 dimensions of "allx" to use as mean balance constraints. 
Selected 12 dimensions of "allx" to use as mean balance constraints. 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01476  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02361  
Re-running at optimal choice of numdims, 16 
Used 13 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.05622 and the L1 discrepancy is 0.104 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01733  
Without balancing, biasbound (norm=1) is 0.05882 and the L1 discrepancy is 0.118 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01449  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0157  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01465  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01608  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01608  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01356  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01488  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01439  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01468  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01485  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.0154  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01933  
Re-running at optimal choice of numdims, 16 
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01756  
Used 12 dimensions of "allx" for mean balancing, and an additional 16 dimensions of "K" from kernel balancing.
Without balancing, biasbound (norm=1) is 0.05873 and the L1 discrepancy is 0.115 
Without balancing, biasbound (norm=1) is 0.05917 and the L1 discrepancy is 0.115 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01493  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02305  
Re-running at optimal choice of numdims, 6 
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01383  
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01332  
Used 14 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01317  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0134  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01611  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01415  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01524  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01378  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01389  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01535  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01438  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01382  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01483  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02343  
Re-running at optimal choice of numdims, 11 
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02185  
Re-running at optimal choice of numdims, 6 
Used 13 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
Used 12 dimensions of "allx" for mean balancing, and an additional 6 dimensions of "K" from kernel balancing.
=====================  SIM: 50 ===================== 
0.727 % cces original strata missing from sample,   and 15962 / 44932 units
Joining with `by = join_by(strata)`
nsim: 50 DEFAULT 
Warning in kbal(allx = kbal_data, sampled = kbal_data_sampled, cat_data = TRUE,  :
  Dimensions of K greater than 4000, using sampled as default bases

Searching for b value which maximizes the variance in K: 2.532 selected 
Building kernel matrix
Running full SVD on kernel matrix 
Without balancing, biasbound (norm=1) is 0.05501 and the L1 discrepancy is 0.103 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.05472  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0244  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01782  
With 16 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.0174  
With 21 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01473  
With 26 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01406  
With 31 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01416  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01427  
With 41 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01487  
With 46 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.02082  
Disregarding ebalance convergence and re-running at optimal choice of numdims, 26 
nsim: 50 CONV 
Without balancing, biasbound (norm=1) is 0.05501 and the L1 discrepancy is 0.103 
With user-specified 26 dimensions, biasbound (norm=1) of  0.01406  
nsim: 50 MEANFIRST 
Selected 15 dimensions of "allx" to use as mean balance constraints. 
Without balancing, biasbound (norm=1) is 0.05501 and the L1 discrepancy is 0.103 
With 1 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01546  
With 6 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01484  
With 11 dimensions of K, ebalance convergence is TRUE yielding biasbound (norm=1) of 0.01362  
With 16 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01323  
With 21 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01292  
With 26 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01189  
With 31 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01264  
With 36 dimensions of K, ebalance convergence is FALSE yielding biasbound (norm=1) of 0.01591  
Re-running at optimal choice of numdims, 11 
Used 15 dimensions of "allx" for mean balancing, and an additional 11 dimensions of "K" from kernel balancing.
    user   system  elapsed 
2423.610  267.368  678.707 
Warning message:
In mclapply(1:nsims, function(nsim) { :
  scheduled cores 7, 4 encountered errors in user code, all values of the jobs will be affected
> 
> outcome = cces$outcome
> good = which(lapply(sims, function (x) return(class(x))) == "list")
> length(good)
[1] 36
> 
> #sims[-good]
> if(SAVE) {
+     save(sims, outcome, tolerance, maxit, increment, min_num_dims, noise, R2_outcome, eval_kpop,
+          coefs, coefs_outcome, selection_model, p_include, pS_denom, manual_lambda, lambda_min,
+          file = paste0("./TEST_res_kpop", eval_kpop, "lambdamin", lambda_min, "man", manual_lambda,
+                        "_noise", noise, "_on",
+                        Sys.Date(),
+                        "_nsims", length(good),
+                        ".RData"))
+ }
> sims = sims[good]
> 
> #combines all weights across rows but can group by b to get them per iteration
> if(eval_kpop) { 
+     weights <- lapply(sims, `[[`, 3) %>% bind_rows() 
+     residuals <- lapply(sims, `[[`, 4) %>% bind_rows()
+     margins <- lapply(sims, `[[`, 8) 
+ } else { 
+     weights = NULL
+     #SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
+     residuals <- lapply(sims, `[[`, 3) %>% bind_rows()
+     ps_dropped = lapply(sims, `[[`, 4) %>% bind_rows()
+     check_samp = lapply(sims, `[[`, 5) %>% bind_rows()
+     margins = lapply(sims, `[[`, 6) 
+ }
> 
> ##################### eval coverage ####################
> coverage <- function(SE, x_bar, truth =NULL, crit_val= qnorm(0.975)) {
+     if(is.null(truth)) {
+         truth = svymean(~outcome, cces_svy)[1]
+     }
+     x_upper = x_bar + (SE*crit_val)
+     x_lower = x_bar - (SE*crit_val)
+     contains_truth = matrix(NA, ncol = ncol(SE), nrow = 1)
+     for(i in 1:ncol(x_upper)) {
+         contains_truth[,i] = sum((truth <= x_upper[,i] & truth >= x_lower[,i]))/nrow(SE)
+     }
+     colnames(contains_truth) = colnames(x_bar)
+     return(contains_truth)
+ }
> 
> # eval coverage of diff SEs
> all_SE_coverage <- function(sims, drop_NA = F, truth = NULL, methods = c("rake|kpop")) {
+     est <- lapply(sims, `[[`, 1) %>% bind_rows()
+     SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
+     est_c = est[grepl(methods, colnames(est))]
+     SEs = SEs[grepl(methods, colnames(SEs))]
+     # a bit of a pain to drop NAs colwise and get coverage rather than
+     # dropping all NA rows and then getting coverage 
+     #(unfairly drops rows in for methods that don't have NAs) 
+     if(drop_NA) {
+         n_drop = NULL
+         coverage_out = NULL
+         for(i in 1:ncol(est_c)) {
+             # drop = which(is.na(est_c[,i]))
+             # est_temp = est_c[-drop, ]
+             est_temp = na.omit(est_c[,i])
+             n_drop = c(n_drop, nrow(est) - length(est_temp))
+             if(i ==1) {
+                 names(n_drop) = colnames(est_c)[i]
+             } else {
+                 names(n_drop)[i] = colnames(est_c)[i]
+             }
+             
+             
+             SEs_temp = na.omit(SEs[grepl(colnames(est_c)[i], colnames(SEs))])
+             
+             SE_fixed = SEs_temp[grepl("SE_fixed$", colnames(SEs_temp))]
+             SE_linear = SEs_temp[grepl("SE_linear$", colnames(SEs_temp))]
+             SE_quasi = SEs_temp[grepl("SE_quasi$", colnames(SEs_temp))]
+             SE_chad= SEs_temp[grepl("SE_chad$", colnames(SEs_temp))]
+             # SE_svy= SEs_temp[grepl("SVY", colnames(SEs_temp))]
+             # search = gsub("_se_SVY","", colnames(SE_svy))
+             
+             coverage_out = cbind(coverage_out, rbind(coverage(SE_fixed, est_temp, truth = truth),
+                                      coverage(SE_linear, est_temp, truth = truth),
+                                      coverage(SE_quasi, est_temp,truth = truth),
+                                      coverage(SE_chad,est_temp,truth = truth)))
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
+             colnames(coverage_out)[i] = colnames(est_c)[i]
+         }
+     } else {
+         est_c = est[grepl(methods, colnames(est))]
+         SEs = SEs[grepl(methods, colnames(SEs))]
+         SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))]
+         SE_linear = SEs[grepl("SE_linear$", colnames(SEs))]
+         SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]
+         SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]
+         
+         SE_svy= SEs[grepl("SVY", colnames(SEs))]
+         if(ncol(SE_svy) != 0){
+             #just making sure we're getting the estimates for the same SEs that we output from svy obj which currently is demos_noedu
+             search = gsub("_se_SVY","", colnames(SE_svy))
+             grepl(search, colnames(est_c))
+             s = coverage(SE_svy, est_c[,grepl(search, colnames(est_c))])
+             s1 = rep(NA, ncol(SE_fixed))
+             s1[grepl(search, colnames(est_c))] = s
+             #colnames(s1) = colnames(SE_fixed)
+             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
+                                  coverage(SE_linear, est_c, truth = truth),
+                                  coverage(SE_quasi, est_c,truth = truth),
+                                  coverage(SE_chad,est_c,truth = truth), 
+                                  s1)
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_svy")
+         } else {
+             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
+                                  coverage(SE_linear, est_c, truth = truth),
+                                  coverage(SE_quasi, est_c,truth = truth),
+                                  coverage(SE_chad,est_c,truth = truth))
+             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
+         }
+         
+     }
+     
+     if(drop_NA) {
+         out = list()
+         out$n_drop = n_drop
+         out$coverage = coverage_out
+     } else {
+         out = coverage_out
+     }
+     return(out)
+ }
> 
> 
> #res: 
> good = which(lapply(sims, function (x) return(class(x))) == "list")
> length(good)
[1] 36
> 
> ######## Bias
> est <- lapply(sims, `[[`, 1) %>% bind_rows()
> est = est[grepl(c("rake|kpop|post|unweighted|h"), colnames(est))]
> bias = colMeans(est, na.rm = T)
> bias = bias - mean(cces$outcome)
> bias = data.frame(bias = t(t(bias))*100)
> bias = bias %>% arrange(desc(abs(bias)))
> round(bias,3)
                      bias
unweighted          -7.825
rake_demos_noeduc   -6.104
rake_demos_weduc    -5.797
post_stratification -2.829
rake_all            -2.815
kpop                -1.153
kpop_conv           -1.146
kpop_mf             -0.868
ht_truth             0.852
hayek_truth          0.283
rake_truth           0.186
kpop_demos             NaN
kpop_demos_wedu        NaN
kpop_all               NaN
> # kable(round(bias, 3), format = "latex", booktabs = T, 
> #       caption = paste0("Bias \\textbf{in Percent} across ", length(good), " sims: All Methods (Target = ", round(mean(outcome),3)*100, ")"))
> 
> 
> ########## SEs
> SE_coverage = all_SE_coverage(sims, truth = mean(outcome), drop_NA = T)
Error in `[.data.frame`(x_upper, , i) : undefined columns selected
Calls: all_SE_coverage -> cbind -> rbind -> coverage -> [ -> [.data.frame
Execution halted
