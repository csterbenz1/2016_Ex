---
title: "Kpop Simulation Results"
output: pdf_document
header-includes:
  - \usepackage{graphicx}
  - \usepackage{booktabs}
  - \usepackage{makecell}
  - \usepackage{xcolor}
---

```{r libs, include =F, message=FALSE, warning= F}
suppressMessages(library(tidyverse))
library(parallel)
suppressMessages(library(knitr))
library(kableExtra)
library(survey)
```

```{r filenames, include = F}
#linear
r33_file_lin = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_res_kpopTRUE_noise1.414_on2022-11-16_nsims494.RData"
r66_file_lin= "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_res_kpopTRUE_noise0.707_on2022-11-16_nsims494.RData"
r66_bern_file_lin  = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_res_kpopTRUE_noise0.707_on_bernTRUE_2022-11-16_nsims494.RData"

#nonlin
r66_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_kpopTRUE_noise0.707106781186548_on2023-01-18_nsims481.RData"

r33_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_kpopTRUE_noise1.4142135623731_on2023-01-19_nsims494.RData"

#r5_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_res_kpopTRUE_noise1_on2023-01-26_nsims481.RData"

r5_nonlin_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_res_kpopTRUE_noise1_on2023-01-31_nsims494.RData"

r5_nonlin_inter_file = "~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2023 Data/noscale_kpopTRUE_noise1_on2023-02-15_nsims494.RData"
#"~/Documents/Cloud Documents/Hazlett:Hartman RA/2016 Election/2016_Ex/Summer 2022/sims/noscale_kpopTRUE_noise1_on2023-02-01_nsims468.RData"

```


```{r functions, include = F}
coverage <- function(SE, x_bar, truth, crit_val= qnorm(0.975)) {
    x_upper = x_bar +  (SE*crit_val)
    x_lower = x_bar - (SE*crit_val)
    contains_truth = matrix(NA, ncol = ncol(SE), nrow = 1)
    for(i in 1:ncol(x_upper)) {
        contains_truth[,i] = sum((truth <= x_upper[,i] & truth >= x_lower[,i]))/nrow(SE)
    }
    colnames(contains_truth) = colnames(x_bar)
    return(contains_truth)
}

# eval coverage of diff SEs
all_SE_coverage <- function(sims, adjust_bias = FALSE, drop_NA = F, truth = NULL, methods = c("rake|kpop")) {

    est <- lapply(sims, `[[`, 1) %>% bind_rows()
    if(adjust_bias) {
        temp = est[grepl(methods, colnames(est))]
        avg_bias = colMeans(temp) - truth
        # avg_bias
        # la %>% group_by(estimator) %>% summarise(unique(avg_bias)/100)
        # avg_bias
        # colnames(test)
        bias_adj = temp
        for(i in 1:ncol(temp)) {
            bias_adj[, i] = temp[,i] - avg_bias[i]
        }
        est[grepl(methods, colnames(est))] = bias_adj
    }
    
    SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
    est_c = est[grepl(methods, colnames(est))]
    SEs = SEs[grepl(methods, colnames(SEs))]
    
    
    if(drop_NA) {
        n_drop = NULL
        coverage_out = NULL
        for(i in 1:ncol(est_c)) {
            # drop = which(is.na(est_c[,i]))
            # est_temp = est_c[-drop, ]
            est_temp = na.omit(est_c[,i])
            n_drop = c(n_drop, nrow(est) - length(est_temp))
            if(i ==1) {
                names(n_drop) = colnames(est_c)[i]
            } else {
                names(n_drop)[i] = colnames(est_c)[i]
            }
            SEs_temp = na.omit(SEs[grepl(paste0(colnames(est_c)[i],"_SE"), colnames(SEs))])
            
            SE_fixed = SEs_temp[grepl("SE_fixed$", colnames(SEs_temp))]
            SE_linear = SEs_temp[grepl("SE_linear$", colnames(SEs_temp))]
            SE_quasi = SEs_temp[grepl("SE_quasi$", colnames(SEs_temp))]
            SE_chad= SEs_temp[grepl("SE_chad$", colnames(SEs_temp))]
            # SE_svy= SEs_temp[grepl("SVY", colnames(SEs_temp))]
            # search = gsub("_se_SVY","", colnames(SE_svy))
            
            coverage_out = cbind(coverage_out, rbind(coverage(SE_fixed, est_temp, truth = truth),
                                                     coverage(SE_linear, est_temp, truth = truth),
                                                     coverage(SE_quasi, est_temp,truth = truth),
                                                     coverage(SE_chad,est_temp,truth = truth)))
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
            colnames(coverage_out)[i] = colnames(est_c)[i]
            
        }
    } else {
        est_c = est[grepl(methods, colnames(est))]
        SEs = SEs[grepl(methods, colnames(SEs))]
        SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))]
        SE_linear = SEs[grepl("SE_linear$", colnames(SEs))]
        SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]
        SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]
        
        SE_svy= SEs[grepl("SVY", colnames(SEs))]
        if(ncol(SE_svy) != 0){
            #just making sure we're getting the estimates for the same SEs that we output from svy obj which currently is demos_noedu
            search = gsub("_se_SVY","", colnames(SE_svy))
            grepl(search, colnames(est_c))
            s = coverage(SE_svy, truth = truth, est_c[,grepl(search, colnames(est_c))])
            s1 = rep(NA, ncol(SE_fixed))
            s1[grepl(search, colnames(est_c))] = s
            #colnames(s1) = colnames(SE_fixed)
            coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
                                 coverage(SE_linear, est_c, truth = truth),
                                 coverage(SE_quasi, est_c,truth = truth),
                                 coverage(SE_chad,est_c,truth = truth), 
                                 s1)
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_svy")
        } else {
            coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
                                 coverage(SE_linear, est_c, truth = truth),
                                 coverage(SE_quasi, est_c,truth = truth),
                                 coverage(SE_chad,est_c,truth = truth))
            rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
        }
        
    }
    
    if(drop_NA) {
        out = list()
        out$n_drop = n_drop
        out$coverage = coverage_out
    } else {
        out = coverage_out
    }
    if(adjust_bias) {
        out$bias_adj_est = est
    }
    return(out)
}


# all_SE_coverage <- function(sims, drop_NA = F, truth = NULL, methods = c("rake|kpop")) {
#     est <- lapply(sims, `[[`, 1) %>% bind_rows()
#     SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
#     est_c = est[grepl(methods, colnames(est))]
#     SEs = SEs[grepl(methods, colnames(SEs))]
#     # a bit of a pain to drop NAs colwise and get coverage rather than
#     # dropping all NA rows and then getting coverage 
#     #(unfairly drops rows in for methods that don't have NAs) 
#     
#     if(drop_NA) {
#         n_drop = NULL
#         coverage_out = NULL
#         for(i in 1:ncol(est_c)) {
#             # drop = which(is.na(est_c[,i]))
#             # est_temp = est_c[-drop, ]
#             est_temp = na.omit(est_c[,i])
#             n_drop = c(n_drop, nrow(est) - length(est_temp))
#             if(i ==1) {
#                 names(n_drop) = colnames(est_c)[i]
#             } else {
#                 names(n_drop)[i] = colnames(est_c)[i]
#             }
#             SEs_temp = na.omit(SEs[grepl(paste0(colnames(est_c)[i],"_SE"), colnames(SEs))])
#             
#             SE_fixed = SEs_temp[grepl("SE_fixed$", colnames(SEs_temp))]
#             SE_linear = SEs_temp[grepl("SE_linear$", colnames(SEs_temp))]
#             SE_quasi = SEs_temp[grepl("SE_quasi$", colnames(SEs_temp))]
#             SE_chad= SEs_temp[grepl("SE_chad$", colnames(SEs_temp))]
#             # SE_svy= SEs_temp[grepl("SVY", colnames(SEs_temp))]
#             # search = gsub("_se_SVY","", colnames(SE_svy))
#             
#             coverage_out = cbind(coverage_out, rbind(coverage(SE_fixed, est_temp, truth = truth),
#                                      coverage(SE_linear, est_temp, truth = truth),
#                                      coverage(SE_quasi, est_temp,truth = truth),
#                                      coverage(SE_chad,est_temp,truth = truth)))
#             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
#             colnames(coverage_out)[i] = colnames(est_c)[i]
#             
#         }
#     } else {
#         est_c = est[grepl(methods, colnames(est))]
#         SEs = SEs[grepl(methods, colnames(SEs))]
#         SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))]
#         SE_linear = SEs[grepl("SE_linear$", colnames(SEs))]
#         SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]
#         SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]
#         
#         SE_svy= SEs[grepl("SVY", colnames(SEs))]
#         if(ncol(SE_svy) != 0){
#             #just making sure we're getting the estimates for the same SEs that we output from svy obj which currently is demos_noedu
#             search = gsub("_se_SVY","", colnames(SE_svy))
#             grepl(search, colnames(est_c))
#             s = coverage(SE_svy, truth = truth, est_c[,grepl(search, colnames(est_c))])
#             s1 = rep(NA, ncol(SE_fixed))
#             s1[grepl(search, colnames(est_c))] = s
#             #colnames(s1) = colnames(SE_fixed)
#             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
#                                  coverage(SE_linear, est_c, truth = truth),
#                                  coverage(SE_quasi, est_c,truth = truth),
#                                  coverage(SE_chad,est_c,truth = truth), 
#                                  s1)
#             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_svy")
#         } else {
#             coverage_out = rbind(coverage(SE_fixed, est_c, truth = truth),
#                                  coverage(SE_linear, est_c, truth = truth),
#                                  coverage(SE_quasi, est_c,truth = truth),
#                                  coverage(SE_chad,est_c,truth = truth))
#             rownames(coverage_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad")
#         }
#         
#     }
#     
#     if(drop_NA) {
#         out = list()
#         out$n_drop = n_drop
#         out$coverage = coverage_out
#     } else {
#         out = coverage_out
#     }
#     return(out)
# }


manual_rescale <- function(dat, a =0, b= 1) {
    rescaled = (b-a)*( dat - min(dat))/(max(dat) - min(dat)) + a
    return(rescaled)
}
```



```{r outcome, include = F}

make_outcome <- function(noise, bern = F, p_include = NULL, outcome = NULL,
                         selection_coefs = NULL, outcome_coefs = NULL, 
                         selection_model = NULL, p_include_denom = 1,
                         coverage_eval = TRUE, linear_model = FALSE) {
    #have to copy paste everything from the sims bc didn't set seed right before noise added UGHUGHGU
    set.seed(9345876)
    
    if(detectCores() > 10) {
      path_data = "/home/csterbenz/Data/"
      cores_saved = 10
    } else if(detectCores() != 4) {
      path_data= "/Users/Ciara_1/Dropbox/kpop/Updated/application/data/"
      cores_saved = 6
    } else {
        path_data= "/Users/Ciara/Dropbox/kpop/Updated/application/data/"
        cores_saved = 2
    }
    
    POPW = FALSE
    bern_unused = FALSE
    coverage_eval_unused = TRUE
    linear_model_unused = FALSE # jsut to not write over the input but keep seed the same
    noise_unused = (1/2)*sqrt(2)
    TEST = FALSE # to run with a linear kernel so it's way faster; UPDATE: errors catch this as mistake and prevent
    tolerance = 1e-4
    maxit = 500
    #both for runtime
    increment = 5
    min_num_dims = NULL
    max_num_dims = NULL
    SAVE = TRUE
    ##### Central Params to adjust
    n_sample = 500
    simple_selection_model = TRUE
    nsims = (detectCores()-cores_saved)*13
    nsims
    ###################### Formulas ################
    formula_rake_demos_noeduc <- ~recode_age_bucket + recode_female + recode_race +
      recode_region + recode_pid_3way
    
    #updated to include 6 way edu
    formula_rake_demos_weduc <- ~recode_age_bucket + recode_female +
      recode_race + recode_region + recode_educ + recode_pid_3way
    
    formula_rake_all_vars <- ~recode_age_bucket + recode_female +
      recode_race + recode_region + recode_pid_3way + recode_educ +
      recode_income_5way + recode_relig_6way + recode_born + recode_attndch_4way
    
    formula_ps <- ~recode_age_3way + recode_female + recode_race +
      recode_region + recode_educ_wh_3way + recode_pid_3way
    
    formula_ps_reduc <- ~recode_age_3way + recode_female +
      recode_race + recode_region + recode_pid_3way  +
      recode_income_3way + recode_born + recode_educ_wh_3way
    
    # #let's coarsen income and religion and attend church for all:
    formula_ps_all <- ~recode_age_3way + recode_female +
      recode_race + recode_region + recode_pid_3way + recode_educ_3way +
      recode_income_3way + recode_relig_6way + recode_born + recode_attndch_bin
    
    create_targets <- function (target_design, target_formula) {
      target_mf <- model.frame(target_formula, model.frame(target_design))
      target_mm <- model.matrix(target_formula, target_mf)
      wts <- weights(target_design)
    
      return(colSums(target_mm * wts) / sum(wts))
    }
    
    manual_rescale <- function(dat, a =0, b= 1) {
        rescaled = (b-a)*( dat - min(dat))/(max(dat) - min(dat)) + a
        return(rescaled)
    }
    
    ### Post-stratification function
    ## For now assumes that strata variable is already created and in
    ## the data set and called "strata"
    postStrat <- function(survey, pop_counts, pop_w_col, strata_pass, warn = T) {
      survey_counts <- survey %>%
        group_by(!!as.symbol(strata_pass)) %>%
        summarize(n = n()) %>%
        ungroup() %>%
        mutate(w_survey = n / sum(n))
    
      pop_counts <- pop_counts %>%
        rename(w_pop = matches(pop_w_col))
      
      if(warn == T & nrow(survey_counts) !=  nrow(pop_counts)) {
          missing_strat = pop_counts[! (( pop_counts[, strata_pass]%>% pull()) %in% (survey_counts[, strata_pass]%>% pull() )), strata_pass]
          warning(paste("Strata in Pop not found in Sample. Dropping", 
                        sum(pop_counts[(pop_counts[, strata_pass] %>% pull()) %in% 
                                           (missing_strat %>% pull()),"n" ]), 
                        "empty cells\n"), immediate.  =T )
      } 
    
      post_strat <- pop_counts %>%
        left_join(survey_counts, by = strata_pass) %>%
        filter(!is.na(w_survey)) %>%
        ## Normalizes back to 1 after dropping
        ## empty cells
        mutate(w_pop = w_pop * 1/sum(w_pop),
               w = w_pop / w_survey) %>%
        dplyr::select(!!as.symbol(strata_pass), w)
    
      survey <- survey %>%
        left_join(post_strat)
    
      return(survey)
    }
    check_sample <- function(sample, selection_model) {
        check = model.matrix(selection_model, data = sample)
        check = colSums(check)
        fail = check[which(check ==0)]
        if(length(fail) ==0) {
            return(NULL)
        } else {
            return(fail)
        }
    }
    check_outcome <- function(outcome) {
        beyond_support = (min(outcome) <0 | max(outcome) > 1)
        return(beyond_support)
    }
    
    bound_outcome <- function(outcome, coefs, increment = 1, increment_intercept = .01, noise, cces_expanded, silent = T) {
        denom = 10
        fail = check_outcome(outcome)
        while(fail) {
            denom = denom + increment
            if(!silent) { cat(denom, ": ") }
            coefs_use = coefs/denom
            outcome = cces_expanded %*% coefs_use
            
            outcome = outcome + rnorm(nrow(cces_expanded), mean = 0, sd = sd(outcome)*noise)
            summary(outcome)
            if(max(outcome) <=1 & min(outcome) <0 ) {
                coefs[1] = coefs[1] + increment_intercept
                if(!silent) { cat("\nmoving intercept up", coefs[1],  "\n") }
                denom = denom - increment
            }
            if(max(outcome) >1 & min(outcome) >=0 ) {
                coefs[1] = coefs[1] - increment_intercept
                if(!silent) { cat("\nmoving intercept down", coefs[1],  "\n") }
                denom = denom - increment
            }
            fail = check_outcome(outcome)
            if(!silent) { cat(round(min(outcome),2), round(max(outcome),2),  "\n") }
        }
        if(!silent) { cat(paste("Min denom:", denom)) }
        return(list(outcome = outcome, coefs = coefs_use, denom = denom))
        
    }
    
    
    ############# Load Data #####################
    #these data have been cleaned already see app_modeled for how it was done
    ## Load Pew
    pew <- readRDS(paste0(path_data, "pew_lasso_061021.rds"))
    
    ### Load Target Data
    cces <- readRDS(paste0(path_data, "cces_lasso_061021.rds"))
    
    ################# Selection Model and Outcome Model ###########
    
    if(is.null(p_include) & is.null(outcome)) {
        if(!coverage_eval) {

            if(simple_selection_model) {
                #first attempt to make this worse
                selection_model = as.formula(~recode_pid_3way:poly(recode_age, 2) +
                                                 recode_female:recode_pid_3way)
                #first attempt to make this worse
                #center age then square
                pew = pew %>% mutate(centered_age = scale(recode_age, scale = F))
                cces = cces %>% mutate(centered_age = scale(recode_age, scale = F))
        
                selection_model = as.formula(~recode_pid_3way*poly(centered_age, 2) +
                                                 #recode_age:recode_pid_3way +
                                                 recode_female*recode_pid_3way)
        
                #selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
        
            } else {
                selection_model = as.formula(~recode_female:recode_pid_3way +
                                                 recode_age:recode_pid_3way +
                                                 #adds a bit mroe bias to edu+d
                                                 recode_pid_race +
                                                 recode_race_reg_wh_educ +
                                                 recode_educ_wh_3way +
                                                 poly(recode_age, 3))
            }
            # Stack data with S = 1 indicating Pew
            stack_data <- data.frame(bind_rows(pew, cces),
                                     S = c(rep(1, nrow(pew)), rep(0, nrow(cces))))
        
        
            mod <- model.matrix(selection_model, data = stack_data)
            nrow(mod)
            ## Remove columns where Pew missing strata
            ncol(mod)
            mod <- mod[, apply(mod[stack_data$S == 1, ], 2, sum) != 0]
            ## Remove columns where CCES missing Strata
            mod <- mod[, apply(mod[stack_data$S == 0, ], 2, sum) != 0]
            ncol(mod)
            lasso_lambda <- cv.glmnet(x= mod[,-1],
                                      y = as.matrix(stack_data$S),
                                      alpha = 1,
                                      family = "binomial",
                                      intercept = TRUE)
            lasso_lambda$lambda.min
            lasso_include <- glmnet(x= mod[,-1],
                                    y = as.matrix(stack_data$S),
                                    alpha = 1,
                                    lambda = lasso_lambda$lambda.min,
                                    weights = if(POPW){ c(rep(1, nrow(pew)),
                                                          cces$commonweight_vv_post)} else {NULL},
                                    family = "binomial",
                                    intercept = TRUE)
        
            lasso_include_coefs <- coef(lasso_include)
            res <- as.matrix(lasso_include_coefs)
            #View(res)
            ncol(mod)
            sum(lasso_include_coefs == 0)
        
            lasso_include_coefs
            lasso_pinclude = predict(lasso_include,
                                     s= lasso_lambda$lambda.min,
                                     type = "response",
                                     newx = mod[stack_data$S == 0,-1])
        
            p_include <- lasso_pinclude
            sum(p_include)
            cor(p_include, cces$outcome)
            cor(p_include, cces$mod_cces_on_cces_pD)
            cor(p_include, cces$mod_cces_on_cces_pR)
        
            ########### Sampling Inclusion Results
            summary(p_include)
            sum(p_include)
            mean(p_include)
            p_include_raw = p_include
        
        
            intercept_shift = 0
            p_include = p_include*(n_sample/sum(p_include)) + intercept_shift
            mean(p_include)
            summary(p_include)
            sum(p_include)
        
            #################### Define Outcome #########
            cces$outcome = cces$diff_cces_on_cces
        } else {
        if(linear_model) {
            ########## DESIGN SELECTION MODEL: Specifying Coefs Directly
            #coefs: pid, age, gender
            selection_model = as.formula(~recode_pid_3way + recode_age_bucket + recode_female)
            cces_expanded = model.matrix(selection_model, data = cces)
            #needs to be n x p X p x 1 -> coef matrix is p x 1
            coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            rownames(coefs) = colnames(cces_expanded)
            coefs[,1] = c(-5.2, #intercept
                          .1, #selection of indep pos
                          .4, #selection of R pos
                          .1, #36-50,
                          .3, #51-64,
                          .8, #65+,
                          .4 #male pos
            )
            
            xbeta = cces_expanded %*% coefs
            p_include = plogis(xbeta)
            sum(p_include)
            summary(p_include)
            
            #################### DESIGN OUTCOME MODEL ##################
            coefs_outcome = coefs
            
            coefs_outcome[,1] = c(6.1, #intercept
                                  -.3, #  indep pos #decreasing lowers mean, increases corr #try 1
                                  -1.1, #  R pos #empirically
                                  -.2 ,#36-50, #empirically in cces lean dem 50% #.55
                                  -.3, #51-64, #empirically lean rep slightly 50%
                                  -.7, #65+, #empirically lean rep 51%
                                  #base cat: 18-35 lean strongly dem 58%
                                  -.7 #male #empirically women lean dem 53%
            )
            coefs_outcome = coefs_outcome/10
            
            xbeta_outcome = cces_expanded %*% coefs_outcome
            
            if(!bern) {
                cat(paste("(linear) Adding sd(outcome)*",round(noise, 3), "\n"))   
                xbeta_outcome = xbeta_outcome + rnorm(nrow(cces), mean = 0, sd = sd(xbeta_outcome)*noise)
            }
            if(bern) {
                cat(paste("(linear) Adding bernoulli ### UPDATE ###",round(noise, 3), "\n"))  
                xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
            }
            cat(paste("Range of outcome w/noise is\n"))
            cat(paste(summary(xbeta_outcome), "\n"))
            if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {warning("outcome beyond prob support for some units when noise is added", immediate. = T)}
            s = summary(lm(xbeta_outcome ~ recode_pid_3way + recode_age_bucket + recode_female,data = cces))
            R2_outcome = s$adj.r.squared
            cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
            cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3)))
            cat(paste("\nCorr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3)))
            #bernoulli draw
            coefs_outcome_pass = coefs_outcome
            cces$outcome = xbeta_outcome
            outcome = xbeta_outcome
    } else {
  
        if(is.null(selection_model)) {
            selection_model = as.formula(~recode_pid_3way + recode_female + recode_age_bucket 
                                          + recode_educ_3way 
                                          + recode_pid_3way:recode_age_bucket)
            
            cces_expanded = model.matrix(selection_model, data = cces)
            coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            rownames(coefs) = colnames(cces_expanded)
                
            #(p(S)) for negative bias select non dem voters
            coefs[,1] = c(-7.7, #intercept -5 w race
                          2, #selection of indep pos
                          3, #selection of R pos
                          .5, #male
                          .15, #36-50,
                          .1, #51-64,
                          .2, #65+,
                          .7, #college
                          -0.6 , #post-grad
                          .2,#ind x 36-50
                          1, #rep x 36-50,
                          .5, #ind x 51-64,
                          .5, #rep x 51-64,
                          -.2, #ind x 65+
                          1 #rep x 65+
            )
            #################### DESIGN OUTCOME MODEL ##################
            coefs_outcome = coefs
            #p(D)
             coefs_outcome[,1] = c(8, #intercept #5 w race
                              -3,#-.5, #selection of indep pos
                              -5,# -.8, #selection of R pos
                              -.3, #male
                              -.5, #36-50,
                              -.1, #51-64,
                              -.2, #65+,
                              .8, #college
                              .9,  #post-grad
                              -2.8,#ind x 36-50
                              -2.8, #rep x 36-50,
                              -.5, #ind x 51-64,
                              -.5, #rep x 51-64,
                              .8, #ind x 65+
                              -1 #rep x 65+
                              )
             cces = cces %>% mutate(strata = paste(recode_pid_3way,
                                      recode_female, 
                                      recode_age_bucket,
                                      recode_educ_3way,
                                      sep = "_"))
    
            pew = pew %>% mutate(strata = paste(recode_pid_3way,
                                                  recode_female, 
                                                  recode_age_bucket,
                                                  recode_educ_3way,
                                                  sep = "_"))
        } else {
            
            #### NB: does not currently make strata consistent w selection model if passed in
            
            cces_expanded = model.matrix(selection_model, data = cces)
            if(is.null(selection_coefs) | is.null(outcome_coefs) |
               nrow(selection_coefs) != ncol(cces_expanded) |
               nrow(outcome_coefs) != ncol(cces_expanded)) {
                stop("when selection model inputted must also input selection and outcome coefsand
                     rows must match levels of selection model.")
            }
            #coefs = matrix(NA,nrow = ncol(cces_expanded), ncol =1 )
            #rownames(coefs) = colnames(cces_expanded)
            coefs = selection_coefs
            coefs_outcome = outcome_coefs
        }
            
            xbeta = cces_expanded %*% coefs
            p_include = plogis(xbeta)
            p_include = p_include/p_include_denom
            sum(p_include)
        
            if(!bern) {
                cat(paste("Adding sd(outcome)*",round(noise, 3), "\n")) 
                #toggle off for earlier results
                if(!is.null(selection_model)) {set.seed(1383904)}
                bound = bound_outcome(outcome = cces_expanded %*% coefs_outcome,
                                      coefs = coefs_outcome,
                                      cces_expanded = cces_expanded,
                                      noise = noise)
                xbeta_outcome = bound$outcome
                coefs_outcome_pass = bound$coefs
                beyond_support = check_outcome(xbeta_outcome)
                cces$outcome = xbeta_outcome
            }
            if(min(xbeta_outcome) <0 | max(xbeta_outcome) > 1) {
                warning("Outcome beyond prob support for some units when noise is added, but should be bounded\n",
                                                                        immediate. = T)
            }
            if(bern) {
                cat(paste("Adding bernoulli noise and making outcome binary\n"))  
                xbeta_outcome = rbinom(nrow(cces), 1, xbeta_outcome) 
                cat(paste("Corr of S (one sample draw) and Y", round(cor(sample, xbeta_outcome),3)))
            }
            
            cat(paste("Range of outcome w/noise is\n"))
            cat(paste("xbeta is", is.null(xbeta_outcome), "null and", length(xbeta_outcome), "long 1\n"))
            cat(paste(summary(xbeta_outcome), "\n"))
            cat(paste("xbeta is", is.null(xbeta_outcome), "null and", length(xbeta_outcome), "long 2\n"))
            s = summary(lm(update(selection_model, outcome ~ .), data = cces))
            R2_outcome = s$adj.r.squared
            cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
            cat(paste("OK2\n"))
            cat(paste("Mean scaled outcome (target) is", round(mean(xbeta_outcome)*100,3), "\n"))
            cat(paste("Corr of sampling prob and outcome ", round(cor(xbeta_outcome, p_include),3),"\n"))
            cces$outcome = xbeta_outcome
            outcome = xbeta_outcome
            }
        
        }
    } else { 
        coefs_outcome_pass = outcome_coefs
        xbeta_outcome = outcome
        if(is.null(selection_model)) {
            stop("pass in selection model to calculate R2")
        }
        s = summary(lm(update(selection_model, outcome ~ .), data = cces))
        R2_outcome = s$adj.r.squared
        cat(paste("R^2 outcome is", round(s$adj.r.squared,3), "\n"))
    }
    
    if(coverage_eval) {
        formula_ps <- selection_model
    } else {
        formula_ps <- ~recode_age_3way + recode_pid_3way + recode_female
    }
   
    ######P-include plot
    gg_dat = data.frame(Selection_Probability = p_include,
                        Pid = cces$recode_pid_3way,
                        Outcome_pD = outcome)
    
    gg_p_include = ggplot(gg_dat) +
        geom_density(aes(x= Selection_Probability)) + 
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of Selection Probabilities")) +
        theme_bw()
    
    la = data.frame(pS = p_include, pid = cces$recode_pid_3way) %>%
        group_by(pid) %>% summarise(max = round(max(pS)*100,2 ))
    
    gg_p_include_pid = ggplot(gg_dat) +
        geom_density(aes(x= Selection_Probability, color = Pid)) + 
        annotate(geom = "label", x=quantile(p_include,.5),
                 y=Inf, vjust = 1,
                 color = "red",
                 label =  paste0("Dem Max P(S)= ", la[la$pid =="Dem", "max"], "%")) + 
        annotate(geom = "label",x=quantile(p_include,.5), 
                 y=Inf, vjust = 3,
                 label= paste0("Ind Max P(S)= ", la[la$pid =="Ind", "max"], "%" ), 
                  color = "green") +
        annotate(geom = "label",x=quantile(p_include,.5), 
                 y=Inf, vjust = 5,
                 label= paste0("Rep Max P(S)= ", la[la$pid =="Rep", "max"], "%" ),
                  color = "blue") +
        # geom_text(x=.10, y=250, label= paste0("Dem Max P(S)=", la[la$pid =="Dem", "max"] ), 
        #           color = "red") +
        #  geom_text(x=.10, y=200, label= paste0("Ind Max P(S)=", la[la$pid =="Ind", "max"] ), 
        #           color = "green") +
        #  geom_text(x=.10, y=150, label= paste0("Rep Max P(S)=", la[la$pid =="Rep", "max"] ), 
        #           color = "Blue") +
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of Seleciton Probabilities by Party")) +
        theme_bw()
    #gg_p_include_pid
    
    gg_p_include_outcome = ggplot(gg_dat) +
        geom_point(aes(x= Selection_Probability, y= Outcome_pD, color = Pid)) + 
        ggtitle(paste("Linear=", linear_model, " R2=",
                      round(R2_outcome,2), "Distribution of p(Y=1) and p(S=1)")) +
        theme_bw()
        
    
    ############### kable_res
    selection_coefs_kable = data.frame(coefs)
    colnames(selection_coefs_kable) = "Coefficient Value"
    rownames(selection_coefs_kable) = gsub("^recode_", "", rownames(selection_coefs_kable))
    selection_coefs_kable = kable(selection_coefs_kable,
                                      format = "latex", booktabs = T,
                       caption = paste("Linear=",linear_model, "Selection Model" ))
    
    cat(paste("Sum of Selection Probs is:", round(sum(p_include), 3), "\n"))
    out = c(min(p_include), 
            quantile(p_include, c(0.25, 0.5, .75)), 
            max(p_include), 
            sum(p_include))
    out = as.matrix(out)
    rownames(out) =  c("Min", "25%", "Mean", "75%", "Max", "Sum")
    colnames(out) = c("Selection Probability")
    
    prob_kable = kable(round(out, 4), format = "latex", 
                       booktabs = T,
                       caption = paste("Linear=",linear_model,"Sample Inclusion Probabilities"))
    
    coefs_outcome_kable = data.frame(coefs_outcome_pass)
    colnames(coefs_outcome_kable) = "Coefficient Value"
    rownames(coefs_outcome_kable) = gsub("^recode_", "", rownames(coefs_outcome_kable))
    coefs_outcome_kable = kable(coefs_outcome_kable,
                                      format = "latex", booktabs = T,
                       caption = paste("Linear=",linear_model," R2=",
                      round(R2_outcome,2), "Outcome Model") )
    

    return(list(R2 = R2_outcome,
                p_include = p_include,
                plot_pinclude = gg_p_include,
                plot_pinclude_pid = gg_p_include_pid,
                plot_pinclude_outcome = gg_p_include_outcome,
                linear = linear_model,
                target = mean(outcome),
                corr = cor(xbeta_outcome, p_include), 
                selection_prob_kable = prob_kable,
                coefs_outcome_kable= coefs_outcome_kable, 
                selection_coefs_kable= selection_coefs_kable))
}

```

```{r emp_SEs, include = F}
empirical_SEs <- function(sims, eval_kpop = T, return_svy_package = F, na_rm = F) {
    SEs = lapply(sims, `[[`, 2) %>% bind_rows()
    est <- lapply(sims, `[[`, 1) %>% bind_rows()
    cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
    #cols = if(eval_kpop) { c(3:7,10:12,14:19)} else {c(3:7,10:12)}
    est =est[, cols]

    #avg SEs
    avg_SE = colMeans(SEs, na.rm = na_rm)
    avg_SE_out = rbind(avg_SE[grepl("SE_fixed$", names(avg_SE))],
                   avg_SE[grepl("SE_linear$", names(avg_SE))],
                   avg_SE[grepl("SE_quasi$", names(avg_SE))],
                   avg_SE[grepl("SE_chad", names(avg_SE))],
                   avg_SE[grepl("SVY", names(avg_SE))])
    rownames(avg_SE_out) = c("SE_fixed", "SE_linear", "SE_quasi", "SE_chad", "SE_SVY") 
    colnames(avg_SE_out) = gsub("_SE_fixed", "", colnames(avg_SE_out))
    
    #bootstrapped SEs
    boot_SE = t(as.matrix(apply(est, 2, sd)))
    SE_boot = boot_SE[, colnames(boot_SE) %in% colnames(avg_SE_out)]
    #stupid fix for non names kpop mf Ses in some sims
    good_cnames = grepl("kpop|rake|post_stratification|unweighted", colnames(avg_SE_out))
    good_cnames = colnames(avg_SE_out)[good_cnames]
    emp_SEs = rbind(avg_SE_out, SE_boot[good_cnames])
    if(rownames(emp_SEs)[nrow(emp_SEs)] == "") { rownames(emp_SEs)[nrow(emp_SEs)] = "SE_boot" }
    
    if(!return_svy_package) {
        avg_SE_out = avg_SE_out[-which(rownames(avg_SE_out) == "SE_SVY"), ]
        emp_SEs = emp_SEs[-which(rownames(emp_SEs) == "SE_SVY"), ]
    }
    
    return(list(emp_SEs =emp_SEs, 
                boot_SE = boot_SE,
                avg_SE = avg_SE_out) )    
}

```


```{r run_outcome, include = F, cache =T}
r33_lin = make_outcome(noise = sqrt(2), linear_model = T)
r66_lin = make_outcome(noise = sqrt(2)*(1/2), linear_model = T)
load(r66_nonlin_file)
r66_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)

load(r33_nonlin_file)
r33_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
#dumb change to update cache
rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)

load(r5_nonlin_file)
r5_nonlin = make_outcome(noise = noise, linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
rm(sims, coefs, coefs_outcome, p_include, outcome, SE_coverage, emp_SE, selection_model, pS_denom)


load(r5_nonlin_inter_file)
r5_nonlin_inter = make_outcome(noise = noise, 
                               linear_model = F,
                          p_include = p_include,
                          outcome = outcome, 
                          selection_model = selection_model,
                          selection_coefs = coefs,
                          outcome_coefs = coefs_outcome)
rm(sims, coefs, coefs_outcome, p_include, outcome, selection_model, pS_denom)

```




# Simulation Set Up: Linear Model

## Linear Selection Model
$p(S=1) = logit^{-1}\Big( PID(3 way) + Age(4way)+ Gender  \Big)$

More specifically:
$$p(S=1) = logit^{-1}\Big( \alpha + \beta_1 Indep + \beta_2 Rep + \beta_3 Age_{36-50} + \beta_4  Age_{51-64} + \beta_5 Age_{65+} + \beta_6 Male \Big)$$

Where coefs are chosen roughly similar to a fitted model to pew that yields a sample size around 500. Namely:
```{r, echo = F}
r33_lin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling  probabilities

```{r, echo = F}
r33_lin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

## Linear Outcome Model

Trying to keep things straight forward, I use the identical outcome model with coefficients chosen to roughly match the observed vote choice Democratic margin. In other words, again we have:
$$p(Vote=D) = \alpha + \beta_1 Indep + \beta_2 Rep + \beta_3 Age_{36-50} + \beta_4  Age_{51-64} + \beta_5 Age_{65+} + \beta_6 Male$$

Coefficients were chose to be realistic and to yield $\hat{y}$'s that lie within a probability range:

```{r, echo = F}
r33_lin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target  in percentage points of:
```{r, echo = F}
cat(paste0("Linear=", r33_lin$linear, " Target= ",round(r33_lin$target,4)*100, "%"))
```


I then add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. 

I run simulations where this noise is $\sqrt(2)$, yielding an $R^2$ of .33, and of $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66. Finally, I run one last batch of simulations additionally adding noise by defining the outcome as bernoulli draws using the $\hat{y}$'s with $R^2$ of .66. This yields a $R^2$ of 0.017.

```{r, echo = F}
cat(paste0("For Linear Model w/ R2=",round(r33_lin$R2,3), " the correlation of Y with p(S=1) is ", round(r33_lin$corr,2), "\n"))
cat(paste0("For Linear Model w/ R2=",round(r66_lin$R2,3), " the correlation of Y with p(S=1) is ", round(r66_lin$corr,2), "\n"))
# cat(paste0("For R2=",round(r66_bern$R2,3), " the correlation with p(S=1) is ", 
#            round(r66_bern$corr,2), "\n"))
```

\clearpage

# Results: Linear Model

Bias and Standard Errors for the linear model with two levels of noise added  ($R^2=0.33$ & $R^2=0.66$)). 

## Linear + $\mathbf{R^2=.33}$

```{r r33_lin_res, echo = F}
load(r33_file_lin)
outcome_obj= r33_lin
good = which(lapply(sims, function (x) return(class(x))) == "list")

r33_emp_SEs = empirical_SEs(sims = sims)

dplyr_is_dumb = bias["unweighted","bias"]
bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )

kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear, " Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r33_emp_SEs$emp_SEs[,c(5:ncol(r33_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear, "Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(r33_lin$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear, "SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

rm(sims, gg_out, table, outcome, bias, SE_coverage)

```


## Linear + $\mathbf{R^2=.66}$

```{r r66_lin_res, echo=F}
load(r66_file_lin)
outcome_obj = r66_lin
good = which(lapply(sims, function (x) return(class(x))) == "list")

r66_emp_SEs = empirical_SEs(sims = sims)


dplyr_is_dumb = bias["unweighted","bias"]
bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear, " Bias \\textbf{in Percent} across ", length(good),
      " sims: All Methods (Target = ", round(mean(outcome),3)*100, "), $R^2$ on Outcome = ",
      round(r66_lin$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(r66_emp_SEs$emp_SEs[,c(5:ncol(r66_emp_SEs$emp_SEs))]*100,3), format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear, "Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear, "SE Coverage Results: Kpop",
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3)))%>%
  kable_styling(latex_options = "hold_position")
rm(sims, gg_out, table, outcome, bias, SE_coverage)
```


```{r, echo = F, eval = F, include = F}
load(r66_bern_file_lin)

good = which(lapply(sims, function (x) return(class(x))) == "list")

r66_bern_emp_SEs = empirical_SEs(sims = sims)

kable(round(bias, 3), format = "latex", booktabs = T,
      caption = paste0("Bias \\textbf{in Percent} across ", length(good),
      " sims: All Methods (Target = ", round(mean(outcome),3)*100, "), $R^2$ on Outcome = ",
      round(r66_bern$R2, 3)))

kable(round(r66_bern_emp_SEs$emp_SEs[,c(5:ncol(r66_bern_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(r66_bern_lin$R2, 3)))

kable(round(SE_coverage[-5, c(5:ncol(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("SE Coverage Results: Kpop",
                          length(good), "sims; $R^2$ on Outcome = ", round(r66_bern_lin$R2, 3)))


```

\clearpage

# Simulation Set Up: Non-linear Models

## Non-Linear Selection Model 1: (no Born)

This is the same model as above simply adding race to increase the complexity of the cross-sectional strata and hopefully increase the bias in ps due to dropped cells.

$$p(S=1) = logit^{-1}\Big( PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) + PID(3way)*Age(4way)\Big)$$


```{r, echo = F}
r66_nonlin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling probabilities:
```{r, echo = F}
r66_nonlin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

\clearpage

Looking visually at the selection probabilities for Model 1:

```{r, echo = F, fig.width=14, fig.height=16}
gridExtra::grid.arrange(r66_nonlin$plot_pinclude, r66_nonlin$plot_pinclude_pid, nrow = 2)
```

## Non-Linear Outcome Model 1:  (no Born)

As always, the outcome models using the same variables as the selection model, here adding race.

$$p(Vote=D) =  PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) + PID(3way)*Age(4way)$$

I again add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. Again this noise is $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66 and $\sqrt(2)$ yielding an $R^2$ of .33 . 

```{r, echo = F}
cat(paste0("For Non-Linear Model 1: included and R2=",round(r33_nonlin$R2,3), "\n the correlation of Y with p(S=1) is ", round(r33_nonlin$corr,2), "\n"))

cat(paste0("For Non-Linear Model 1: included and R2=",round(r66_nonlin$R2,3), "\n the correlation of Y with p(S=1) is ", round(r66_nonlin$corr,2), "\n"))

```


Coefficients are chosen through an automated procedure to be vaguely realistic and to yield $\hat{y}$'s that lie within a probability range. As a result, different noise-levels lead to the selection of slightly different outcome coefficients and population targets.

```{r, echo = F}
r33_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
r66_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target in percentage points of:
```{r, echo = F}
cat(paste0("M1: Linear=", r33_nonlin$linear, " Target= ",round(r33_nonlin$target,4)*100, "%"))
cat(paste0("M1: Linear=", r66_nonlin$linear, " Target= ",round(r66_nonlin$target,4)*100, "%"))
```


### Post-Stratification Dropped Cells 

I set up this non-linear model with variables that have many levels so that the intersection of $X^P$ in post-stratification is so complex that we're forced to drop units in the CCES population that have empty cells in the sample. I add race to this model to try and increase the complexity of the strata intersection and increase the bias of ps. Doing this, we drop the following:

```{r ps_drop_model2, echo = F}
load(r33_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model 1 and R2=",round(r33_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE,  selection_model, pS_denom)


load(r66_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model 1 and R2=",round(r66_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,  selection_model, pS_denom)

```

\clearpage

#### Visual Correlation of Selection and Non-linear Outcome Model with Race

```{r, echo = F, fig.align = 'center', fig.width = 10, fig.height = 7}
 r66_nonlin$plot_pinclude_outcome
```

\clearpage


## Non-Linear Selection Model 2: (Born no interaction)

Here I add born again status and an interaction between born again status and age buckets to our non-linear model.

$$p(S=1) = logit^{-1}\Big( PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\ + BornAgain + PID(3way)*Age(4way) \Big)$$


```{r, echo = F}
r5_nonlin$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling probabilities:
```{r, echo = F}
r5_nonlin$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

\clearpage

Looking visually at the selection probabilities for Model 2 with the born again (no interaction)

```{r, echo = F, fig.width=14, fig.height=16}
gridExtra::grid.arrange(r5_nonlin$plot_pinclude, r5_nonlin$plot_pinclude_pid, nrow = 2)
```

\clearpage

## Non-Linear Outcome Model 2: (Born no interaction)

As always, the outcome models using the same variables as the selection model.

$$p(Vote=D) =  PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\ + BornAgain + PID(3way)*Age(4way)$$

I again add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. Again this noise is $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66 and $\sqrt(2)$ yielding an $R^2$ of ..5

```{r, echo = F}
cat(paste0("For Non-Linear Model 2 w/BORNAGAIN included\n R2=",round(r5_nonlin$R2,3), " the correlation of Y with p(S=1) is ", round(r5_nonlin$corr,2), "\n"))
```

I start with coefficients as simply the negative selection model coefficients then through an automated procedure adjust them until they produce $\hat{y}$'s that lie within a probability range.

```{r, echo = F}
r5_nonlin$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target in percentage points of:
```{r, echo = F}
cat(paste0("M2: Linear=", r5_nonlin$linear, " Target= ",round(r5_nonlin$target,4)*100, "%"))
```


### Post-Stratification Dropped Cells Model 2 (Born no interaction)

```{r ps_drop_model, echo = F}
load(r5_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model 2: w/BORNAGAIN included \n R2=",round(r5_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```

\clearpage


## Non-Linear Selection Model 3: (Born w/interaction)

Here I add born again status and an interaction between born again status and age buckets to our non-linear model.

\begin{align*}
p(S=1) &= logit^{-1}\Big( PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\ 
&+ BornAgain + PID(3way)*Age(4way) + BornAgain*Age(4way)\Big)
\end{align*}


```{r, echo = F}
r5_nonlin_inter$selection_coefs_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields the following sampling probabilities:
```{r, echo = F}
r5_nonlin_inter$selection_prob_kable %>%
  kable_styling(latex_options = "hold_position")
```

\clearpage

Looking visually at the selection probabilities for Model 3 with the born again interaction

```{r, echo = F, fig.width=14, fig.height=16}
gridExtra::grid.arrange(r5_nonlin_inter$plot_pinclude, r5_nonlin_inter$plot_pinclude_pid, nrow = 2)
```

\clearpage

## Non-Linear Outcome Model 3: (Born w/interaction)

As always, the outcome models using the same variables as the selection model.


\begin{align*}
p(Vote=D) &= PID(3way) + Age(4way)+ Gender + Educ(3way) + Race(4way) \\
&+ BornAgain + PID(3way)*Age(4way) + BornAgain*Age(4way)
\end{align*}


I again add normally distributed noise to this outcome with mean zero and standard deviation $\sigma = sd(Y)*noise$. Again this noise is $\frac{1}{2}*\sqrt(2)$ yielding an $R^2$ of .66 and $\sqrt(2)$ yielding an $R^2$ of .5 . 

```{r, echo = F}
cat(paste0("For Non-Linear Model 3 w/BORNAGAIN INTER included\n R2=",round(r5_nonlin_inter$R2,3), " the correlation of Y with p(S=1) is ", round(r5_nonlin_inter$corr,2), "\n"))
```

I start with coefficients as simply the negative selection model coefficients then through an automated procedure adjust them until they produce $\hat{y}$'s that lie within a probability range.

```{r, echo = F}
r5_nonlin_inter$coefs_outcome_kable %>%
  kable_styling(latex_options = "hold_position")
```

This yields a population target in percentage points of:
```{r, echo = F}
cat(paste0("M3: Linear=", r5_nonlin_inter$linear, " Target= ",round(r5_nonlin_inter$target,4)*100, "%"))
```


### Post-Stratification Dropped Cells Model 3 (Born w/interaction)

```{r ps_drop_m3, echo = F}
load(r5_nonlin_inter_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
temp = sims[good]
ps_dropped <- lapply(temp, `[[`, 4)
dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
# dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
# dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

cat(paste0("For Non-Linear Model 3: BORNAGAIN + INTER included \n R2=",round(r5_nonlin$R2,3), " PS must drop an average of ", round(mean(dropped_cells$sum),2),
           " units\nw/a sd=", round(sd(dropped_cells$sum)),
           " which is ", round(100*(mean(dropped_cells$sum)/44932),2),"% of the population\n"))
rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)
```

\clearpage




## Non-Linear Results:

Recall the true model includes: pid, gender, age buckets, 3way education and race. So it looks like our automated MF routine **"kpop_mf"** is potentially looking like the best recommendation?

- **"post_stratification"** applies ps on the true model (pid, education, gender, age, race)
- **"rake_truth"** calibrates on the true model (pid, education, gender, age, race)
- **"kpop REDUC"** is the kpop run without any constraints added, but with ONLY the variables in the selection model (pid, education, gender, age, race) given provided to kpop. 
- **"kpop Converged"** refers to default kpop with the full data, but forcing ebalance convergence. 

The other kpop methods have constraints appended to match the variables in each of the raking methods with names referring accordingly. **kpop+MF (XX)** appends "XX" variables directly while **kpop aMF** is the automated mean first approach.

- **"Demos"** appends age_buckets (4way), gender, race (4way), region (4way), and pid (3way) **"Demos+Edu"** adds education (6way) to these (NB: note that this "demos" model is almost the true model, but adds region and misses 3way education so it's interesting it seems to do so well)
- **"All"** appends all available variables including age buckets, gender, race, region, pid, education, income (5way), religion (5way), born-again (binary), and church attendance (4way).
- **"kpop+aMF"** refers to the automated mean first routine which appends an $m$ optimal number of constraints which are the $m$ left singular vectors of the svd of all the variables (same variables as **"all"**).


```{r r33_res, echo=F}
load(r33_nonlin_file)
outcome_obj = r33_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r33 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

gg_out_r33 = ggplot(data = plot_lasso_margin_r33,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("M1: Race Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))

table_r33 = plot_lasso_margin_r33 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))
#table_r33
dplyr_sux = table_r33 %>% filter(estimator_name == "Unweighted") %>% dplyr::select(Bias) %>% pull()
table_r33 = table_r33 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r33 = as.data.frame(table_r33)
rownames(table_r33) = table_r33$estimator_name
table_r33 = table_r33[,-1]

rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)
```


```{r r66_res, echo=F}
load(r66_nonlin_file)
outcome_obj = r66_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r66 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

gg_out_r66 = ggplot(data = plot_lasso_margin_r66,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("M1:Race Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))


table_r66 = plot_lasso_margin_r66 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))

dplyr_sux = table_r66 %>% filter(estimator_name == "Unweighted") %>% dplyr::select(Bias) %>% pull()
table_r66 = table_r66 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r66 = as.data.frame(table_r66)
rownames(table_r66) = table_r66$estimator_name
table_r66 = table_r66[,-1]

rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)
```

```{r r5_res, echo=F}
load(r5_nonlin_file)
outcome_obj = r5_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r5 <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

gg_out_r5 = ggplot(data = plot_lasso_margin_r5,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("M2: Born+no inter Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))


table_r5 = plot_lasso_margin_r5 %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))

dplyr_sux = table_r5 %>% filter(estimator_name == "Unweighted") %>% dplyr::select(Bias) %>% pull()
table_r5 = table_r5 %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r5 = as.data.frame(table_r5)
rownames(table_r5) = table_r5$estimator_name
table_r5 = table_r5[,-1]
rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```


```{r r5_inter_res, echo=F}
load(r5_nonlin_inter_file)
outcome_obj = r5_nonlin_inter
good = which(lapply(sims, function (x) return(class(x))) == "list")

est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
plot = est
margin_sim = mean(outcome)*100
plot_lasso_margin_r5_inter <- plot %>% 
    dplyr::select(unweighted, 
                  rake_demos_noeduc,
                  rake_demos_weduc,
                  rake_all,
                  post_stratification,
                  #post_strat_reduc,
                  #post_strat_all,
                  rake_truth,
                  kpop, 
                  kpop_reduc, 
                  kpop_conv,
                  kpop_mf, 
                  kpop_demos,
                  kpop_demos_wedu,
                  kpop_all, 
                  ht_truth,
                  hayek_truth) %>% 
    pivot_longer(everything(),
                 names_to = "estimator", 
                 values_to = "margin") %>%
    mutate(margin = margin * 100,
           estimator_name = factor(case_when(estimator == "kpop" ~ "kpop",
                                             estimator == "kpop_reduc" ~ "kpop\n REDUC",
                                             estimator == "kpop_mf" ~ "kpop aMF (All)",
                                             estimator == "kpop_conv" ~ "kpop Converged",
                                             estimator == "kpop_demos" ~ "kpop+MF:\n (Demos)",
                                             estimator == "kpop_demos_wedu" ~ "kpop+MF:\n (Demos+Edu)",
                                             estimator == "kpop_all" ~ "kpop+MF:\n (All)",
                                             estimator == "rake_demos_noeduc" ~ "Mean Calibration:\n (Demos)",
                                             estimator == "rake_demos_weduc" ~  "Mean Calibration:\n (Demos+Edu)",
                                             estimator == "rake_all" ~ "Mean Calibration:\n (All)",
                                             estimator == "rake_truth" ~ "Mean Calibration:\n True Selection\nModel",
                                             estimator == "post_stratification" ~ "Post-Strat Truth",
                                             estimator == "post_strat_reduc" ~ "Post-Stratification:\n (Reduc)",
                                             estimator == "post_strat_all" ~ "Post-Strat All",
                                             estimator == "unweighted" ~ "Unweighted",
                                             estimator == "ht_truth" ~ "Horvitz-Thompson",
                                             estimator == "hayek_truth" ~ "Hayek"),
                                   levels = c("Unweighted", 
                                              "Mean Calibration:\n (Demos)",
                                              "Mean Calibration:\n (Demos+Edu)",
                                              "Mean Calibration:\n (All)",
                                              "Post-Strat Truth", 
                                              #"Post-Stratification:\n (Reduc)", 
                                              #"Post-Strat All",
                                              "kpop",
                                              "kpop\n REDUC",
                                               "kpop Converged",
                                              "kpop aMF (All)",
                                              "kpop+MF:\n (Demos)",
                                              "kpop+MF:\n (Demos+Edu)",
                                              "kpop+MF:\n (All)",
                                              "Mean Calibration:\n True Selection\nModel",
                                              "Horvitz-Thompson",
                                              "Hayek"
                                   ) ) )

gg_out_r5_inter = ggplot(data = plot_lasso_margin_r5_inter,
                aes(x = estimator_name, y = margin)) +
    geom_boxplot(alpha = 0.2) +
    geom_hline(yintercept = mean(outcome)*100) +
    theme_bw() +
    xlab("") +
    ylab("Modeled Vote Margin") +
    annotate(geom = "text", x = 0.85, y = mean(outcome)*100 +0.25, size = 2.7, angle = 90,
             label = "True Target\nPopulation\nMargin", hjust = 0) +
    ggtitle("M3: Born+inter Nonlinear", paste0(nrow(est)," R2=",round(outcome_obj$R2,2), " sims w/avg n_samp =", round(mean(est$n)))) +
    theme(panel.grid.major.x = element_blank(),
          axis.text.x = element_text(angle = 45, hjust = 1))


table_r5_inter = plot_lasso_margin_r5_inter %>% 
    mutate(estimator_name = gsub("\n", " ", estimator_name)) %>%
    group_by(estimator_name) %>%
    summarize(
        Bias = mean(margin - margin_sim),
        #SE_boot= sd(margin),
        MSE = mean((margin - margin_sim)^2)
    ) %>% arrange(-abs(Bias))

dplyr_sux = table_r5_inter %>% filter(estimator_name == "Unweighted") %>% dplyr::select(Bias) %>% pull()
table_r5_inter = table_r5_inter %>% mutate(Bias_Reduc = 1- Bias / dplyr_sux)
table_r5_inter = as.data.frame(table_r5_inter)
rownames(table_r5_inter) = table_r5_inter$estimator_name
table_r5_inter = table_r5_inter[,-1]
rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom)
#rm(coefs, coefs_outcome, p_include, outcome, sims,SE_coverage, emp_SE, selection_model, pS_denom)
```



### Model 1:Non-linear + $\mathbf{R^2=.33}$ (Race only)

Note there is less bias here that the $R^2=.66$ results. This is because the automated procedure which selected the outcome coefficients resulted in an outcome model that was less strongly correlated with the selection probability. We still get around the same proportional decrease in bias however. Note that, unfortunately, we have very slight undercoverage for several of the kpop methods (most importantly kpop_mf and kpop_all)



```{r non_lin_res_race_r33, echo = F, warnings = F}
load(r33_nonlin_file)
outcome_obj = r33_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)
r33_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

# est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias))) 
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )

SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage

kable(round(table_r33, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(M1: Race) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r33_emp_SEs$emp_SEs[,grepl("kpop", colnames(r33_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(M1: Race) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(M1: Race) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)
rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom, table_r33, r33_emp_SEs)
```


\clearpage

### Model 1: Non-linear + $\mathbf{R^2=.66}$ (Race only)

```{r non_lin_res_race_r66, echo = F, warnings = F}
load(r66_nonlin_file)
outcome_obj = r66_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)

r66_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

#est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias)))
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage

kable(round(table_r66, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(M1: Race) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")

kable(round(r66_emp_SEs$emp_SEs[,grepl("kpop", colnames(r66_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(M1: Race) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(M1: Race) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)

rm(coefs, coefs_outcome, p_include, outcome, sims, selection_model, pS_denom,
   SE_coverage, table_r66, r66_emp_SEs)
```

\clearpage

### Model 2: Non-linear + $\mathbf{R^2=.5}$ (Born no Interaction)

```{r non_lin_res_race_r55, echo = F, warnings = F}
load(r5_nonlin_file)
rm(table)
outcome_obj = r5_nonlin
good = which(lapply(sims, function (x) return(class(x))) == "list")
rm(SE_coverage)

# est <- lapply(sims[good], `[[`, 1) %>% bind_rows()
# cols = grepl("kpop|rake|post_stratification|unweighted", colnames(est))
# bias = colMeans(est[, cols], na.rm = T)
# bias = bias - mean(outcome)
# bias = data.frame(bias = t(t(bias))*100)
# bias = bias %>% arrange(desc(abs(bias)))
# dplyr_is_dumb = bias["unweighted","bias"]
# bias = bias %>% mutate(bias_reduction = 1 - bias/dplyr_is_dumb )
# bias
SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage
r5_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))


kable(round(table_r5, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(M2:Born+ no inter) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r5_emp_SEs$emp_SEs[,grepl("kpop", colnames(r5_emp_SEs$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(M2:Born+no inter) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(M2:Born+no inter) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")
              #mean(est$n)
rm(coefs, coefs_outcome, p_include, outcome, sims, emp_SE, selection_model, pS_denom, 
   SE_coverage, table_r5, r5_emp_SEs)
```

\clearpage
### Model 3: Non-linear + $\mathbf{R^2=.5}$ (Born w/Interaction)

```{r non_lin_res_race_r55_inter, echo = F, warnings = F}
load(r5_nonlin_inter_file)
#rm(table)
outcome_obj = r5_nonlin_inter
good = which(lapply(sims, function (x) return(class(x))) == "list")
#rm(SE_coverage)

SE_coverage = all_SE_coverage(sims[good], truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage
r5_emp_SEs_inter = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

kable(round(table_r5_inter, 3), format = "latex", booktabs = T,
      caption = paste0("Linear=",outcome_obj$linear,"(M3:Born+inter) Bias \\textbf{in Percent} across ", length(good),
                       " sims: All Methods (Target = ", 
                       round(mean(outcome),3)*100, "), $R^2$ on Outcome = ",
                       round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(r5_emp_SEs_inter$emp_SEs[,grepl("kpop", colnames(r5_emp_SEs_inter$emp_SEs))]*100,3), 
      format = "latex", booktabs = T, 
      caption = paste("Linear=",outcome_obj$linear,"(M3:Born+inter) Empirical SE Results \\textbf{in Percent}: Avg SE + Boot SE (sd(estimate)) ", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(M3:Born+inter) SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")


SE_coverage = all_SE_coverage(sims[good], adjust_bias = TRUE, 
                              truth = mean(outcome), drop_NA = T)
SE_coverage = SE_coverage$coverage

kable(round(SE_coverage[-5, grepl("kpop", colnames(SE_coverage))], 3), format = "latex", booktabs = T,
          caption = paste("Linear=",outcome_obj$linear,"(M3:Born+inter) BIAS ADJUSTED SE Coverage Results: Kpop", 
                          length(good), "sims; $R^2$ on Outcome = ", round(outcome_obj$R2, 3))) %>%
  kable_styling(latex_options = "hold_position")




rm(coefs, coefs_outcome, p_include, outcome, sims, SE_coverage, selection_model, pS_denom, r5_emp_SEs_inter)
```

\clearpage

### Box Plots 

```{r echo=F,fig.width=8, fig.height=6}
gg_out_r33
gg_out_r66
gg_out_r5
gg_out_r5_inter
```



\clearpage
### Closer Look at Standard Errors - Model 3: Non-linear + $\mathbf{R^2=.5}$ (Born w/Interaction)


```{r la_se, echo = F}
##### LA SEs
load(r5_nonlin_inter_file)
outcome_obj = r5_nonlin_inter
good = which(lapply(sims, function (x) return(class(x))) == "list")
#1. look at SEs:
est <- lapply(sims, `[[`, 1) %>% bind_rows()
SEs <- lapply(sims, `[[`, 2) %>% bind_rows()
methods = "rake|kpop|unweighted"
SEs = SEs[grepl(methods, colnames(SEs))]
r5_emp_SEs = suppressWarnings(empirical_SEs(sims = sims[good], na_rm = T))

SE_fixed = SEs[grepl("SE_fixed$", colnames(SEs))] %>% 
    pivot_longer(everything(),names_to = "estimator", 
                 values_to = "SE_fixed") %>% mutate(estimator = gsub("_SE_fixed", "", estimator))
SE_linear = SEs[grepl("SE_linear$", colnames(SEs))] %>% 
    pivot_longer(everything(),names_to = "estimator", 
                 values_to = "SE_linear") %>% mutate(estimator = gsub("_SE_linear", "", estimator))
SE_quasi = SEs[grepl("SE_quasi$", colnames(SEs))]%>% 
    pivot_longer(everything(),names_to = "estimator", 
                 values_to = "SE_quasi") %>% mutate(estimator = gsub("_SE_quasi", "", estimator))
SE_chad= SEs[grepl("SE_chad$", colnames(SEs))]%>% 
    pivot_longer(everything(),names_to = "estimator", 
                 values_to = "SE_chad") %>% mutate(estimator = gsub("_SE_chad", "", estimator))
tot = cbind(SE_fixed, SE_linear[,2], SE_quasi[,2], SE_chad[,2])

cols = grepl("kpop|unweighted|rake", colnames(est))
SE_boot =  t(as.matrix(apply(est, 2, sd)))
SE_boot = SE_boot[, cols]
Boot = data.frame(estimator=names(SE_boot),
                  SE_Boot = c(SE_boot))
method ="kpop_all|mf|truth"
sub = tot %>% filter(grepl(method, estimator) | estimator == "kpop")
sub2 = sub %>% pivot_longer(cols= c(2:5), names_to = "SE_type") %>% rename(SE = value)

gg_t2 = ggplot(sub2) + 
    geom_density(aes(x = SE, color = SE_type, fill = SE_type ), alpha = .2) +  
    geom_vline(aes(xintercept =SE_Boot),
               data = Boot %>% filter(grepl(method, estimator)| estimator == "kpop")) +
    annotate(geom = "label",
             x= .005,
             y=Inf, vjust = 1.4,
             color = "Black",
             parse = T,
             label =  "sd(Y_hat)") + 
    theme_bw()  + 
    xlab("SE Estimate") + 
    ggtitle("Distribution of SEs by Method and SE Estimator") + 
    facet_grid(cols = vars(estimator))



######## Subset Big and Small SEs
kpop_all_SE = SEs[, grepl("kpop_mf", colnames(SEs))]
#lets get those in the top 75% percentile
#start by arranging this will be a bitch if we dont do it by column
kpop_mf_chad = kpop_all_SE$kpop_mf_SE_chad %>% sort()
big_SE = which(kpop_all_SE$kpop_mf_SE_chad %in% kpop_mf_chad[round(length(kpop_mf_chad)*.75):length(good)])
#subset sims by 75%+ SEs vs rest
b_SE = sims[big_SE]
s_SE = sims[-big_SE]



########## Look at samples
samp_check <- lapply(b_SE, `[[`, 5)
samp_check = samp_check %>% bind_rows()
s_samp_check <- lapply(s_SE, `[[`, 5)
s_samp_check= s_samp_check %>% bind_rows()
# <= 5%
#cat("How many samples have strata in outcome/selection model with less than 5% units?\n Below is SEs in the top 75 percentile versus the rest")
samp = cbind(top_75  = summary(samp_check$check.leq_5pp), lower_75 = summary(s_samp_check$check.leq_5pp))



##### numdims:
big_est = est[big_SE,]
s_est = est[-big_SE,]
#interesting on avg the bigger SEs have slkightlyu fewer numdims and lower variance in the choice
#that is the polar opposite of what id expect....bc if the weights have more variance, presumably we have more numdims?
#but it's the opposite, maybe weightsare  more constrained with more numdims> 
#cat("What about the average dimensions of K selected?")
k_avg = cbind(top_75_avg = colMeans(big_est[,grepl("numdims", colnames(est))]), lower_75_avg = colMeans(s_est[,grepl("numdims", colnames(est))]))
#cat("What about the sd in this choice of dimensions of K selected?")
k_sd = cbind(top_75_sd = apply(big_est[,grepl("numdims", colnames(est))],2, sd),
      lower_75_sd = apply(s_est[,grepl("numdims", colnames(est))], 2, sd))



########## numdims 
#3. look at var(w) and max and min weights 
weights = lapply(sims, `[[`, 3) #%>% bind_rows()
b_weights = weights[big_SE]
s_weights = weights[-big_SE]


#mean summary stats
t = lapply(b_weights, function(x) data.frame(apply(x, 2, summary)))
t = t %>% bind_rows()
min = colMeans(t[grepl("Min", rownames(t)),])
max = colMeans(t[grepl("Max", rownames(t)),])
mean = colMeans(t[grepl("Mean", rownames(t)),])
perc_25 = colMeans(t[grepl("1st", rownames(t)),])
perc_75 = colMeans(t[grepl("3rd", rownames(t)),])    
avg_w = rbind(min = min, perc_25 = perc_25, mean = mean, perc_75 = perc_75, max = max)

t = lapply(s_weights, function(x) data.frame(apply(x, 2, summary)))
t = t %>% bind_rows()
min = colMeans(t[grepl("Min", rownames(t)),])
max = colMeans(t[grepl("Max", rownames(t)),])
mean = colMeans(t[grepl("Mean", rownames(t)),])
perc_25 = colMeans(t[grepl("1st", rownames(t)),])
perc_75 = colMeans(t[grepl("3rd", rownames(t)),])    
avg_w_s = rbind(min = min, perc_25 = perc_25, mean = mean, perc_75 = perc_75, max = max)



#rounding helps: so min and 25% are slightly lower, but notable that max is in the tenths here
#round(t(avg_w) - t(avg_w_s),3)#

#lol ok but like we probably just want sd?? lol, indeed look slike sd of weights is bigger for big SEs
t = lapply(b_weights, function(x) apply(x, 2, sd)) %>% bind_rows()
s = lapply(s_weights, function(x) apply(x, 2, sd)) %>% bind_rows()
#cat("Do the larger SEs have weights with, on average, greater variance? Yes!")
var_w = round(t(rbind(var_lower_75 = colMeans(s)[-1], var_top_75 = colMeans(t)[-1], 
              Diff = colMeans(t)[-1] - colMeans(s)[-1])),3)

t = b_weights %>% bind_rows()
s = s_weights %>% bind_rows()
t = t[, -1]
s = s[,-1]
t = t %>% pivot_longer(cols= c(1:7), names_to = "Method") %>% rename(weight = value)
s = s %>% pivot_longer(cols= c(1:7), names_to = "Method") %>% rename(weight = value)
ggdat = rbind(t,s) %>% mutate(SE_percentile = c(rep("Top 75%", nrow(t)), rep("Bottom 75%", nrow(s))))
#ok so there's some HUGE weights in kpop_reduc, let's get rid of those bc we dont really care about this method
# logi = ggdat$weight >= max(ggdat$weight) - 10
# # sum(logi)
# # ggdat[logi, ]
# ggdat = ggdat %>% filter(Method != "kpop_w_reduc")
# nrow(ggdat)
gg_allweights = ggplot(ggdat) + 
    geom_density(aes(x = weight, color = Method, fill = Method), alpha = .2)+ 
    theme_bw() + 
    ggtitle("Distribution of All Weights across sims by SE percentile and Method") + 
    facet_wrap(~SE_percentile)


gg_allweights_zoom = ggplot(ggdat) + 
    geom_density(aes(x = weight, color = Method, fill = Method), alpha = .2)+ 
    theme_bw() + 
    xlim(0, 3) + 
    ggtitle("Distribution of All Weights across sims by SE percentile and Method (xlim(0,3))") + 
    facet_wrap(~SE_percentile)


t = lapply(b_weights, function(x) apply(x, 2, max)) %>% bind_rows()
s = lapply(s_weights, function(x) apply(x, 2, max)) %>% bind_rows()
t = t[, -1]
s = s[,-1]
t = t %>% pivot_longer(cols= c(1:7), names_to = "Method") %>% rename(max_weight = value)
s = s %>% pivot_longer(cols= c(1:7), names_to = "Method") %>% rename(max_weight = value)
ggdat = rbind(t,s) %>% mutate(SE_percentile = c(rep("Top 75%", nrow(t)), rep("Bottom 75%", nrow(s))))
gg_max_weights = ggplot(ggdat) + 
    geom_density(aes(x = max_weight, color = Method, fill = Method), alpha = .2)+ 
    theme_bw() + 
    ggtitle("Distribution of Max Weights across sims by SE percentile and Method") + 
    facet_wrap(~SE_percentile)
#ok this looks the same... 
avg_w = avg_w[, -1]
avg_w_s = avg_w_s[, -1]
```

#### Sample:

How many samples have strata in outcome/selection model with less than 5% units? Below is SEs in the top 75 percentile versus the rest.

We see that, yes the larger SEs have samples which have slightly more strata with 5% or fewer units in them, however, the difference is slight. We don't appear to be getting crazy wonky samples. Notably no sample has a strata with 1% or fewer  sampled units, sso there are no extremely "bad" samples.

```{r samp_se, echo = F}
kable(samp,3, format = "latex", booktabs = T,
                       caption = paste("Distribution of Strata containing less than 5 Percent of Sample Units by SE Percentile: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") )%>% kable_styling(latex_options = "hold_position")
```

\clearpage
#### Dimensions of K

What about the dimensions of K that are balanced on? Interestingly we see, that the simulations with larger SEs balance on fewer dimensions of K on average, and have a lower variance in this selection. This is slightly counter-intuitive to me. I would have expected large SEs to correspond with high variance weights that are a result from balancing on a larger, and generally higher range of dimensions of K. Perhaps.however, instead the weights, being less constrained end up with more variance.

```{r k_se, echo = F}
kable(k_avg,3, format = "latex", booktabs = T,
                       caption = paste("Avgerage Dimensions of K by SE Percentile: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") )%>% kable_styling(latex_options = "hold_position")
kable(k_sd,3, format = "latex", booktabs = T,
                       caption = paste("SD of Dimensions of K by SE Percentile: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") )%>% kable_styling(latex_options = "hold_position")
```


#### Weights

How do the weights look? Indeed larger SEs correspond to weights with generally larger variance as we expect. Looking at the other moments, as we would expect for larger variances, we see that they also have slightly higher max weights and also lower min and 25th percentile weights. Interestingly, however, they have almost the same, even slightly smaller, 75 percentile weights.

```{r weights_se, echo = F}
kable(round(t(avg_w),3), format = "latex", booktabs = T,
                       caption = paste("Avg Moments of Top 75 Percentile Weights: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") )%>%
  kable_styling(latex_options = "hold_position")
kable(round(t(avg_w_s),3), format = "latex", booktabs = T,
                       caption = paste("Avg Moments of Bottom 75 Percentile Weights: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") ) %>%
  kable_styling(latex_options = "hold_position")

#bigger max, smaller min, smaller 25% and 75% 
#interesting that the larger weights have a larger 75%, though its in the hundreths so its fairly similar
kable(round(t(avg_w) - t(avg_w_s),3), format = "latex", booktabs = T,
                       caption = paste("Avg Moments Difference Top 75 Percentile - Bottom: Linear=",outcome_obj$linear," R2=",
                      round(outcome_obj$R2,2), "Outcome Model") ) %>% kable_styling(latex_options = "hold_position")

```

\clearpage

#### Graphically...

Below is the original plot showing a longer right tail of SEs. The other plots are less informative. Additionally the distribution of all weights across simulation subsetted by whether they yielded SE in the top 25 percentile or not. As we can see in both there are some very large weights, but they are rare and they occur in samples that yield SEs in the top 25th percent and not. Zooming in a bit to a smaller range, we see again that the distribution of the weights looks roughly similar for this SE percentile cutoff.
Taking a closer look at the large weights specifically, looking at the distirbution of max weights, again, we're not seeing anything all too different between samples taht yield SEs in the top 25 percentile and not.
\clearpage

```{r se_plot, echo = F, fig.width=12, fig.height=8}
gg_t2
gg_allweights
gg_allweights_zoom
gg_max_weights

```












```{r la, echo = F, include = F}
load(r66_nonlin_file)
good = which(lapply(sims, function (x) return(class(x))) == "list")
ps_dropped <- lapply(sims[good], `[[`, 4)
samp_check <- lapply(sims[good], `[[`, 5)

dropped_cells = lapply(ps_dropped, `[[`, 1) %>% bind_rows()
dropped_cells_reduc = lapply(ps_dropped, `[[`, 2)  %>% bind_rows()
dropped_cells_all = lapply(ps_dropped, `[[`, 3) %>% bind_rows()

mean(dropped_cells$sum)
samp_check = samp_check %>% bind_rows()
sum(samp_check$bad_sample)
mean(samp_check$drop_ps)
table(samp_check$check.leq_1pp) == length(good)
unique(samp_check$check.fail)
#what about less than 5pp? hmm yes more
table(samp_check$check.leq_5pp)
```




